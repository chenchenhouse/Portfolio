{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a672b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from pyppeteer import launch,launcher\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime,timedelta,timezone\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import logging\n",
    "import tkinter\n",
    "import joblib\n",
    "import json\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf5645d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>HomeScore</th>\n",
       "      <th>Away</th>\n",
       "      <th>AwayScore</th>\n",
       "      <th>讓分</th>\n",
       "      <th>總分</th>\n",
       "      <th>Eventcode_x</th>\n",
       "      <th>主勝(初)</th>\n",
       "      <th>客勝(初)</th>\n",
       "      <th>主勝率(初)</th>\n",
       "      <th>...</th>\n",
       "      <th>Away_starters5_BLK</th>\n",
       "      <th>Away_starters5_TOV</th>\n",
       "      <th>Away_starters5_PF</th>\n",
       "      <th>Away_starters5_PTS</th>\n",
       "      <th>Away_starters5_Game_Score</th>\n",
       "      <th>Away_starters5_+/-</th>\n",
       "      <th>Away_starters5_noplay_PointDiff</th>\n",
       "      <th>客隊ELO</th>\n",
       "      <th>主隊ELO</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>BOS</td>\n",
       "      <td>101</td>\n",
       "      <td>IND</td>\n",
       "      <td>98</td>\n",
       "      <td>5.5</td>\n",
       "      <td>191.0</td>\n",
       "      <td>188660</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.45</td>\n",
       "      <td>61.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>2257.9675</td>\n",
       "      <td>1970.8143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>PHO</td>\n",
       "      <td>112</td>\n",
       "      <td>SAC</td>\n",
       "      <td>114</td>\n",
       "      <td>5.0</td>\n",
       "      <td>208.5</td>\n",
       "      <td>188656</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.51</td>\n",
       "      <td>62.63</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>15.4</td>\n",
       "      <td>10.45</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>1918.5838</td>\n",
       "      <td>2154.6768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>UTA</td>\n",
       "      <td>82</td>\n",
       "      <td>DAL</td>\n",
       "      <td>105</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>205.5</td>\n",
       "      <td>188655</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.65</td>\n",
       "      <td>43.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.69</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2220.9141</td>\n",
       "      <td>1920.5684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>IND</td>\n",
       "      <td>90</td>\n",
       "      <td>WAS</td>\n",
       "      <td>97</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>183.5</td>\n",
       "      <td>188666</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.59</td>\n",
       "      <td>40.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2094.4555</td>\n",
       "      <td>2254.1106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>MIA</td>\n",
       "      <td>102</td>\n",
       "      <td>MIN</td>\n",
       "      <td>92</td>\n",
       "      <td>7.5</td>\n",
       "      <td>205.0</td>\n",
       "      <td>188667</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.35</td>\n",
       "      <td>71.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>27.5</td>\n",
       "      <td>20.63</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2080.6630</td>\n",
       "      <td>2355.2815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>CHI</td>\n",
       "      <td>124</td>\n",
       "      <td>IND</td>\n",
       "      <td>109</td>\n",
       "      <td>7.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484626</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.46</td>\n",
       "      <td>72.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>11.03</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1942.8995</td>\n",
       "      <td>2193.6864</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>MIN</td>\n",
       "      <td>134</td>\n",
       "      <td>SAS</td>\n",
       "      <td>122</td>\n",
       "      <td>9.5</td>\n",
       "      <td>235.5</td>\n",
       "      <td>484627</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.90</td>\n",
       "      <td>75.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>11.6</td>\n",
       "      <td>9.00</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>2113.3237</td>\n",
       "      <td>2066.1966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>UTA</td>\n",
       "      <td>109</td>\n",
       "      <td>HOU</td>\n",
       "      <td>101</td>\n",
       "      <td>8.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484628</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.79</td>\n",
       "      <td>65.93</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.84</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>1794.1439</td>\n",
       "      <td>2285.1027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>DEN</td>\n",
       "      <td>110</td>\n",
       "      <td>LAL</td>\n",
       "      <td>99</td>\n",
       "      <td>5.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484629</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.97</td>\n",
       "      <td>68.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>10.7</td>\n",
       "      <td>7.87</td>\n",
       "      <td>-6.7</td>\n",
       "      <td>2088.1050</td>\n",
       "      <td>2225.9103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>POR</td>\n",
       "      <td>98</td>\n",
       "      <td>MIA</td>\n",
       "      <td>119</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>221.5</td>\n",
       "      <td>484630</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.95</td>\n",
       "      <td>51.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.90</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2238.5712</td>\n",
       "      <td>2060.2239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7357 rows × 568 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Home  HomeScore Away  AwayScore   讓分     總分  Eventcode_x  主勝(初)  \\\n",
       "Matchtime                                                                    \n",
       "2014-11-08  BOS        101  IND         98  5.5  191.0       188660   1.53   \n",
       "2014-11-08  PHO        112  SAC        114  5.0  208.5       188656   1.50   \n",
       "2014-11-08  UTA         82  DAL        105 -2.5  205.5       188655   2.17   \n",
       "2014-11-09  IND         90  WAS         97 -3.5  183.5       188666   2.33   \n",
       "2014-11-09  MIA        102  MIN         92  7.5  205.0       188667   1.32   \n",
       "...         ...        ...  ...        ...  ...    ...          ...    ...   \n",
       "2022-10-27  CHI        124  IND        109  7.5  232.5       484626   1.30   \n",
       "2022-10-27  MIN        134  SAS        122  9.5  235.5       484627   1.25   \n",
       "2022-10-27  UTA        109  HOU        101  8.5  232.5       484628   1.44   \n",
       "2022-10-27  DEN        110  LAL         99  5.5  232.5       484629   1.39   \n",
       "2022-10-27  POR         98  MIA        119 -2.5  221.5       484630   1.85   \n",
       "\n",
       "            客勝(初)  主勝率(初)  ...  Away_starters5_BLK  Away_starters5_TOV  \\\n",
       "Matchtime                  ...                                           \n",
       "2014-11-08   2.45   61.56  ...                 0.5                 0.0   \n",
       "2014-11-08   2.51   62.63  ...                 1.3                 0.2   \n",
       "2014-11-08   1.65   43.24  ...                 0.4                 0.1   \n",
       "2014-11-09   1.59   40.60  ...                 0.0                 0.3   \n",
       "2014-11-09   3.35   71.53  ...                 0.6                 0.5   \n",
       "...           ...     ...  ...                 ...                 ...   \n",
       "2022-10-27   3.46   72.59  ...                 0.9                 0.3   \n",
       "2022-10-27   3.90   75.65  ...                 0.8                 0.7   \n",
       "2022-10-27   2.79   65.93  ...                 1.0                 0.1   \n",
       "2022-10-27   2.97   68.05  ...                 0.8                 0.6   \n",
       "2022-10-27   1.95   51.34  ...                 0.8                 0.2   \n",
       "\n",
       "            Away_starters5_PF  Away_starters5_PTS  Away_starters5_Game_Score  \\\n",
       "Matchtime                                                                      \n",
       "2014-11-08                0.8                 1.9                        4.3   \n",
       "2014-11-08                1.3                 1.8                       15.4   \n",
       "2014-11-08                1.4                 1.8                        4.5   \n",
       "2014-11-09                1.0                 1.1                        3.9   \n",
       "2014-11-09                2.3                 2.1                       27.5   \n",
       "...                       ...                 ...                        ...   \n",
       "2022-10-27                0.6                 4.0                       13.5   \n",
       "2022-10-27                2.2                 3.1                       11.6   \n",
       "2022-10-27                1.2                 3.9                        9.0   \n",
       "2022-10-27                1.9                 1.9                       10.7   \n",
       "2022-10-27                1.0                 2.9                       10.0   \n",
       "\n",
       "            Away_starters5_+/-  Away_starters5_noplay_PointDiff      客隊ELO  \\\n",
       "Matchtime                                                                    \n",
       "2014-11-08                3.70                             -8.2  2257.9675   \n",
       "2014-11-08               10.45                             -3.9  1918.5838   \n",
       "2014-11-08                2.69                              6.6  2220.9141   \n",
       "2014-11-09                3.29                              3.8  2094.4555   \n",
       "2014-11-09               20.63                              0.8  2080.6630   \n",
       "...                        ...                              ...        ...   \n",
       "2022-10-27               11.03                              6.9  1942.8995   \n",
       "2022-10-27                9.00                             -7.0  2113.3237   \n",
       "2022-10-27                6.84                            -10.3  1794.1439   \n",
       "2022-10-27                7.87                             -6.7  2088.1050   \n",
       "2022-10-27                7.90                              5.9  2238.5712   \n",
       "\n",
       "                主隊ELO  win  \n",
       "Matchtime                   \n",
       "2014-11-08  1970.8143    1  \n",
       "2014-11-08  2154.6768    0  \n",
       "2014-11-08  1920.5684    0  \n",
       "2014-11-09  2254.1106    0  \n",
       "2014-11-09  2355.2815    1  \n",
       "...               ...  ...  \n",
       "2022-10-27  2193.6864    1  \n",
       "2022-10-27  2066.1966    1  \n",
       "2022-10-27  2285.1027    1  \n",
       "2022-10-27  2225.9103    1  \n",
       "2022-10-27  2060.2239    0  \n",
       "\n",
       "[7357 rows x 568 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "與資料庫中NBA_BasicData資料內容相同\n",
    "'''\n",
    "df = pd.read_excel(r\"C:/Users/user/NBA預測/20221217/predictdata2.xlsx\",index_col='Matchtime')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206a2529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Home_starters1_Minutes',\n",
       " 'Home_starters2_Minutes',\n",
       " 'Home_starters3_Minutes',\n",
       " 'Home_starters4_Minutes',\n",
       " 'Home_starters5_Minutes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_col = []\n",
    "for c in df.columns.values:\n",
    "    if ('Minutes' in c) & ('Home' in c):\n",
    "        home_col.append(c)\n",
    "home_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b57b369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Home', 'HomeScore', 'Away', 'AwayScore', '讓分', '總分',\n",
       "       'Eventcode_x', '主勝(初)', '客勝(初)', '主勝率(初)', '客勝率(初)', '凱利指數(初)',\n",
       "       '凱利指數(終)', '主勝(終)', '客勝(終)', '主勝率(終)', '客勝率(終)', 'Away_game',\n",
       "       'Away_win', 'Away_lose', 'Away_win_score', 'Away_lose_score',\n",
       "       'Away_net_score', 'Away_rank', 'Away_win_rate', 'Away_game_inhome',\n",
       "       'Away_win_inhome', 'Away_lose_inhome', 'Away_win_score_inhome',\n",
       "       'Away_lose_score_inhome', 'Away_net_score_inhome',\n",
       "       'Away_rank_inhome', 'Away_win_rate_inhome', 'Away_game_inaway',\n",
       "       'Away_win_inaway', 'Away_lose_inaway', 'Away_win_score_inaway',\n",
       "       'Away_lose_score_inaway', 'Away_net_score_inaway',\n",
       "       'Away_rank_inaway', 'Away_win_rate_inaway', 'Away_game_insix',\n",
       "       'Away_win_insix', 'Away_lose_insix', 'Away_win_score_insix',\n",
       "       'Away_lose_score_insix', 'Away_net_score_insix',\n",
       "       'Away_win_rate_insix', 'Home_game', 'Home_win', 'Home_lose',\n",
       "       'Home_win_score', 'Home_lose_score', 'Home_net_score', 'Home_rank',\n",
       "       'Home_win_rate', 'Home_game_inhome', 'Home_win_inhome',\n",
       "       'Home_lose_inhome', 'Home_win_score_inhome',\n",
       "       'Home_lose_score_inhome', 'Home_net_score_inhome',\n",
       "       'Home_rank_inhome', 'Home_win_rate_inhome', 'Home_game_inaway',\n",
       "       'Home_win_inaway', 'Home_lose_inaway', 'Home_win_score_inaway',\n",
       "       'Home_lose_score_inaway', 'Home_net_score_inaway',\n",
       "       'Home_rank_inaway', 'Home_win_rate_inaway', 'Home_game_insix',\n",
       "       'Home_win_insix', 'Home_lose_insix', 'Home_win_score_insix',\n",
       "       'Home_lose_score_insix', 'Home_net_score_insix',\n",
       "       'Home_win_rate_insix', 'Both_battle', 'Away_battle', 'Home_battle',\n",
       "       'Eventcode_y', 'Home_Line1_inHome', 'Home_Line2_inHome',\n",
       "       'Home_Line3_inHome', 'Home_Line4_inHome', 'Home_Pace_inHome',\n",
       "       'Home_Total_FG_inHome', 'Home_Total_FGA_inHome',\n",
       "       'Home_Total_FG%_inHome', 'Home_Total_3P_inHome',\n",
       "       'Home_Total_3PA_inHome', 'Home_Total_3P%_inHome',\n",
       "       'Home_Total_FT_inHome', 'Home_Total_FTA_inHome',\n",
       "       'Home_Total_FT%_inHome', 'Home_Total_ORB_inHome',\n",
       "       'Home_Total_DRB_inHome', 'Home_Total_TBR_inHome',\n",
       "       'Home_Total_AST_inHome', 'Home_Total_STL_inHome',\n",
       "       'Home_Total_BLK_inHome', 'Home_Total_TOV_inHome',\n",
       "       'Home_Total_PF_inHome', 'Home_Total_PTS_inHome',\n",
       "       'Home_Total_TS%_inHome', 'Home_Total_eFG%_inHome',\n",
       "       'Home_Total_3PAr_inHome', 'Home_Total_FTr_inHome',\n",
       "       'Home_Total_ORB%_inHome', 'Home_Total_DRB%_inHome',\n",
       "       'Home_Total_TRB%_inHome', 'Home_Total_AST%_inHome',\n",
       "       'Home_Total_STL%_inHome', 'Home_Total_BLK%_inHome',\n",
       "       'Home_Total_TOV%_inHome', 'Home_Total_ORtg_inHome',\n",
       "       'Home_Total_DRtg_inHome', 'Home_Ties_inHome',\n",
       "       'Home_LeadChanges_inHome', 'Home_GameTied_inHome',\n",
       "       'Home_Led_inHome', 'Home_MCpoints_inHome', 'Home_LSdrought_inHome',\n",
       "       'Home_Line1_inAway', 'Home_Line2_inAway', 'Home_Line3_inAway',\n",
       "       'Home_Line4_inAway', 'Home_Pace_inAway', 'Home_Total_FG_inAway',\n",
       "       'Home_Total_FGA_inAway', 'Home_Total_FG%_inAway',\n",
       "       'Home_Total_3P_inAway', 'Home_Total_3PA_inAway',\n",
       "       'Home_Total_3P%_inAway', 'Home_Total_FT_inAway',\n",
       "       'Home_Total_FTA_inAway', 'Home_Total_FT%_inAway',\n",
       "       'Home_Total_ORB_inAway', 'Home_Total_DRB_inAway',\n",
       "       'Home_Total_TBR_inAway', 'Home_Total_AST_inAway',\n",
       "       'Home_Total_STL_inAway', 'Home_Total_BLK_inAway',\n",
       "       'Home_Total_TOV_inAway', 'Home_Total_PF_inAway',\n",
       "       'Home_Total_PTS_inAway', 'Home_Total_TS%_inAway',\n",
       "       'Home_Total_eFG%_inAway', 'Home_Total_3PAr_inAway',\n",
       "       'Home_Total_FTr_inAway', 'Home_Total_ORB%_inAway',\n",
       "       'Home_Total_DRB%_inAway', 'Home_Total_TRB%_inAway',\n",
       "       'Home_Total_AST%_inAway', 'Home_Total_STL%_inAway',\n",
       "       'Home_Total_BLK%_inAway', 'Home_Total_TOV%_inAway',\n",
       "       'Home_Total_ORtg_inAway', 'Home_Total_DRtg_inAway',\n",
       "       'Home_Ties_inAway', 'Home_LeadChanges_inAway',\n",
       "       'Home_GameTied_inAway', 'Home_Led_inAway', 'Home_MCpoints_inAway',\n",
       "       'Home_LSdrought_inAway', 'Home_Line1_Total', 'Home_Line2_Total',\n",
       "       'Home_Line3_Total', 'Home_Line4_Total', 'Home_Pace_Total',\n",
       "       'Home_Total_FG_Total', 'Home_Total_FGA_Total',\n",
       "       'Home_Total_FG%_Total', 'Home_Total_3P_Total',\n",
       "       'Home_Total_3PA_Total', 'Home_Total_3P%_Total',\n",
       "       'Home_Total_FT_Total', 'Home_Total_FTA_Total',\n",
       "       'Home_Total_FT%_Total', 'Home_Total_ORB_Total',\n",
       "       'Home_Total_DRB_Total', 'Home_Total_TBR_Total',\n",
       "       'Home_Total_AST_Total', 'Home_Total_STL_Total',\n",
       "       'Home_Total_BLK_Total', 'Home_Total_TOV_Total',\n",
       "       'Home_Total_PF_Total', 'Home_Total_PTS_Total',\n",
       "       'Home_Total_TS%_Total', 'Home_Total_eFG%_Total',\n",
       "       'Home_Total_3PAr_Total', 'Home_Total_FTr_Total',\n",
       "       'Home_Total_ORB%_Total', 'Home_Total_DRB%_Total',\n",
       "       'Home_Total_TRB%_Total', 'Home_Total_AST%_Total',\n",
       "       'Home_Total_STL%_Total', 'Home_Total_BLK%_Total',\n",
       "       'Home_Total_TOV%_Total', 'Home_Total_ORtg_Total',\n",
       "       'Home_Total_DRtg_Total', 'Home_Ties_Total',\n",
       "       'Home_LeadChanges_Total', 'Home_GameTied_Total', 'Home_Led_Total',\n",
       "       'Home_MCpoints_Total', 'Home_LSdrought_Total', 'Away_Line1_inHome',\n",
       "       'Away_Line2_inHome', 'Away_Line3_inHome', 'Away_Line4_inHome',\n",
       "       'Away_Pace_inHome', 'Away_Total_FG_inHome',\n",
       "       'Away_Total_FGA_inHome', 'Away_Total_FG%_inHome',\n",
       "       'Away_Total_3P_inHome', 'Away_Total_3PA_inHome',\n",
       "       'Away_Total_3P%_inHome', 'Away_Total_FT_inHome',\n",
       "       'Away_Total_FTA_inHome', 'Away_Total_FT%_inHome',\n",
       "       'Away_Total_ORB_inHome', 'Away_Total_DRB_inHome',\n",
       "       'Away_Total_TBR_inHome', 'Away_Total_AST_inHome',\n",
       "       'Away_Total_STL_inHome', 'Away_Total_BLK_inHome',\n",
       "       'Away_Total_TOV_inHome', 'Away_Total_PF_inHome',\n",
       "       'Away_Total_PTS_inHome', 'Away_Total_TS%_inHome',\n",
       "       'Away_Total_eFG%_inHome', 'Away_Total_3PAr_inHome',\n",
       "       'Away_Total_FTr_inHome', 'Away_Total_ORB%_inHome',\n",
       "       'Away_Total_DRB%_inHome', 'Away_Total_TRB%_inHome',\n",
       "       'Away_Total_AST%_inHome', 'Away_Total_STL%_inHome',\n",
       "       'Away_Total_BLK%_inHome', 'Away_Total_TOV%_inHome',\n",
       "       'Away_Total_ORtg_inHome', 'Away_Total_DRtg_inHome',\n",
       "       'Away_Ties_inHome', 'Away_LeadChanges_inHome',\n",
       "       'Away_GameTied_inHome', 'Away_Led_inHome', 'Away_MCpoints_inHome',\n",
       "       'Away_LSdrought_inHome', 'Away_Line1_inAway', 'Away_Line2_inAway',\n",
       "       'Away_Line3_inAway', 'Away_Line4_inAway', 'Away_Pace_inAway',\n",
       "       'Away_Total_FG_inAway', 'Away_Total_FGA_inAway',\n",
       "       'Away_Total_FG%_inAway', 'Away_Total_3P_inAway',\n",
       "       'Away_Total_3PA_inAway', 'Away_Total_3P%_inAway',\n",
       "       'Away_Total_FT_inAway', 'Away_Total_FTA_inAway',\n",
       "       'Away_Total_FT%_inAway', 'Away_Total_ORB_inAway',\n",
       "       'Away_Total_DRB_inAway', 'Away_Total_TBR_inAway',\n",
       "       'Away_Total_AST_inAway', 'Away_Total_STL_inAway',\n",
       "       'Away_Total_BLK_inAway', 'Away_Total_TOV_inAway',\n",
       "       'Away_Total_PF_inAway', 'Away_Total_PTS_inAway',\n",
       "       'Away_Total_TS%_inAway', 'Away_Total_eFG%_inAway',\n",
       "       'Away_Total_3PAr_inAway', 'Away_Total_FTr_inAway',\n",
       "       'Away_Total_ORB%_inAway', 'Away_Total_DRB%_inAway',\n",
       "       'Away_Total_TRB%_inAway', 'Away_Total_AST%_inAway',\n",
       "       'Away_Total_STL%_inAway', 'Away_Total_BLK%_inAway',\n",
       "       'Away_Total_TOV%_inAway', 'Away_Total_ORtg_inAway',\n",
       "       'Away_Total_DRtg_inAway', 'Away_Ties_inAway',\n",
       "       'Away_LeadChanges_inAway', 'Away_GameTied_inAway',\n",
       "       'Away_Led_inAway', 'Away_MCpoints_inAway', 'Away_LSdrought_inAway',\n",
       "       'Away_Line1_Total', 'Away_Line2_Total', 'Away_Line3_Total',\n",
       "       'Away_Line4_Total', 'Away_Pace_Total', 'Away_Total_FG_Total',\n",
       "       'Away_Total_FGA_Total', 'Away_Total_FG%_Total',\n",
       "       'Away_Total_3P_Total', 'Away_Total_3PA_Total',\n",
       "       'Away_Total_3P%_Total', 'Away_Total_FT_Total',\n",
       "       'Away_Total_FTA_Total', 'Away_Total_FT%_Total',\n",
       "       'Away_Total_ORB_Total', 'Away_Total_DRB_Total',\n",
       "       'Away_Total_TBR_Total', 'Away_Total_AST_Total',\n",
       "       'Away_Total_STL_Total', 'Away_Total_BLK_Total',\n",
       "       'Away_Total_TOV_Total', 'Away_Total_PF_Total',\n",
       "       'Away_Total_PTS_Total', 'Away_Total_TS%_Total',\n",
       "       'Away_Total_eFG%_Total', 'Away_Total_3PAr_Total',\n",
       "       'Away_Total_FTr_Total', 'Away_Total_ORB%_Total',\n",
       "       'Away_Total_DRB%_Total', 'Away_Total_TRB%_Total',\n",
       "       'Away_Total_AST%_Total', 'Away_Total_STL%_Total',\n",
       "       'Away_Total_BLK%_Total', 'Away_Total_TOV%_Total',\n",
       "       'Away_Total_ORtg_Total', 'Away_Total_DRtg_Total',\n",
       "       'Away_Ties_Total', 'Away_LeadChanges_Total', 'Away_GameTied_Total',\n",
       "       'Away_Led_Total', 'Away_MCpoints_Total', 'Away_LSdrought_Total',\n",
       "       'Home_starters1_PointDiff', 'Home_starters1_Game_Start',\n",
       "       'Home_starters1_Minutes', 'Home_starters1_FG',\n",
       "       'Home_starters1_FGA', 'Home_starters1_FG_P', 'Home_starters1_P3',\n",
       "       'Home_starters1_P3A', 'Home_starters1_P3_P', 'Home_starters1_FT',\n",
       "       'Home_starters1_FTA', 'Home_starters1_FT_P', 'Home_starters1_ORB',\n",
       "       'Home_starters1_TRB', 'Home_starters1_AST', 'Home_starters1_STL',\n",
       "       'Home_starters1_BLK', 'Home_starters1_TOV', 'Home_starters1_PF',\n",
       "       'Home_starters1_PTS', 'Home_starters1_Game_Score',\n",
       "       'Home_starters1_+/-', 'Home_starters1_noplay_PointDiff',\n",
       "       'Home_starters2_PointDiff', 'Home_starters2_Game_Start',\n",
       "       'Home_starters2_Minutes', 'Home_starters2_FG',\n",
       "       'Home_starters2_FGA', 'Home_starters2_FG_P', 'Home_starters2_P3',\n",
       "       'Home_starters2_P3A', 'Home_starters2_P3_P', 'Home_starters2_FT',\n",
       "       'Home_starters2_FTA', 'Home_starters2_FT_P', 'Home_starters2_ORB',\n",
       "       'Home_starters2_TRB', 'Home_starters2_AST', 'Home_starters2_STL',\n",
       "       'Home_starters2_BLK', 'Home_starters2_TOV', 'Home_starters2_PF',\n",
       "       'Home_starters2_PTS', 'Home_starters2_Game_Score',\n",
       "       'Home_starters2_+/-', 'Home_starters2_noplay_PointDiff',\n",
       "       'Home_starters3_PointDiff', 'Home_starters3_Game_Start',\n",
       "       'Home_starters3_Minutes', 'Home_starters3_FG',\n",
       "       'Home_starters3_FGA', 'Home_starters3_FG_P', 'Home_starters3_P3',\n",
       "       'Home_starters3_P3A', 'Home_starters3_P3_P', 'Home_starters3_FT',\n",
       "       'Home_starters3_FTA', 'Home_starters3_FT_P', 'Home_starters3_ORB',\n",
       "       'Home_starters3_TRB', 'Home_starters3_AST', 'Home_starters3_STL',\n",
       "       'Home_starters3_BLK', 'Home_starters3_TOV', 'Home_starters3_PF',\n",
       "       'Home_starters3_PTS', 'Home_starters3_Game_Score',\n",
       "       'Home_starters3_+/-', 'Home_starters3_noplay_PointDiff',\n",
       "       'Home_starters4_PointDiff', 'Home_starters4_Game_Start',\n",
       "       'Home_starters4_Minutes', 'Home_starters4_FG',\n",
       "       'Home_starters4_FGA', 'Home_starters4_FG_P', 'Home_starters4_P3',\n",
       "       'Home_starters4_P3A', 'Home_starters4_P3_P', 'Home_starters4_FT',\n",
       "       'Home_starters4_FTA', 'Home_starters4_FT_P', 'Home_starters4_ORB',\n",
       "       'Home_starters4_TRB', 'Home_starters4_AST', 'Home_starters4_STL',\n",
       "       'Home_starters4_BLK', 'Home_starters4_TOV', 'Home_starters4_PF',\n",
       "       'Home_starters4_PTS', 'Home_starters4_Game_Score',\n",
       "       'Home_starters4_+/-', 'Home_starters4_noplay_PointDiff',\n",
       "       'Home_starters5_PointDiff', 'Home_starters5_Game_Start',\n",
       "       'Home_starters5_Minutes', 'Home_starters5_FG',\n",
       "       'Home_starters5_FGA', 'Home_starters5_FG_P', 'Home_starters5_P3',\n",
       "       'Home_starters5_P3A', 'Home_starters5_P3_P', 'Home_starters5_FT',\n",
       "       'Home_starters5_FTA', 'Home_starters5_FT_P', 'Home_starters5_ORB',\n",
       "       'Home_starters5_TRB', 'Home_starters5_AST', 'Home_starters5_STL',\n",
       "       'Home_starters5_BLK', 'Home_starters5_TOV', 'Home_starters5_PF',\n",
       "       'Home_starters5_PTS', 'Home_starters5_Game_Score',\n",
       "       'Home_starters5_+/-', 'Home_starters5_noplay_PointDiff',\n",
       "       'Away_starters1_PointDiff', 'Away_starters1_Game_Start',\n",
       "       'Away_starters1_Minutes', 'Away_starters1_FG',\n",
       "       'Away_starters1_FGA', 'Away_starters1_FG_P', 'Away_starters1_P3',\n",
       "       'Away_starters1_P3A', 'Away_starters1_P3_P', 'Away_starters1_FT',\n",
       "       'Away_starters1_FTA', 'Away_starters1_FT_P', 'Away_starters1_ORB',\n",
       "       'Away_starters1_TRB', 'Away_starters1_AST', 'Away_starters1_STL',\n",
       "       'Away_starters1_BLK', 'Away_starters1_TOV', 'Away_starters1_PF',\n",
       "       'Away_starters1_PTS', 'Away_starters1_Game_Score',\n",
       "       'Away_starters1_+/-', 'Away_starters1_noplay_PointDiff',\n",
       "       'Away_starters2_PointDiff', 'Away_starters2_Game_Start',\n",
       "       'Away_starters2_Minutes', 'Away_starters2_FG',\n",
       "       'Away_starters2_FGA', 'Away_starters2_FG_P', 'Away_starters2_P3',\n",
       "       'Away_starters2_P3A', 'Away_starters2_P3_P', 'Away_starters2_FT',\n",
       "       'Away_starters2_FTA', 'Away_starters2_FT_P', 'Away_starters2_ORB',\n",
       "       'Away_starters2_TRB', 'Away_starters2_AST', 'Away_starters2_STL',\n",
       "       'Away_starters2_BLK', 'Away_starters2_TOV', 'Away_starters2_PF',\n",
       "       'Away_starters2_PTS', 'Away_starters2_Game_Score',\n",
       "       'Away_starters2_+/-', 'Away_starters2_noplay_PointDiff',\n",
       "       'Away_starters3_PointDiff', 'Away_starters3_Game_Start',\n",
       "       'Away_starters3_Minutes', 'Away_starters3_FG',\n",
       "       'Away_starters3_FGA', 'Away_starters3_FG_P', 'Away_starters3_P3',\n",
       "       'Away_starters3_P3A', 'Away_starters3_P3_P', 'Away_starters3_FT',\n",
       "       'Away_starters3_FTA', 'Away_starters3_FT_P', 'Away_starters3_ORB',\n",
       "       'Away_starters3_TRB', 'Away_starters3_AST', 'Away_starters3_STL',\n",
       "       'Away_starters3_BLK', 'Away_starters3_TOV', 'Away_starters3_PF',\n",
       "       'Away_starters3_PTS', 'Away_starters3_Game_Score',\n",
       "       'Away_starters3_+/-', 'Away_starters3_noplay_PointDiff',\n",
       "       'Away_starters4_PointDiff', 'Away_starters4_Game_Start',\n",
       "       'Away_starters4_Minutes', 'Away_starters4_FG',\n",
       "       'Away_starters4_FGA', 'Away_starters4_FG_P', 'Away_starters4_P3',\n",
       "       'Away_starters4_P3A', 'Away_starters4_P3_P', 'Away_starters4_FT',\n",
       "       'Away_starters4_FTA', 'Away_starters4_FT_P', 'Away_starters4_ORB',\n",
       "       'Away_starters4_TRB', 'Away_starters4_AST', 'Away_starters4_STL',\n",
       "       'Away_starters4_BLK', 'Away_starters4_TOV', 'Away_starters4_PF',\n",
       "       'Away_starters4_PTS', 'Away_starters4_Game_Score',\n",
       "       'Away_starters4_+/-', 'Away_starters4_noplay_PointDiff',\n",
       "       'Away_starters5_PointDiff', 'Away_starters5_Game_Start',\n",
       "       'Away_starters5_Minutes', 'Away_starters5_FG',\n",
       "       'Away_starters5_FGA', 'Away_starters5_FG_P', 'Away_starters5_P3',\n",
       "       'Away_starters5_P3A', 'Away_starters5_P3_P', 'Away_starters5_FT',\n",
       "       'Away_starters5_FTA', 'Away_starters5_FT_P', 'Away_starters5_ORB',\n",
       "       'Away_starters5_TRB', 'Away_starters5_AST', 'Away_starters5_STL',\n",
       "       'Away_starters5_BLK', 'Away_starters5_TOV', 'Away_starters5_PF',\n",
       "       'Away_starters5_PTS', 'Away_starters5_Game_Score',\n",
       "       'Away_starters5_+/-', 'Away_starters5_noplay_PointDiff', '客隊ELO',\n",
       "       '主隊ELO', 'win'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886e9c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Home_starters1_PointDiff',\n",
       " 'Home_starters1_Game_Start',\n",
       " 'Home_starters1_Minutes',\n",
       " 'Home_starters1_FG',\n",
       " 'Home_starters1_FGA',\n",
       " 'Home_starters1_FG_P',\n",
       " 'Home_starters1_P3',\n",
       " 'Home_starters1_P3A',\n",
       " 'Home_starters1_P3_P',\n",
       " 'Home_starters1_FT',\n",
       " 'Home_starters1_FTA',\n",
       " 'Home_starters1_FT_P',\n",
       " 'Home_starters1_ORB',\n",
       " 'Home_starters1_TRB',\n",
       " 'Home_starters1_AST',\n",
       " 'Home_starters1_STL',\n",
       " 'Home_starters1_BLK',\n",
       " 'Home_starters1_TOV',\n",
       " 'Home_starters1_PF',\n",
       " 'Home_starters1_PTS',\n",
       " 'Home_starters1_Game_Score',\n",
       " 'Home_starters1_+/-',\n",
       " 'Home_starters1_noplay_PointDiff']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_starter_col = []\n",
    "for c in df.columns.values:\n",
    "    if ('starters1' in c) & ('Home' in c):\n",
    "        home_starter_col.append(c)\n",
    "home_starter_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac410128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Away_starters1_Minutes',\n",
       " 'Away_starters2_Minutes',\n",
       " 'Away_starters3_Minutes',\n",
       " 'Away_starters4_Minutes',\n",
       " 'Away_starters5_Minutes']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "away_col = []\n",
    "for c in df.columns.values:\n",
    "    if ('Minutes' in c) & ('Away' in c):\n",
    "        away_col.append(c)\n",
    "away_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a238a2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Away_starters1_PointDiff',\n",
       " 'Away_starters1_Game_Start',\n",
       " 'Away_starters1_Minutes',\n",
       " 'Away_starters1_FG',\n",
       " 'Away_starters1_FGA',\n",
       " 'Away_starters1_FG_P',\n",
       " 'Away_starters1_P3',\n",
       " 'Away_starters1_P3A',\n",
       " 'Away_starters1_P3_P',\n",
       " 'Away_starters1_FT',\n",
       " 'Away_starters1_FTA',\n",
       " 'Away_starters1_FT_P',\n",
       " 'Away_starters1_ORB',\n",
       " 'Away_starters1_TRB',\n",
       " 'Away_starters1_AST',\n",
       " 'Away_starters1_STL',\n",
       " 'Away_starters1_BLK',\n",
       " 'Away_starters1_TOV',\n",
       " 'Away_starters1_PF',\n",
       " 'Away_starters1_PTS',\n",
       " 'Away_starters1_Game_Score',\n",
       " 'Away_starters1_+/-',\n",
       " 'Away_starters1_noplay_PointDiff',\n",
       " 'Away_starters2_PointDiff',\n",
       " 'Away_starters2_Game_Start',\n",
       " 'Away_starters2_Minutes',\n",
       " 'Away_starters2_FG',\n",
       " 'Away_starters2_FGA',\n",
       " 'Away_starters2_FG_P',\n",
       " 'Away_starters2_P3',\n",
       " 'Away_starters2_P3A',\n",
       " 'Away_starters2_P3_P',\n",
       " 'Away_starters2_FT',\n",
       " 'Away_starters2_FTA',\n",
       " 'Away_starters2_FT_P',\n",
       " 'Away_starters2_ORB',\n",
       " 'Away_starters2_TRB',\n",
       " 'Away_starters2_AST',\n",
       " 'Away_starters2_STL',\n",
       " 'Away_starters2_BLK',\n",
       " 'Away_starters2_TOV',\n",
       " 'Away_starters2_PF',\n",
       " 'Away_starters2_PTS',\n",
       " 'Away_starters2_Game_Score',\n",
       " 'Away_starters2_+/-',\n",
       " 'Away_starters2_noplay_PointDiff',\n",
       " 'Away_starters3_PointDiff',\n",
       " 'Away_starters3_Game_Start',\n",
       " 'Away_starters3_Minutes',\n",
       " 'Away_starters3_FG',\n",
       " 'Away_starters3_FGA',\n",
       " 'Away_starters3_FG_P',\n",
       " 'Away_starters3_P3',\n",
       " 'Away_starters3_P3A',\n",
       " 'Away_starters3_P3_P',\n",
       " 'Away_starters3_FT',\n",
       " 'Away_starters3_FTA',\n",
       " 'Away_starters3_FT_P',\n",
       " 'Away_starters3_ORB',\n",
       " 'Away_starters3_TRB',\n",
       " 'Away_starters3_AST',\n",
       " 'Away_starters3_STL',\n",
       " 'Away_starters3_BLK',\n",
       " 'Away_starters3_TOV',\n",
       " 'Away_starters3_PF',\n",
       " 'Away_starters3_PTS',\n",
       " 'Away_starters3_Game_Score',\n",
       " 'Away_starters3_+/-',\n",
       " 'Away_starters3_noplay_PointDiff',\n",
       " 'Away_starters4_PointDiff',\n",
       " 'Away_starters4_Game_Start',\n",
       " 'Away_starters4_Minutes',\n",
       " 'Away_starters4_FG',\n",
       " 'Away_starters4_FGA',\n",
       " 'Away_starters4_FG_P',\n",
       " 'Away_starters4_P3',\n",
       " 'Away_starters4_P3A',\n",
       " 'Away_starters4_P3_P',\n",
       " 'Away_starters4_FT',\n",
       " 'Away_starters4_FTA',\n",
       " 'Away_starters4_FT_P',\n",
       " 'Away_starters4_ORB',\n",
       " 'Away_starters4_TRB',\n",
       " 'Away_starters4_AST',\n",
       " 'Away_starters4_STL',\n",
       " 'Away_starters4_BLK',\n",
       " 'Away_starters4_TOV',\n",
       " 'Away_starters4_PF',\n",
       " 'Away_starters4_PTS',\n",
       " 'Away_starters4_Game_Score',\n",
       " 'Away_starters4_+/-',\n",
       " 'Away_starters4_noplay_PointDiff',\n",
       " 'Away_starters5_PointDiff',\n",
       " 'Away_starters5_Game_Start',\n",
       " 'Away_starters5_Minutes',\n",
       " 'Away_starters5_FG',\n",
       " 'Away_starters5_FGA',\n",
       " 'Away_starters5_FG_P',\n",
       " 'Away_starters5_P3',\n",
       " 'Away_starters5_P3A',\n",
       " 'Away_starters5_P3_P',\n",
       " 'Away_starters5_FT',\n",
       " 'Away_starters5_FTA',\n",
       " 'Away_starters5_FT_P',\n",
       " 'Away_starters5_ORB',\n",
       " 'Away_starters5_TRB',\n",
       " 'Away_starters5_AST',\n",
       " 'Away_starters5_STL',\n",
       " 'Away_starters5_BLK',\n",
       " 'Away_starters5_TOV',\n",
       " 'Away_starters5_PF',\n",
       " 'Away_starters5_PTS',\n",
       " 'Away_starters5_Game_Score',\n",
       " 'Away_starters5_+/-',\n",
       " 'Away_starters5_noplay_PointDiff']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "away_starter_col = []\n",
    "for c in df.columns.values:\n",
    "    if ('starters' in c) & ('Away' in c):\n",
    "        away_starter_col.append(c)\n",
    "away_starter_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b247303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Home_starters5_Minutes    34.92\n",
       "Home_starters2_Minutes    34.60\n",
       "Home_starters3_Minutes    33.25\n",
       "Home_starters1_Minutes    32.49\n",
       "Home_starters4_Minutes    26.98\n",
       "Name: 2014-11-08 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts_m = df[home_col]\n",
    "a = starts_m.iloc[0].sort_values(ascending=False)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089bae0e",
   "metadata": {},
   "source": [
    "# 修改先發投手位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2ab06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#主隊\n",
    "df_starts_all = pd.DataFrame()\n",
    "starts_m = df[home_col]\n",
    "for i in range(len(starts_m)):\n",
    "    df_starters_one = pd.DataFrame()\n",
    "    df_starters_new = starts_m.iloc[i].sort_values(ascending=False)\n",
    "    count = 1\n",
    "    for s in df_starters_new.index:\n",
    "        starter_number = s.replace(\"_Minutes\",\"\")\n",
    "        home_starter_col = []\n",
    "        for c in df.columns.values:\n",
    "            if (starter_number in c):\n",
    "                home_starter_col.append(c)\n",
    "        starter_number = starter_number.replace(\"Home_starters\",\"\")\n",
    "        starts_df  = df[home_starter_col].iloc[i:i+1]     \n",
    "        starts_df.columns = starts_df.columns.str.replace(f's{starter_number}_',f's{str(count)}_')\n",
    "        df_starters_one = pd.concat([df_starters_one,starts_df],axis=1)\n",
    "        count += 1\n",
    "    df_starts_all = df_starts_all.append(df_starters_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b0ee15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home_starters1_PointDiff</th>\n",
       "      <th>Home_starters1_Game_Start</th>\n",
       "      <th>Home_starters1_Minutes</th>\n",
       "      <th>Home_starters1_FG</th>\n",
       "      <th>Home_starters1_FGA</th>\n",
       "      <th>Home_starters1_FG_P</th>\n",
       "      <th>Home_starters1_P3</th>\n",
       "      <th>Home_starters1_P3A</th>\n",
       "      <th>Home_starters1_P3_P</th>\n",
       "      <th>Home_starters1_FT</th>\n",
       "      <th>...</th>\n",
       "      <th>Home_starters5_TRB</th>\n",
       "      <th>Home_starters5_AST</th>\n",
       "      <th>Home_starters5_STL</th>\n",
       "      <th>Home_starters5_BLK</th>\n",
       "      <th>Home_starters5_TOV</th>\n",
       "      <th>Home_starters5_PF</th>\n",
       "      <th>Home_starters5_PTS</th>\n",
       "      <th>Home_starters5_Game_Score</th>\n",
       "      <th>Home_starters5_+/-</th>\n",
       "      <th>Home_starters5_noplay_PointDiff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>-5.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.92</td>\n",
       "      <td>4.9</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.1</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>11.90</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.29</td>\n",
       "      <td>6.1</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.8</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>-5.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.88</td>\n",
       "      <td>6.2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.7</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>13.8</td>\n",
       "      <td>10.31</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>-1.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>30.74</td>\n",
       "      <td>4.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.2</td>\n",
       "      <td>5.35</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.91</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.01</td>\n",
       "      <td>-3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>-6.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.14</td>\n",
       "      <td>9.6</td>\n",
       "      <td>18.4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.01</td>\n",
       "      <td>7.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>11.6</td>\n",
       "      <td>8.25</td>\n",
       "      <td>-8.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.53</td>\n",
       "      <td>8.9</td>\n",
       "      <td>18.4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>10.7</td>\n",
       "      <td>9.49</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>4.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.95</td>\n",
       "      <td>7.1</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.4</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>9.50</td>\n",
       "      <td>-6.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>-1.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.59</td>\n",
       "      <td>11.8</td>\n",
       "      <td>18.2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.01</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>15.8</td>\n",
       "      <td>11.61</td>\n",
       "      <td>-6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>-5.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.76</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.8</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>8.4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>18.8</td>\n",
       "      <td>13.41</td>\n",
       "      <td>-3.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7357 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Home_starters1_PointDiff  Home_starters1_Game_Start  \\\n",
       "Matchtime                                                         \n",
       "2014-11-08                      -5.6                        1.0   \n",
       "2014-11-08                      -0.5                        1.0   \n",
       "2014-11-08                      -5.7                        1.0   \n",
       "2014-11-09                      -1.9                        0.8   \n",
       "2014-11-09                       1.4                        1.0   \n",
       "...                              ...                        ...   \n",
       "2022-10-27                      -6.6                        1.0   \n",
       "2022-10-27                       1.1                        1.0   \n",
       "2022-10-27                       4.6                        1.0   \n",
       "2022-10-27                      -1.9                        1.0   \n",
       "2022-10-27                      -5.7                        1.0   \n",
       "\n",
       "            Home_starters1_Minutes  Home_starters1_FG  Home_starters1_FGA  \\\n",
       "Matchtime                                                                   \n",
       "2014-11-08                   34.92                4.9                10.3   \n",
       "2014-11-08                   35.29                6.1                13.8   \n",
       "2014-11-08                   35.88                6.2                13.8   \n",
       "2014-11-09                   30.74                4.6                11.4   \n",
       "2014-11-09                   34.91                6.7                14.5   \n",
       "...                            ...                ...                 ...   \n",
       "2022-10-27                   35.14                9.6                18.4   \n",
       "2022-10-27                   34.53                8.9                18.4   \n",
       "2022-10-27                   31.95                7.1                14.2   \n",
       "2022-10-27                   35.59               11.8                18.2   \n",
       "2022-10-27                   35.76                9.0                19.3   \n",
       "\n",
       "            Home_starters1_FG_P  Home_starters1_P3  Home_starters1_P3A  \\\n",
       "Matchtime                                                                \n",
       "2014-11-08                 0.04                0.5                 1.9   \n",
       "2014-11-08                 0.04                0.5                 2.9   \n",
       "2014-11-08                 0.04                1.4                 5.0   \n",
       "2014-11-09                 0.03                1.4                 3.9   \n",
       "2014-11-09                 0.04                1.1                 3.6   \n",
       "...                         ...                ...                 ...   \n",
       "2022-10-27                 0.04                0.6                 1.6   \n",
       "2022-10-27                 0.04                2.9                 8.6   \n",
       "2022-10-27                 0.05                2.5                 6.6   \n",
       "2022-10-27                 0.06                0.5                 2.3   \n",
       "2022-10-27                 0.04                3.8                10.1   \n",
       "\n",
       "            Home_starters1_P3_P  Home_starters1_FT  ...  Home_starters5_TRB  \\\n",
       "Matchtime                                           ...                       \n",
       "2014-11-08                 0.01                1.0  ...                 5.1   \n",
       "2014-11-08                 0.02                2.8  ...                 3.7   \n",
       "2014-11-08                 0.02                5.1  ...                 5.7   \n",
       "2014-11-09                 0.03                1.9  ...                 5.0   \n",
       "2014-11-09                 0.03                4.6  ...                 1.6   \n",
       "...                         ...                ...  ...                 ...   \n",
       "2022-10-27                 2.01                7.5  ...                 2.5   \n",
       "2022-10-27                 0.03                3.3  ...                 2.3   \n",
       "2022-10-27                 0.04                3.4  ...                 2.8   \n",
       "2022-10-27                 1.01                6.0  ...                 2.9   \n",
       "2022-10-27                 0.03                8.4  ...                 3.5   \n",
       "\n",
       "            Home_starters5_AST  Home_starters5_STL  Home_starters5_BLK  \\\n",
       "Matchtime                                                                \n",
       "2014-11-08                 7.2                 2.0                 0.7   \n",
       "2014-11-08                 5.8                 0.2                 0.6   \n",
       "2014-11-08                 9.3                 1.0                 0.3   \n",
       "2014-11-09                 6.8                 1.4                 0.6   \n",
       "2014-11-09                 1.8                 3.3                 1.0   \n",
       "...                        ...                 ...                 ...   \n",
       "2022-10-27                 3.4                 1.1                 0.5   \n",
       "2022-10-27                 3.3                 0.8                 1.1   \n",
       "2022-10-27                 4.1                 4.0                 1.0   \n",
       "2022-10-27                 3.4                 2.3                 1.1   \n",
       "2022-10-27                 3.9                 1.6                 0.6   \n",
       "\n",
       "            Home_starters5_TOV  Home_starters5_PF  Home_starters5_PTS  \\\n",
       "Matchtime                                                               \n",
       "2014-11-08                 0.1                1.9                 4.0   \n",
       "2014-11-08                 1.0                0.9                 2.1   \n",
       "2014-11-08                 0.5                2.1                 2.5   \n",
       "2014-11-09                 0.1                2.5                 2.5   \n",
       "2014-11-09                 0.0                1.3                 1.8   \n",
       "...                        ...                ...                 ...   \n",
       "2022-10-27                 0.6                1.5                 1.5   \n",
       "2022-10-27                 1.2                0.9                 2.4   \n",
       "2022-10-27                 0.4                2.3                 3.3   \n",
       "2022-10-27                 0.2                1.5                 2.6   \n",
       "2022-10-27                 0.6                1.3                 1.8   \n",
       "\n",
       "            Home_starters5_Game_Score  Home_starters5_+/-  \\\n",
       "Matchtime                                                   \n",
       "2014-11-08                       15.6               11.90   \n",
       "2014-11-08                        6.3                5.86   \n",
       "2014-11-08                       13.8               10.31   \n",
       "2014-11-09                        8.2                5.35   \n",
       "2014-11-09                        6.3                5.01   \n",
       "...                               ...                 ...   \n",
       "2022-10-27                       11.6                8.25   \n",
       "2022-10-27                       10.7                9.49   \n",
       "2022-10-27                       11.2                9.50   \n",
       "2022-10-27                       15.8               11.61   \n",
       "2022-10-27                       18.8               13.41   \n",
       "\n",
       "            Home_starters5_noplay_PointDiff  \n",
       "Matchtime                                    \n",
       "2014-11-08                             5.00  \n",
       "2014-11-08                             0.80  \n",
       "2014-11-08                             2.10  \n",
       "2014-11-09                             7.00  \n",
       "2014-11-09                            -3.67  \n",
       "...                                     ...  \n",
       "2022-10-27                            -8.10  \n",
       "2022-10-27                             1.20  \n",
       "2022-10-27                            -6.10  \n",
       "2022-10-27                            -6.50  \n",
       "2022-10-27                            -3.80  \n",
       "\n",
       "[7357 rows x 115 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_starts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2415cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_starts_all.columns:\n",
    "    df[c] = df_starts_all[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f1b742c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>HomeScore</th>\n",
       "      <th>Away</th>\n",
       "      <th>AwayScore</th>\n",
       "      <th>讓分</th>\n",
       "      <th>總分</th>\n",
       "      <th>Eventcode_x</th>\n",
       "      <th>主勝(初)</th>\n",
       "      <th>客勝(初)</th>\n",
       "      <th>主勝率(初)</th>\n",
       "      <th>...</th>\n",
       "      <th>Away_starters5_BLK</th>\n",
       "      <th>Away_starters5_TOV</th>\n",
       "      <th>Away_starters5_PF</th>\n",
       "      <th>Away_starters5_PTS</th>\n",
       "      <th>Away_starters5_Game_Score</th>\n",
       "      <th>Away_starters5_+/-</th>\n",
       "      <th>Away_starters5_noplay_PointDiff</th>\n",
       "      <th>客隊ELO</th>\n",
       "      <th>主隊ELO</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>BOS</td>\n",
       "      <td>101</td>\n",
       "      <td>IND</td>\n",
       "      <td>98</td>\n",
       "      <td>5.5</td>\n",
       "      <td>191.0</td>\n",
       "      <td>188660</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.45</td>\n",
       "      <td>61.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>2257.9675</td>\n",
       "      <td>1970.8143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>PHO</td>\n",
       "      <td>112</td>\n",
       "      <td>SAC</td>\n",
       "      <td>114</td>\n",
       "      <td>5.0</td>\n",
       "      <td>208.5</td>\n",
       "      <td>188656</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.51</td>\n",
       "      <td>62.63</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>15.4</td>\n",
       "      <td>10.45</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>1918.5838</td>\n",
       "      <td>2154.6768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>UTA</td>\n",
       "      <td>82</td>\n",
       "      <td>DAL</td>\n",
       "      <td>105</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>205.5</td>\n",
       "      <td>188655</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.65</td>\n",
       "      <td>43.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.69</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2220.9141</td>\n",
       "      <td>1920.5684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>IND</td>\n",
       "      <td>90</td>\n",
       "      <td>WAS</td>\n",
       "      <td>97</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>183.5</td>\n",
       "      <td>188666</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.59</td>\n",
       "      <td>40.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2094.4555</td>\n",
       "      <td>2254.1106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>MIA</td>\n",
       "      <td>102</td>\n",
       "      <td>MIN</td>\n",
       "      <td>92</td>\n",
       "      <td>7.5</td>\n",
       "      <td>205.0</td>\n",
       "      <td>188667</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.35</td>\n",
       "      <td>71.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>27.5</td>\n",
       "      <td>20.63</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2080.6630</td>\n",
       "      <td>2355.2815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>CHI</td>\n",
       "      <td>124</td>\n",
       "      <td>IND</td>\n",
       "      <td>109</td>\n",
       "      <td>7.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484626</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.46</td>\n",
       "      <td>72.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>11.03</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1942.8995</td>\n",
       "      <td>2193.6864</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>MIN</td>\n",
       "      <td>134</td>\n",
       "      <td>SAS</td>\n",
       "      <td>122</td>\n",
       "      <td>9.5</td>\n",
       "      <td>235.5</td>\n",
       "      <td>484627</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.90</td>\n",
       "      <td>75.65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.1</td>\n",
       "      <td>11.6</td>\n",
       "      <td>9.00</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>2113.3237</td>\n",
       "      <td>2066.1966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>UTA</td>\n",
       "      <td>109</td>\n",
       "      <td>HOU</td>\n",
       "      <td>101</td>\n",
       "      <td>8.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484628</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.79</td>\n",
       "      <td>65.93</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.84</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>1794.1439</td>\n",
       "      <td>2285.1027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>DEN</td>\n",
       "      <td>110</td>\n",
       "      <td>LAL</td>\n",
       "      <td>99</td>\n",
       "      <td>5.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484629</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.97</td>\n",
       "      <td>68.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>10.7</td>\n",
       "      <td>7.87</td>\n",
       "      <td>-6.7</td>\n",
       "      <td>2088.1050</td>\n",
       "      <td>2225.9103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>POR</td>\n",
       "      <td>98</td>\n",
       "      <td>MIA</td>\n",
       "      <td>119</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>221.5</td>\n",
       "      <td>484630</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.95</td>\n",
       "      <td>51.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.90</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2238.5712</td>\n",
       "      <td>2060.2239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7357 rows × 568 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Home  HomeScore Away  AwayScore   讓分     總分  Eventcode_x  主勝(初)  \\\n",
       "Matchtime                                                                    \n",
       "2014-11-08  BOS        101  IND         98  5.5  191.0       188660   1.53   \n",
       "2014-11-08  PHO        112  SAC        114  5.0  208.5       188656   1.50   \n",
       "2014-11-08  UTA         82  DAL        105 -2.5  205.5       188655   2.17   \n",
       "2014-11-09  IND         90  WAS         97 -3.5  183.5       188666   2.33   \n",
       "2014-11-09  MIA        102  MIN         92  7.5  205.0       188667   1.32   \n",
       "...         ...        ...  ...        ...  ...    ...          ...    ...   \n",
       "2022-10-27  CHI        124  IND        109  7.5  232.5       484626   1.30   \n",
       "2022-10-27  MIN        134  SAS        122  9.5  235.5       484627   1.25   \n",
       "2022-10-27  UTA        109  HOU        101  8.5  232.5       484628   1.44   \n",
       "2022-10-27  DEN        110  LAL         99  5.5  232.5       484629   1.39   \n",
       "2022-10-27  POR         98  MIA        119 -2.5  221.5       484630   1.85   \n",
       "\n",
       "            客勝(初)  主勝率(初)  ...  Away_starters5_BLK  Away_starters5_TOV  \\\n",
       "Matchtime                  ...                                           \n",
       "2014-11-08   2.45   61.56  ...                 0.5                 0.0   \n",
       "2014-11-08   2.51   62.63  ...                 1.3                 0.2   \n",
       "2014-11-08   1.65   43.24  ...                 0.4                 0.1   \n",
       "2014-11-09   1.59   40.60  ...                 0.0                 0.3   \n",
       "2014-11-09   3.35   71.53  ...                 0.6                 0.5   \n",
       "...           ...     ...  ...                 ...                 ...   \n",
       "2022-10-27   3.46   72.59  ...                 0.9                 0.3   \n",
       "2022-10-27   3.90   75.65  ...                 0.8                 0.7   \n",
       "2022-10-27   2.79   65.93  ...                 1.0                 0.1   \n",
       "2022-10-27   2.97   68.05  ...                 0.8                 0.6   \n",
       "2022-10-27   1.95   51.34  ...                 0.8                 0.2   \n",
       "\n",
       "            Away_starters5_PF  Away_starters5_PTS  Away_starters5_Game_Score  \\\n",
       "Matchtime                                                                      \n",
       "2014-11-08                0.8                 1.9                        4.3   \n",
       "2014-11-08                1.3                 1.8                       15.4   \n",
       "2014-11-08                1.4                 1.8                        4.5   \n",
       "2014-11-09                1.0                 1.1                        3.9   \n",
       "2014-11-09                2.3                 2.1                       27.5   \n",
       "...                       ...                 ...                        ...   \n",
       "2022-10-27                0.6                 4.0                       13.5   \n",
       "2022-10-27                2.2                 3.1                       11.6   \n",
       "2022-10-27                1.2                 3.9                        9.0   \n",
       "2022-10-27                1.9                 1.9                       10.7   \n",
       "2022-10-27                1.0                 2.9                       10.0   \n",
       "\n",
       "            Away_starters5_+/-  Away_starters5_noplay_PointDiff      客隊ELO  \\\n",
       "Matchtime                                                                    \n",
       "2014-11-08                3.70                             -8.2  2257.9675   \n",
       "2014-11-08               10.45                             -3.9  1918.5838   \n",
       "2014-11-08                2.69                              6.6  2220.9141   \n",
       "2014-11-09                3.29                              3.8  2094.4555   \n",
       "2014-11-09               20.63                              0.8  2080.6630   \n",
       "...                        ...                              ...        ...   \n",
       "2022-10-27               11.03                              6.9  1942.8995   \n",
       "2022-10-27                9.00                             -7.0  2113.3237   \n",
       "2022-10-27                6.84                            -10.3  1794.1439   \n",
       "2022-10-27                7.87                             -6.7  2088.1050   \n",
       "2022-10-27                7.90                              5.9  2238.5712   \n",
       "\n",
       "                主隊ELO  win  \n",
       "Matchtime                   \n",
       "2014-11-08  1970.8143    1  \n",
       "2014-11-08  2154.6768    0  \n",
       "2014-11-08  1920.5684    0  \n",
       "2014-11-09  2254.1106    0  \n",
       "2014-11-09  2355.2815    1  \n",
       "...               ...  ...  \n",
       "2022-10-27  2193.6864    1  \n",
       "2022-10-27  2066.1966    1  \n",
       "2022-10-27  2285.1027    1  \n",
       "2022-10-27  2225.9103    1  \n",
       "2022-10-27  2060.2239    0  \n",
       "\n",
       "[7357 rows x 568 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f4b38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#客隊\n",
    "df_starts_all = pd.DataFrame()\n",
    "starts_m = df[away_col]\n",
    "for i in range(len(starts_m)):\n",
    "    df_starters_one = pd.DataFrame()\n",
    "    df_starters_new = starts_m.iloc[i].sort_values(ascending=False)\n",
    "    count = 1\n",
    "    for s in df_starters_new.index:\n",
    "        starter_number = s.replace(\"_Minutes\",\"\")\n",
    "        away_starter_col = []\n",
    "        for c in df.columns.values:\n",
    "            if (starter_number in c):\n",
    "                away_starter_col.append(c)\n",
    "        starter_number = starter_number.replace(\"Away_starters\",\"\")\n",
    "        starts_df  = df[away_starter_col].iloc[i:i+1]     \n",
    "        starts_df.columns = starts_df.columns.str.replace(f's{starter_number}_',f's{str(count)}_')\n",
    "        df_starters_one = pd.concat([df_starters_one,starts_df],axis=1)\n",
    "        count += 1\n",
    "    df_starts_all = df_starts_all.append(df_starters_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cbaa469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Away_starters1_PointDiff</th>\n",
       "      <th>Away_starters1_Game_Start</th>\n",
       "      <th>Away_starters1_Minutes</th>\n",
       "      <th>Away_starters1_FG</th>\n",
       "      <th>Away_starters1_FGA</th>\n",
       "      <th>Away_starters1_FG_P</th>\n",
       "      <th>Away_starters1_P3</th>\n",
       "      <th>Away_starters1_P3A</th>\n",
       "      <th>Away_starters1_P3_P</th>\n",
       "      <th>Away_starters1_FT</th>\n",
       "      <th>...</th>\n",
       "      <th>Away_starters5_TRB</th>\n",
       "      <th>Away_starters5_AST</th>\n",
       "      <th>Away_starters5_STL</th>\n",
       "      <th>Away_starters5_BLK</th>\n",
       "      <th>Away_starters5_TOV</th>\n",
       "      <th>Away_starters5_PF</th>\n",
       "      <th>Away_starters5_PTS</th>\n",
       "      <th>Away_starters5_Game_Score</th>\n",
       "      <th>Away_starters5_+/-</th>\n",
       "      <th>Away_starters5_noplay_PointDiff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>-14.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.09</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>-20.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.84</td>\n",
       "      <td>5.1</td>\n",
       "      <td>10.9</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>5.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.51</td>\n",
       "      <td>2.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>1.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>21.81</td>\n",
       "      <td>3.3</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>2.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.66</td>\n",
       "      <td>9.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.8</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.94</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.7</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.9</td>\n",
       "      <td>7.93</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>-7.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.38</td>\n",
       "      <td>7.5</td>\n",
       "      <td>15.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>9.83</td>\n",
       "      <td>-7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>-10.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.71</td>\n",
       "      <td>7.9</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.2</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.7</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.84</td>\n",
       "      <td>-10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.32</td>\n",
       "      <td>13.2</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.02</td>\n",
       "      <td>6.6</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>11.8</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.03</td>\n",
       "      <td>7.9</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.8</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6.88</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7357 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Away_starters1_PointDiff  Away_starters1_Game_Start  \\\n",
       "Matchtime                                                         \n",
       "2014-11-08                     -14.1                        0.0   \n",
       "2014-11-08                     -20.7                        0.0   \n",
       "2014-11-08                       5.8                        1.0   \n",
       "2014-11-09                       1.1                        0.2   \n",
       "2014-11-09                       2.7                        1.0   \n",
       "...                              ...                        ...   \n",
       "2022-10-27                       3.2                        1.0   \n",
       "2022-10-27                      -7.9                        1.0   \n",
       "2022-10-27                     -10.8                        1.0   \n",
       "2022-10-27                       1.3                        1.0   \n",
       "2022-10-27                       2.2                        1.0   \n",
       "\n",
       "            Away_starters1_Minutes  Away_starters1_FG  Away_starters1_FGA  \\\n",
       "Matchtime                                                                   \n",
       "2014-11-08                   20.09                2.8                 6.2   \n",
       "2014-11-08                   25.84                5.1                10.9   \n",
       "2014-11-08                   28.51                2.8                 7.1   \n",
       "2014-11-09                   21.81                3.3                 8.7   \n",
       "2014-11-09                   37.66                9.5                18.0   \n",
       "...                            ...                ...                 ...   \n",
       "2022-10-27                   33.94                8.0                15.8   \n",
       "2022-10-27                   30.38                7.5                15.1   \n",
       "2022-10-27                   34.71                7.9                16.5   \n",
       "2022-10-27                   35.32               13.2                23.3   \n",
       "2022-10-27                   37.03                7.9                17.8   \n",
       "\n",
       "            Away_starters1_FG_P  Away_starters1_P3  Away_starters1_P3A  \\\n",
       "Matchtime                                                                \n",
       "2014-11-08                 1.03                0.6                 1.6   \n",
       "2014-11-08                 1.04                3.2                 8.2   \n",
       "2014-11-08                 0.03                0.5                 2.2   \n",
       "2014-11-09                 0.04                1.1                 3.0   \n",
       "2014-11-09                 0.05                3.8                 7.4   \n",
       "...                         ...                ...                 ...   \n",
       "2022-10-27                 0.05                3.7                 8.3   \n",
       "2022-10-27                 0.05                1.9                 5.1   \n",
       "2022-10-27                 0.04                3.2                 8.3   \n",
       "2022-10-27                 0.05                1.3                 5.4   \n",
       "2022-10-27                 0.04                3.8                 9.7   \n",
       "\n",
       "            Away_starters1_P3_P  Away_starters1_FT  ...  Away_starters5_TRB  \\\n",
       "Matchtime                                           ...                       \n",
       "2014-11-08                 0.02                1.1  ...                 0.2   \n",
       "2014-11-08                 1.03                2.0  ...                 2.8   \n",
       "2014-11-08                 0.02                0.6  ...                 1.3   \n",
       "2014-11-09                 1.02                1.0  ...                 0.6   \n",
       "2014-11-09                 0.04                4.7  ...                 1.0   \n",
       "...                         ...                ...  ...                 ...   \n",
       "2022-10-27                 0.04                3.0  ...                 3.1   \n",
       "2022-10-27                 0.03                5.1  ...                 2.1   \n",
       "2022-10-27                 0.03                2.7  ...                 2.1   \n",
       "2022-10-27                 0.02                6.6  ...                 2.0   \n",
       "2022-10-27                 0.03                3.2  ...                 3.8   \n",
       "\n",
       "            Away_starters5_AST  Away_starters5_STL  Away_starters5_BLK  \\\n",
       "Matchtime                                                                \n",
       "2014-11-08                 0.2                 0.5                 0.0   \n",
       "2014-11-08                 4.6                 1.6                 0.2   \n",
       "2014-11-08                 2.0                 0.0                 0.2   \n",
       "2014-11-09                 0.8                 0.4                 0.6   \n",
       "2014-11-09                 1.7                 0.8                 0.1   \n",
       "...                        ...                 ...                 ...   \n",
       "2022-10-27                 5.3                 0.7                 0.4   \n",
       "2022-10-27                 3.0                 3.7                 1.0   \n",
       "2022-10-27                 4.2                 2.5                 1.0   \n",
       "2022-10-27                 2.4                 0.9                 0.7   \n",
       "2022-10-27                 4.9                 1.7                 0.7   \n",
       "\n",
       "            Away_starters5_TOV  Away_starters5_PF  Away_starters5_PTS  \\\n",
       "Matchtime                                                               \n",
       "2014-11-08                 0.0                0.3                 0.6   \n",
       "2014-11-08                 0.3                2.1                 2.7   \n",
       "2014-11-08                 0.1                0.2                 1.0   \n",
       "2014-11-09                 0.1                0.2                 0.8   \n",
       "2014-11-09                 0.0                0.5                 1.6   \n",
       "...                        ...                ...                 ...   \n",
       "2022-10-27                 0.6                1.0                 2.2   \n",
       "2022-10-27                 0.2                1.3                 1.5   \n",
       "2022-10-27                 0.1                1.2                 3.9   \n",
       "2022-10-27                 0.2                0.7                 1.7   \n",
       "2022-10-27                 0.4                0.9                 2.0   \n",
       "\n",
       "            Away_starters5_Game_Score  Away_starters5_+/-  \\\n",
       "Matchtime                                                   \n",
       "2014-11-08                        1.1                0.25   \n",
       "2014-11-08                        8.1                5.85   \n",
       "2014-11-08                        0.7                1.04   \n",
       "2014-11-09                        2.6                2.27   \n",
       "2014-11-09                        4.0                1.81   \n",
       "...                               ...                 ...   \n",
       "2022-10-27                        8.9                7.93   \n",
       "2022-10-27                       11.7                9.83   \n",
       "2022-10-27                        9.0                6.84   \n",
       "2022-10-27                       11.8                7.60   \n",
       "2022-10-27                        8.7                6.88   \n",
       "\n",
       "            Away_starters5_noplay_PointDiff  \n",
       "Matchtime                                    \n",
       "2014-11-08                              0.9  \n",
       "2014-11-08                             -0.3  \n",
       "2014-11-08                             -4.1  \n",
       "2014-11-09                             -2.0  \n",
       "2014-11-09                             -2.0  \n",
       "...                                     ...  \n",
       "2022-10-27                             -2.0  \n",
       "2022-10-27                             -7.7  \n",
       "2022-10-27                            -10.3  \n",
       "2022-10-27                              0.3  \n",
       "2022-10-27                              0.9  \n",
       "\n",
       "[7357 rows x 115 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_starts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9e21672",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_starts_all.columns:\n",
    "    df[c] = df_starts_all[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1d51296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>HomeScore</th>\n",
       "      <th>Away</th>\n",
       "      <th>AwayScore</th>\n",
       "      <th>讓分</th>\n",
       "      <th>總分</th>\n",
       "      <th>Eventcode_x</th>\n",
       "      <th>主勝(初)</th>\n",
       "      <th>客勝(初)</th>\n",
       "      <th>主勝率(初)</th>\n",
       "      <th>...</th>\n",
       "      <th>Away_starters5_BLK</th>\n",
       "      <th>Away_starters5_TOV</th>\n",
       "      <th>Away_starters5_PF</th>\n",
       "      <th>Away_starters5_PTS</th>\n",
       "      <th>Away_starters5_Game_Score</th>\n",
       "      <th>Away_starters5_+/-</th>\n",
       "      <th>Away_starters5_noplay_PointDiff</th>\n",
       "      <th>客隊ELO</th>\n",
       "      <th>主隊ELO</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>BOS</td>\n",
       "      <td>101</td>\n",
       "      <td>IND</td>\n",
       "      <td>98</td>\n",
       "      <td>5.5</td>\n",
       "      <td>191.0</td>\n",
       "      <td>188660</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.45</td>\n",
       "      <td>61.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2257.9675</td>\n",
       "      <td>1970.8143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>PHO</td>\n",
       "      <td>112</td>\n",
       "      <td>SAC</td>\n",
       "      <td>114</td>\n",
       "      <td>5.0</td>\n",
       "      <td>208.5</td>\n",
       "      <td>188656</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.51</td>\n",
       "      <td>62.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>1918.5838</td>\n",
       "      <td>2154.6768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>UTA</td>\n",
       "      <td>82</td>\n",
       "      <td>DAL</td>\n",
       "      <td>105</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>205.5</td>\n",
       "      <td>188655</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.65</td>\n",
       "      <td>43.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-4.1</td>\n",
       "      <td>2220.9141</td>\n",
       "      <td>1920.5684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>IND</td>\n",
       "      <td>90</td>\n",
       "      <td>WAS</td>\n",
       "      <td>97</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>183.5</td>\n",
       "      <td>188666</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.59</td>\n",
       "      <td>40.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2094.4555</td>\n",
       "      <td>2254.1106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>MIA</td>\n",
       "      <td>102</td>\n",
       "      <td>MIN</td>\n",
       "      <td>92</td>\n",
       "      <td>7.5</td>\n",
       "      <td>205.0</td>\n",
       "      <td>188667</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.35</td>\n",
       "      <td>71.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2080.6630</td>\n",
       "      <td>2355.2815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>CHI</td>\n",
       "      <td>124</td>\n",
       "      <td>IND</td>\n",
       "      <td>109</td>\n",
       "      <td>7.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484626</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.46</td>\n",
       "      <td>72.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.9</td>\n",
       "      <td>7.93</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1942.8995</td>\n",
       "      <td>2193.6864</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>MIN</td>\n",
       "      <td>134</td>\n",
       "      <td>SAS</td>\n",
       "      <td>122</td>\n",
       "      <td>9.5</td>\n",
       "      <td>235.5</td>\n",
       "      <td>484627</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.90</td>\n",
       "      <td>75.65</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>9.83</td>\n",
       "      <td>-7.7</td>\n",
       "      <td>2113.3237</td>\n",
       "      <td>2066.1966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>UTA</td>\n",
       "      <td>109</td>\n",
       "      <td>HOU</td>\n",
       "      <td>101</td>\n",
       "      <td>8.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484628</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.79</td>\n",
       "      <td>65.93</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.84</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>1794.1439</td>\n",
       "      <td>2285.1027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>DEN</td>\n",
       "      <td>110</td>\n",
       "      <td>LAL</td>\n",
       "      <td>99</td>\n",
       "      <td>5.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484629</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.97</td>\n",
       "      <td>68.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>11.8</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2088.1050</td>\n",
       "      <td>2225.9103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>POR</td>\n",
       "      <td>98</td>\n",
       "      <td>MIA</td>\n",
       "      <td>119</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>221.5</td>\n",
       "      <td>484630</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.95</td>\n",
       "      <td>51.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6.88</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2238.5712</td>\n",
       "      <td>2060.2239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7357 rows × 568 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Home  HomeScore Away  AwayScore   讓分     總分  Eventcode_x  主勝(初)  \\\n",
       "Matchtime                                                                    \n",
       "2014-11-08  BOS        101  IND         98  5.5  191.0       188660   1.53   \n",
       "2014-11-08  PHO        112  SAC        114  5.0  208.5       188656   1.50   \n",
       "2014-11-08  UTA         82  DAL        105 -2.5  205.5       188655   2.17   \n",
       "2014-11-09  IND         90  WAS         97 -3.5  183.5       188666   2.33   \n",
       "2014-11-09  MIA        102  MIN         92  7.5  205.0       188667   1.32   \n",
       "...         ...        ...  ...        ...  ...    ...          ...    ...   \n",
       "2022-10-27  CHI        124  IND        109  7.5  232.5       484626   1.30   \n",
       "2022-10-27  MIN        134  SAS        122  9.5  235.5       484627   1.25   \n",
       "2022-10-27  UTA        109  HOU        101  8.5  232.5       484628   1.44   \n",
       "2022-10-27  DEN        110  LAL         99  5.5  232.5       484629   1.39   \n",
       "2022-10-27  POR         98  MIA        119 -2.5  221.5       484630   1.85   \n",
       "\n",
       "            客勝(初)  主勝率(初)  ...  Away_starters5_BLK  Away_starters5_TOV  \\\n",
       "Matchtime                  ...                                           \n",
       "2014-11-08   2.45   61.56  ...                 0.0                 0.0   \n",
       "2014-11-08   2.51   62.63  ...                 0.2                 0.3   \n",
       "2014-11-08   1.65   43.24  ...                 0.2                 0.1   \n",
       "2014-11-09   1.59   40.60  ...                 0.6                 0.1   \n",
       "2014-11-09   3.35   71.53  ...                 0.1                 0.0   \n",
       "...           ...     ...  ...                 ...                 ...   \n",
       "2022-10-27   3.46   72.59  ...                 0.4                 0.6   \n",
       "2022-10-27   3.90   75.65  ...                 1.0                 0.2   \n",
       "2022-10-27   2.79   65.93  ...                 1.0                 0.1   \n",
       "2022-10-27   2.97   68.05  ...                 0.7                 0.2   \n",
       "2022-10-27   1.95   51.34  ...                 0.7                 0.4   \n",
       "\n",
       "            Away_starters5_PF  Away_starters5_PTS  Away_starters5_Game_Score  \\\n",
       "Matchtime                                                                      \n",
       "2014-11-08                0.3                 0.6                        1.1   \n",
       "2014-11-08                2.1                 2.7                        8.1   \n",
       "2014-11-08                0.2                 1.0                        0.7   \n",
       "2014-11-09                0.2                 0.8                        2.6   \n",
       "2014-11-09                0.5                 1.6                        4.0   \n",
       "...                       ...                 ...                        ...   \n",
       "2022-10-27                1.0                 2.2                        8.9   \n",
       "2022-10-27                1.3                 1.5                       11.7   \n",
       "2022-10-27                1.2                 3.9                        9.0   \n",
       "2022-10-27                0.7                 1.7                       11.8   \n",
       "2022-10-27                0.9                 2.0                        8.7   \n",
       "\n",
       "            Away_starters5_+/-  Away_starters5_noplay_PointDiff      客隊ELO  \\\n",
       "Matchtime                                                                    \n",
       "2014-11-08                0.25                              0.9  2257.9675   \n",
       "2014-11-08                5.85                             -0.3  1918.5838   \n",
       "2014-11-08                1.04                             -4.1  2220.9141   \n",
       "2014-11-09                2.27                             -2.0  2094.4555   \n",
       "2014-11-09                1.81                             -2.0  2080.6630   \n",
       "...                        ...                              ...        ...   \n",
       "2022-10-27                7.93                             -2.0  1942.8995   \n",
       "2022-10-27                9.83                             -7.7  2113.3237   \n",
       "2022-10-27                6.84                            -10.3  1794.1439   \n",
       "2022-10-27                7.60                              0.3  2088.1050   \n",
       "2022-10-27                6.88                              0.9  2238.5712   \n",
       "\n",
       "                主隊ELO  win  \n",
       "Matchtime                   \n",
       "2014-11-08  1970.8143    1  \n",
       "2014-11-08  2154.6768    0  \n",
       "2014-11-08  1920.5684    0  \n",
       "2014-11-09  2254.1106    0  \n",
       "2014-11-09  2355.2815    1  \n",
       "...               ...  ...  \n",
       "2022-10-27  2193.6864    1  \n",
       "2022-10-27  2066.1966    1  \n",
       "2022-10-27  2285.1027    1  \n",
       "2022-10-27  2225.9103    1  \n",
       "2022-10-27  2060.2239    0  \n",
       "\n",
       "[7357 rows x 568 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8e94846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Home_starters1_+/-',\n",
       " 'Home_starters2_+/-',\n",
       " 'Home_starters3_+/-',\n",
       " 'Home_starters4_+/-',\n",
       " 'Home_starters5_+/-']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = []\n",
    "for c in df.columns:\n",
    "    if ('_+/-' in c) & ('Home' in c):\n",
    "        col.append(c)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b8c4106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Away_starters1_+/-',\n",
       " 'Away_starters2_+/-',\n",
       " 'Away_starters3_+/-',\n",
       " 'Away_starters4_+/-',\n",
       " 'Away_starters5_+/-']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col2 = []\n",
    "for c in df.columns:\n",
    "    if ('_+/-' in c) & ('Away' in c):\n",
    "        col2.append(c)\n",
    "col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cf6f95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>HomeScore</th>\n",
       "      <th>Away</th>\n",
       "      <th>AwayScore</th>\n",
       "      <th>讓分</th>\n",
       "      <th>總分</th>\n",
       "      <th>Eventcode_x</th>\n",
       "      <th>主勝(初)</th>\n",
       "      <th>客勝(初)</th>\n",
       "      <th>主勝率(初)</th>\n",
       "      <th>...</th>\n",
       "      <th>Away_starters5_PF</th>\n",
       "      <th>Away_starters5_PTS</th>\n",
       "      <th>Away_starters5_Game_Score</th>\n",
       "      <th>Away_starters5_+/-</th>\n",
       "      <th>Away_starters5_noplay_PointDiff</th>\n",
       "      <th>客隊ELO</th>\n",
       "      <th>主隊ELO</th>\n",
       "      <th>win</th>\n",
       "      <th>Home_startersAll_+/-</th>\n",
       "      <th>Away_startersAll_+/-</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>BOS</td>\n",
       "      <td>101</td>\n",
       "      <td>IND</td>\n",
       "      <td>98</td>\n",
       "      <td>5.5</td>\n",
       "      <td>191.0</td>\n",
       "      <td>188660</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.45</td>\n",
       "      <td>61.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2257.9675</td>\n",
       "      <td>1970.8143</td>\n",
       "      <td>1</td>\n",
       "      <td>12.344</td>\n",
       "      <td>2.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>PHO</td>\n",
       "      <td>112</td>\n",
       "      <td>SAC</td>\n",
       "      <td>114</td>\n",
       "      <td>5.0</td>\n",
       "      <td>208.5</td>\n",
       "      <td>188656</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.51</td>\n",
       "      <td>62.63</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8.1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>1918.5838</td>\n",
       "      <td>2154.6768</td>\n",
       "      <td>0</td>\n",
       "      <td>9.736</td>\n",
       "      <td>5.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-08</th>\n",
       "      <td>UTA</td>\n",
       "      <td>82</td>\n",
       "      <td>DAL</td>\n",
       "      <td>105</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>205.5</td>\n",
       "      <td>188655</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.65</td>\n",
       "      <td>43.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-4.1</td>\n",
       "      <td>2220.9141</td>\n",
       "      <td>1920.5684</td>\n",
       "      <td>0</td>\n",
       "      <td>11.628</td>\n",
       "      <td>4.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>IND</td>\n",
       "      <td>90</td>\n",
       "      <td>WAS</td>\n",
       "      <td>97</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>183.5</td>\n",
       "      <td>188666</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1.59</td>\n",
       "      <td>40.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.27</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2094.4555</td>\n",
       "      <td>2254.1106</td>\n",
       "      <td>0</td>\n",
       "      <td>7.424</td>\n",
       "      <td>3.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-09</th>\n",
       "      <td>MIA</td>\n",
       "      <td>102</td>\n",
       "      <td>MIN</td>\n",
       "      <td>92</td>\n",
       "      <td>7.5</td>\n",
       "      <td>205.0</td>\n",
       "      <td>188667</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.35</td>\n",
       "      <td>71.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2080.6630</td>\n",
       "      <td>2355.2815</td>\n",
       "      <td>1</td>\n",
       "      <td>9.782</td>\n",
       "      <td>8.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>CHI</td>\n",
       "      <td>124</td>\n",
       "      <td>IND</td>\n",
       "      <td>109</td>\n",
       "      <td>7.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484626</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.46</td>\n",
       "      <td>72.59</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.9</td>\n",
       "      <td>7.93</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1942.8995</td>\n",
       "      <td>2193.6864</td>\n",
       "      <td>1</td>\n",
       "      <td>13.276</td>\n",
       "      <td>14.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>MIN</td>\n",
       "      <td>134</td>\n",
       "      <td>SAS</td>\n",
       "      <td>122</td>\n",
       "      <td>9.5</td>\n",
       "      <td>235.5</td>\n",
       "      <td>484627</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.90</td>\n",
       "      <td>75.65</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>9.83</td>\n",
       "      <td>-7.7</td>\n",
       "      <td>2113.3237</td>\n",
       "      <td>2066.1966</td>\n",
       "      <td>1</td>\n",
       "      <td>14.510</td>\n",
       "      <td>11.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>UTA</td>\n",
       "      <td>109</td>\n",
       "      <td>HOU</td>\n",
       "      <td>101</td>\n",
       "      <td>8.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484628</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.79</td>\n",
       "      <td>65.93</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.84</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>1794.1439</td>\n",
       "      <td>2285.1027</td>\n",
       "      <td>1</td>\n",
       "      <td>11.280</td>\n",
       "      <td>10.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>DEN</td>\n",
       "      <td>110</td>\n",
       "      <td>LAL</td>\n",
       "      <td>99</td>\n",
       "      <td>5.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484629</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.97</td>\n",
       "      <td>68.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>11.8</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2088.1050</td>\n",
       "      <td>2225.9103</td>\n",
       "      <td>1</td>\n",
       "      <td>16.328</td>\n",
       "      <td>15.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>POR</td>\n",
       "      <td>98</td>\n",
       "      <td>MIA</td>\n",
       "      <td>119</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>221.5</td>\n",
       "      <td>484630</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.95</td>\n",
       "      <td>51.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6.88</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2238.5712</td>\n",
       "      <td>2060.2239</td>\n",
       "      <td>0</td>\n",
       "      <td>15.966</td>\n",
       "      <td>14.432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7357 rows × 570 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Home  HomeScore Away  AwayScore   讓分     總分  Eventcode_x  主勝(初)  \\\n",
       "Matchtime                                                                    \n",
       "2014-11-08  BOS        101  IND         98  5.5  191.0       188660   1.53   \n",
       "2014-11-08  PHO        112  SAC        114  5.0  208.5       188656   1.50   \n",
       "2014-11-08  UTA         82  DAL        105 -2.5  205.5       188655   2.17   \n",
       "2014-11-09  IND         90  WAS         97 -3.5  183.5       188666   2.33   \n",
       "2014-11-09  MIA        102  MIN         92  7.5  205.0       188667   1.32   \n",
       "...         ...        ...  ...        ...  ...    ...          ...    ...   \n",
       "2022-10-27  CHI        124  IND        109  7.5  232.5       484626   1.30   \n",
       "2022-10-27  MIN        134  SAS        122  9.5  235.5       484627   1.25   \n",
       "2022-10-27  UTA        109  HOU        101  8.5  232.5       484628   1.44   \n",
       "2022-10-27  DEN        110  LAL         99  5.5  232.5       484629   1.39   \n",
       "2022-10-27  POR         98  MIA        119 -2.5  221.5       484630   1.85   \n",
       "\n",
       "            客勝(初)  主勝率(初)  ...  Away_starters5_PF  Away_starters5_PTS  \\\n",
       "Matchtime                  ...                                          \n",
       "2014-11-08   2.45   61.56  ...                0.3                 0.6   \n",
       "2014-11-08   2.51   62.63  ...                2.1                 2.7   \n",
       "2014-11-08   1.65   43.24  ...                0.2                 1.0   \n",
       "2014-11-09   1.59   40.60  ...                0.2                 0.8   \n",
       "2014-11-09   3.35   71.53  ...                0.5                 1.6   \n",
       "...           ...     ...  ...                ...                 ...   \n",
       "2022-10-27   3.46   72.59  ...                1.0                 2.2   \n",
       "2022-10-27   3.90   75.65  ...                1.3                 1.5   \n",
       "2022-10-27   2.79   65.93  ...                1.2                 3.9   \n",
       "2022-10-27   2.97   68.05  ...                0.7                 1.7   \n",
       "2022-10-27   1.95   51.34  ...                0.9                 2.0   \n",
       "\n",
       "            Away_starters5_Game_Score  Away_starters5_+/-  \\\n",
       "Matchtime                                                   \n",
       "2014-11-08                        1.1                0.25   \n",
       "2014-11-08                        8.1                5.85   \n",
       "2014-11-08                        0.7                1.04   \n",
       "2014-11-09                        2.6                2.27   \n",
       "2014-11-09                        4.0                1.81   \n",
       "...                               ...                 ...   \n",
       "2022-10-27                        8.9                7.93   \n",
       "2022-10-27                       11.7                9.83   \n",
       "2022-10-27                        9.0                6.84   \n",
       "2022-10-27                       11.8                7.60   \n",
       "2022-10-27                        8.7                6.88   \n",
       "\n",
       "            Away_starters5_noplay_PointDiff      客隊ELO      主隊ELO  win  \\\n",
       "Matchtime                                                                \n",
       "2014-11-08                              0.9  2257.9675  1970.8143    1   \n",
       "2014-11-08                             -0.3  1918.5838  2154.6768    0   \n",
       "2014-11-08                             -4.1  2220.9141  1920.5684    0   \n",
       "2014-11-09                             -2.0  2094.4555  2254.1106    0   \n",
       "2014-11-09                             -2.0  2080.6630  2355.2815    1   \n",
       "...                                     ...        ...        ...  ...   \n",
       "2022-10-27                             -2.0  1942.8995  2193.6864    1   \n",
       "2022-10-27                             -7.7  2113.3237  2066.1966    1   \n",
       "2022-10-27                            -10.3  1794.1439  2285.1027    1   \n",
       "2022-10-27                              0.3  2088.1050  2225.9103    1   \n",
       "2022-10-27                              0.9  2238.5712  2060.2239    0   \n",
       "\n",
       "            Home_startersAll_+/-  Away_startersAll_+/-  \n",
       "Matchtime                                               \n",
       "2014-11-08                12.344                 2.610  \n",
       "2014-11-08                 9.736                 5.962  \n",
       "2014-11-08                11.628                 4.310  \n",
       "2014-11-09                 7.424                 3.062  \n",
       "2014-11-09                 9.782                 8.950  \n",
       "...                          ...                   ...  \n",
       "2022-10-27                13.276                14.372  \n",
       "2022-10-27                14.510                11.776  \n",
       "2022-10-27                11.280                10.264  \n",
       "2022-10-27                16.328                15.546  \n",
       "2022-10-27                15.966                14.432  \n",
       "\n",
       "[7357 rows x 570 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_updown = []\n",
    "a_updown = []\n",
    "for i in range(len(df)):\n",
    "    home_updown = df[col].iloc[i].mean()\n",
    "    h_updown.append(home_updown)\n",
    "    away_updown = df[col2].iloc[i].mean()\n",
    "    a_updown.append(away_updown)\n",
    "df['Home_startersAll_+/-'] = h_updown\n",
    "df['Away_startersAll_+/-'] = a_updown\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "282e21a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Home', 'HomeScore', 'Away', 'AwayScore', '讓分', '總分',\n",
       "       'Eventcode_x', '主勝(初)', '客勝(初)', '主勝率(初)', '客勝率(初)', '凱利指數(初)',\n",
       "       '凱利指數(終)', '主勝(終)', '客勝(終)', '主勝率(終)', '客勝率(終)', 'Away_game',\n",
       "       'Away_win', 'Away_lose', 'Away_win_score', 'Away_lose_score',\n",
       "       'Away_net_score', 'Away_rank', 'Away_win_rate', 'Away_game_inhome',\n",
       "       'Away_win_inhome', 'Away_lose_inhome', 'Away_win_score_inhome',\n",
       "       'Away_lose_score_inhome', 'Away_net_score_inhome',\n",
       "       'Away_rank_inhome', 'Away_win_rate_inhome', 'Away_game_inaway',\n",
       "       'Away_win_inaway', 'Away_lose_inaway', 'Away_win_score_inaway',\n",
       "       'Away_lose_score_inaway', 'Away_net_score_inaway',\n",
       "       'Away_rank_inaway', 'Away_win_rate_inaway', 'Away_game_insix',\n",
       "       'Away_win_insix', 'Away_lose_insix', 'Away_win_score_insix',\n",
       "       'Away_lose_score_insix', 'Away_net_score_insix',\n",
       "       'Away_win_rate_insix', 'Home_game', 'Home_win', 'Home_lose',\n",
       "       'Home_win_score', 'Home_lose_score', 'Home_net_score', 'Home_rank',\n",
       "       'Home_win_rate', 'Home_game_inhome', 'Home_win_inhome',\n",
       "       'Home_lose_inhome', 'Home_win_score_inhome',\n",
       "       'Home_lose_score_inhome', 'Home_net_score_inhome',\n",
       "       'Home_rank_inhome', 'Home_win_rate_inhome', 'Home_game_inaway',\n",
       "       'Home_win_inaway', 'Home_lose_inaway', 'Home_win_score_inaway',\n",
       "       'Home_lose_score_inaway', 'Home_net_score_inaway',\n",
       "       'Home_rank_inaway', 'Home_win_rate_inaway', 'Home_game_insix',\n",
       "       'Home_win_insix', 'Home_lose_insix', 'Home_win_score_insix',\n",
       "       'Home_lose_score_insix', 'Home_net_score_insix',\n",
       "       'Home_win_rate_insix', 'Both_battle', 'Away_battle', 'Home_battle',\n",
       "       'Eventcode_y', 'Home_Line1_inHome', 'Home_Line2_inHome',\n",
       "       'Home_Line3_inHome', 'Home_Line4_inHome', 'Home_Pace_inHome',\n",
       "       'Home_Total_FG_inHome', 'Home_Total_FGA_inHome',\n",
       "       'Home_Total_FG%_inHome', 'Home_Total_3P_inHome',\n",
       "       'Home_Total_3PA_inHome', 'Home_Total_3P%_inHome',\n",
       "       'Home_Total_FT_inHome', 'Home_Total_FTA_inHome',\n",
       "       'Home_Total_FT%_inHome', 'Home_Total_ORB_inHome',\n",
       "       'Home_Total_DRB_inHome', 'Home_Total_TBR_inHome',\n",
       "       'Home_Total_AST_inHome', 'Home_Total_STL_inHome',\n",
       "       'Home_Total_BLK_inHome', 'Home_Total_TOV_inHome',\n",
       "       'Home_Total_PF_inHome', 'Home_Total_PTS_inHome',\n",
       "       'Home_Total_TS%_inHome', 'Home_Total_eFG%_inHome',\n",
       "       'Home_Total_3PAr_inHome', 'Home_Total_FTr_inHome',\n",
       "       'Home_Total_ORB%_inHome', 'Home_Total_DRB%_inHome',\n",
       "       'Home_Total_TRB%_inHome', 'Home_Total_AST%_inHome',\n",
       "       'Home_Total_STL%_inHome', 'Home_Total_BLK%_inHome',\n",
       "       'Home_Total_TOV%_inHome', 'Home_Total_ORtg_inHome',\n",
       "       'Home_Total_DRtg_inHome', 'Home_Ties_inHome',\n",
       "       'Home_LeadChanges_inHome', 'Home_GameTied_inHome',\n",
       "       'Home_Led_inHome', 'Home_MCpoints_inHome', 'Home_LSdrought_inHome',\n",
       "       'Home_Line1_inAway', 'Home_Line2_inAway', 'Home_Line3_inAway',\n",
       "       'Home_Line4_inAway', 'Home_Pace_inAway', 'Home_Total_FG_inAway',\n",
       "       'Home_Total_FGA_inAway', 'Home_Total_FG%_inAway',\n",
       "       'Home_Total_3P_inAway', 'Home_Total_3PA_inAway',\n",
       "       'Home_Total_3P%_inAway', 'Home_Total_FT_inAway',\n",
       "       'Home_Total_FTA_inAway', 'Home_Total_FT%_inAway',\n",
       "       'Home_Total_ORB_inAway', 'Home_Total_DRB_inAway',\n",
       "       'Home_Total_TBR_inAway', 'Home_Total_AST_inAway',\n",
       "       'Home_Total_STL_inAway', 'Home_Total_BLK_inAway',\n",
       "       'Home_Total_TOV_inAway', 'Home_Total_PF_inAway',\n",
       "       'Home_Total_PTS_inAway', 'Home_Total_TS%_inAway',\n",
       "       'Home_Total_eFG%_inAway', 'Home_Total_3PAr_inAway',\n",
       "       'Home_Total_FTr_inAway', 'Home_Total_ORB%_inAway',\n",
       "       'Home_Total_DRB%_inAway', 'Home_Total_TRB%_inAway',\n",
       "       'Home_Total_AST%_inAway', 'Home_Total_STL%_inAway',\n",
       "       'Home_Total_BLK%_inAway', 'Home_Total_TOV%_inAway',\n",
       "       'Home_Total_ORtg_inAway', 'Home_Total_DRtg_inAway',\n",
       "       'Home_Ties_inAway', 'Home_LeadChanges_inAway',\n",
       "       'Home_GameTied_inAway', 'Home_Led_inAway', 'Home_MCpoints_inAway',\n",
       "       'Home_LSdrought_inAway', 'Home_Line1_Total', 'Home_Line2_Total',\n",
       "       'Home_Line3_Total', 'Home_Line4_Total', 'Home_Pace_Total',\n",
       "       'Home_Total_FG_Total', 'Home_Total_FGA_Total',\n",
       "       'Home_Total_FG%_Total', 'Home_Total_3P_Total',\n",
       "       'Home_Total_3PA_Total', 'Home_Total_3P%_Total',\n",
       "       'Home_Total_FT_Total', 'Home_Total_FTA_Total',\n",
       "       'Home_Total_FT%_Total', 'Home_Total_ORB_Total',\n",
       "       'Home_Total_DRB_Total', 'Home_Total_TBR_Total',\n",
       "       'Home_Total_AST_Total', 'Home_Total_STL_Total',\n",
       "       'Home_Total_BLK_Total', 'Home_Total_TOV_Total',\n",
       "       'Home_Total_PF_Total', 'Home_Total_PTS_Total',\n",
       "       'Home_Total_TS%_Total', 'Home_Total_eFG%_Total',\n",
       "       'Home_Total_3PAr_Total', 'Home_Total_FTr_Total',\n",
       "       'Home_Total_ORB%_Total', 'Home_Total_DRB%_Total',\n",
       "       'Home_Total_TRB%_Total', 'Home_Total_AST%_Total',\n",
       "       'Home_Total_STL%_Total', 'Home_Total_BLK%_Total',\n",
       "       'Home_Total_TOV%_Total', 'Home_Total_ORtg_Total',\n",
       "       'Home_Total_DRtg_Total', 'Home_Ties_Total',\n",
       "       'Home_LeadChanges_Total', 'Home_GameTied_Total', 'Home_Led_Total',\n",
       "       'Home_MCpoints_Total', 'Home_LSdrought_Total', 'Away_Line1_inHome',\n",
       "       'Away_Line2_inHome', 'Away_Line3_inHome', 'Away_Line4_inHome',\n",
       "       'Away_Pace_inHome', 'Away_Total_FG_inHome',\n",
       "       'Away_Total_FGA_inHome', 'Away_Total_FG%_inHome',\n",
       "       'Away_Total_3P_inHome', 'Away_Total_3PA_inHome',\n",
       "       'Away_Total_3P%_inHome', 'Away_Total_FT_inHome',\n",
       "       'Away_Total_FTA_inHome', 'Away_Total_FT%_inHome',\n",
       "       'Away_Total_ORB_inHome', 'Away_Total_DRB_inHome',\n",
       "       'Away_Total_TBR_inHome', 'Away_Total_AST_inHome',\n",
       "       'Away_Total_STL_inHome', 'Away_Total_BLK_inHome',\n",
       "       'Away_Total_TOV_inHome', 'Away_Total_PF_inHome',\n",
       "       'Away_Total_PTS_inHome', 'Away_Total_TS%_inHome',\n",
       "       'Away_Total_eFG%_inHome', 'Away_Total_3PAr_inHome',\n",
       "       'Away_Total_FTr_inHome', 'Away_Total_ORB%_inHome',\n",
       "       'Away_Total_DRB%_inHome', 'Away_Total_TRB%_inHome',\n",
       "       'Away_Total_AST%_inHome', 'Away_Total_STL%_inHome',\n",
       "       'Away_Total_BLK%_inHome', 'Away_Total_TOV%_inHome',\n",
       "       'Away_Total_ORtg_inHome', 'Away_Total_DRtg_inHome',\n",
       "       'Away_Ties_inHome', 'Away_LeadChanges_inHome',\n",
       "       'Away_GameTied_inHome', 'Away_Led_inHome', 'Away_MCpoints_inHome',\n",
       "       'Away_LSdrought_inHome', 'Away_Line1_inAway', 'Away_Line2_inAway',\n",
       "       'Away_Line3_inAway', 'Away_Line4_inAway', 'Away_Pace_inAway',\n",
       "       'Away_Total_FG_inAway', 'Away_Total_FGA_inAway',\n",
       "       'Away_Total_FG%_inAway', 'Away_Total_3P_inAway',\n",
       "       'Away_Total_3PA_inAway', 'Away_Total_3P%_inAway',\n",
       "       'Away_Total_FT_inAway', 'Away_Total_FTA_inAway',\n",
       "       'Away_Total_FT%_inAway', 'Away_Total_ORB_inAway',\n",
       "       'Away_Total_DRB_inAway', 'Away_Total_TBR_inAway',\n",
       "       'Away_Total_AST_inAway', 'Away_Total_STL_inAway',\n",
       "       'Away_Total_BLK_inAway', 'Away_Total_TOV_inAway',\n",
       "       'Away_Total_PF_inAway', 'Away_Total_PTS_inAway',\n",
       "       'Away_Total_TS%_inAway', 'Away_Total_eFG%_inAway',\n",
       "       'Away_Total_3PAr_inAway', 'Away_Total_FTr_inAway',\n",
       "       'Away_Total_ORB%_inAway', 'Away_Total_DRB%_inAway',\n",
       "       'Away_Total_TRB%_inAway', 'Away_Total_AST%_inAway',\n",
       "       'Away_Total_STL%_inAway', 'Away_Total_BLK%_inAway',\n",
       "       'Away_Total_TOV%_inAway', 'Away_Total_ORtg_inAway',\n",
       "       'Away_Total_DRtg_inAway', 'Away_Ties_inAway',\n",
       "       'Away_LeadChanges_inAway', 'Away_GameTied_inAway',\n",
       "       'Away_Led_inAway', 'Away_MCpoints_inAway', 'Away_LSdrought_inAway',\n",
       "       'Away_Line1_Total', 'Away_Line2_Total', 'Away_Line3_Total',\n",
       "       'Away_Line4_Total', 'Away_Pace_Total', 'Away_Total_FG_Total',\n",
       "       'Away_Total_FGA_Total', 'Away_Total_FG%_Total',\n",
       "       'Away_Total_3P_Total', 'Away_Total_3PA_Total',\n",
       "       'Away_Total_3P%_Total', 'Away_Total_FT_Total',\n",
       "       'Away_Total_FTA_Total', 'Away_Total_FT%_Total',\n",
       "       'Away_Total_ORB_Total', 'Away_Total_DRB_Total',\n",
       "       'Away_Total_TBR_Total', 'Away_Total_AST_Total',\n",
       "       'Away_Total_STL_Total', 'Away_Total_BLK_Total',\n",
       "       'Away_Total_TOV_Total', 'Away_Total_PF_Total',\n",
       "       'Away_Total_PTS_Total', 'Away_Total_TS%_Total',\n",
       "       'Away_Total_eFG%_Total', 'Away_Total_3PAr_Total',\n",
       "       'Away_Total_FTr_Total', 'Away_Total_ORB%_Total',\n",
       "       'Away_Total_DRB%_Total', 'Away_Total_TRB%_Total',\n",
       "       'Away_Total_AST%_Total', 'Away_Total_STL%_Total',\n",
       "       'Away_Total_BLK%_Total', 'Away_Total_TOV%_Total',\n",
       "       'Away_Total_ORtg_Total', 'Away_Total_DRtg_Total',\n",
       "       'Away_Ties_Total', 'Away_LeadChanges_Total', 'Away_GameTied_Total',\n",
       "       'Away_Led_Total', 'Away_MCpoints_Total', 'Away_LSdrought_Total',\n",
       "       'Home_starters1_PointDiff', 'Home_starters1_Game_Start',\n",
       "       'Home_starters1_Minutes', 'Home_starters1_FG',\n",
       "       'Home_starters1_FGA', 'Home_starters1_FG_P', 'Home_starters1_P3',\n",
       "       'Home_starters1_P3A', 'Home_starters1_P3_P', 'Home_starters1_FT',\n",
       "       'Home_starters1_FTA', 'Home_starters1_FT_P', 'Home_starters1_ORB',\n",
       "       'Home_starters1_TRB', 'Home_starters1_AST', 'Home_starters1_STL',\n",
       "       'Home_starters1_BLK', 'Home_starters1_TOV', 'Home_starters1_PF',\n",
       "       'Home_starters1_PTS', 'Home_starters1_Game_Score',\n",
       "       'Home_starters1_+/-', 'Home_starters1_noplay_PointDiff',\n",
       "       'Home_starters2_PointDiff', 'Home_starters2_Game_Start',\n",
       "       'Home_starters2_Minutes', 'Home_starters2_FG',\n",
       "       'Home_starters2_FGA', 'Home_starters2_FG_P', 'Home_starters2_P3',\n",
       "       'Home_starters2_P3A', 'Home_starters2_P3_P', 'Home_starters2_FT',\n",
       "       'Home_starters2_FTA', 'Home_starters2_FT_P', 'Home_starters2_ORB',\n",
       "       'Home_starters2_TRB', 'Home_starters2_AST', 'Home_starters2_STL',\n",
       "       'Home_starters2_BLK', 'Home_starters2_TOV', 'Home_starters2_PF',\n",
       "       'Home_starters2_PTS', 'Home_starters2_Game_Score',\n",
       "       'Home_starters2_+/-', 'Home_starters2_noplay_PointDiff',\n",
       "       'Home_starters3_PointDiff', 'Home_starters3_Game_Start',\n",
       "       'Home_starters3_Minutes', 'Home_starters3_FG',\n",
       "       'Home_starters3_FGA', 'Home_starters3_FG_P', 'Home_starters3_P3',\n",
       "       'Home_starters3_P3A', 'Home_starters3_P3_P', 'Home_starters3_FT',\n",
       "       'Home_starters3_FTA', 'Home_starters3_FT_P', 'Home_starters3_ORB',\n",
       "       'Home_starters3_TRB', 'Home_starters3_AST', 'Home_starters3_STL',\n",
       "       'Home_starters3_BLK', 'Home_starters3_TOV', 'Home_starters3_PF',\n",
       "       'Home_starters3_PTS', 'Home_starters3_Game_Score',\n",
       "       'Home_starters3_+/-', 'Home_starters3_noplay_PointDiff',\n",
       "       'Home_starters4_PointDiff', 'Home_starters4_Game_Start',\n",
       "       'Home_starters4_Minutes', 'Home_starters4_FG',\n",
       "       'Home_starters4_FGA', 'Home_starters4_FG_P', 'Home_starters4_P3',\n",
       "       'Home_starters4_P3A', 'Home_starters4_P3_P', 'Home_starters4_FT',\n",
       "       'Home_starters4_FTA', 'Home_starters4_FT_P', 'Home_starters4_ORB',\n",
       "       'Home_starters4_TRB', 'Home_starters4_AST', 'Home_starters4_STL',\n",
       "       'Home_starters4_BLK', 'Home_starters4_TOV', 'Home_starters4_PF',\n",
       "       'Home_starters4_PTS', 'Home_starters4_Game_Score',\n",
       "       'Home_starters4_+/-', 'Home_starters4_noplay_PointDiff',\n",
       "       'Home_starters5_PointDiff', 'Home_starters5_Game_Start',\n",
       "       'Home_starters5_Minutes', 'Home_starters5_FG',\n",
       "       'Home_starters5_FGA', 'Home_starters5_FG_P', 'Home_starters5_P3',\n",
       "       'Home_starters5_P3A', 'Home_starters5_P3_P', 'Home_starters5_FT',\n",
       "       'Home_starters5_FTA', 'Home_starters5_FT_P', 'Home_starters5_ORB',\n",
       "       'Home_starters5_TRB', 'Home_starters5_AST', 'Home_starters5_STL',\n",
       "       'Home_starters5_BLK', 'Home_starters5_TOV', 'Home_starters5_PF',\n",
       "       'Home_starters5_PTS', 'Home_starters5_Game_Score',\n",
       "       'Home_starters5_+/-', 'Home_starters5_noplay_PointDiff',\n",
       "       'Away_starters1_PointDiff', 'Away_starters1_Game_Start',\n",
       "       'Away_starters1_Minutes', 'Away_starters1_FG',\n",
       "       'Away_starters1_FGA', 'Away_starters1_FG_P', 'Away_starters1_P3',\n",
       "       'Away_starters1_P3A', 'Away_starters1_P3_P', 'Away_starters1_FT',\n",
       "       'Away_starters1_FTA', 'Away_starters1_FT_P', 'Away_starters1_ORB',\n",
       "       'Away_starters1_TRB', 'Away_starters1_AST', 'Away_starters1_STL',\n",
       "       'Away_starters1_BLK', 'Away_starters1_TOV', 'Away_starters1_PF',\n",
       "       'Away_starters1_PTS', 'Away_starters1_Game_Score',\n",
       "       'Away_starters1_+/-', 'Away_starters1_noplay_PointDiff',\n",
       "       'Away_starters2_PointDiff', 'Away_starters2_Game_Start',\n",
       "       'Away_starters2_Minutes', 'Away_starters2_FG',\n",
       "       'Away_starters2_FGA', 'Away_starters2_FG_P', 'Away_starters2_P3',\n",
       "       'Away_starters2_P3A', 'Away_starters2_P3_P', 'Away_starters2_FT',\n",
       "       'Away_starters2_FTA', 'Away_starters2_FT_P', 'Away_starters2_ORB',\n",
       "       'Away_starters2_TRB', 'Away_starters2_AST', 'Away_starters2_STL',\n",
       "       'Away_starters2_BLK', 'Away_starters2_TOV', 'Away_starters2_PF',\n",
       "       'Away_starters2_PTS', 'Away_starters2_Game_Score',\n",
       "       'Away_starters2_+/-', 'Away_starters2_noplay_PointDiff',\n",
       "       'Away_starters3_PointDiff', 'Away_starters3_Game_Start',\n",
       "       'Away_starters3_Minutes', 'Away_starters3_FG',\n",
       "       'Away_starters3_FGA', 'Away_starters3_FG_P', 'Away_starters3_P3',\n",
       "       'Away_starters3_P3A', 'Away_starters3_P3_P', 'Away_starters3_FT',\n",
       "       'Away_starters3_FTA', 'Away_starters3_FT_P', 'Away_starters3_ORB',\n",
       "       'Away_starters3_TRB', 'Away_starters3_AST', 'Away_starters3_STL',\n",
       "       'Away_starters3_BLK', 'Away_starters3_TOV', 'Away_starters3_PF',\n",
       "       'Away_starters3_PTS', 'Away_starters3_Game_Score',\n",
       "       'Away_starters3_+/-', 'Away_starters3_noplay_PointDiff',\n",
       "       'Away_starters4_PointDiff', 'Away_starters4_Game_Start',\n",
       "       'Away_starters4_Minutes', 'Away_starters4_FG',\n",
       "       'Away_starters4_FGA', 'Away_starters4_FG_P', 'Away_starters4_P3',\n",
       "       'Away_starters4_P3A', 'Away_starters4_P3_P', 'Away_starters4_FT',\n",
       "       'Away_starters4_FTA', 'Away_starters4_FT_P', 'Away_starters4_ORB',\n",
       "       'Away_starters4_TRB', 'Away_starters4_AST', 'Away_starters4_STL',\n",
       "       'Away_starters4_BLK', 'Away_starters4_TOV', 'Away_starters4_PF',\n",
       "       'Away_starters4_PTS', 'Away_starters4_Game_Score',\n",
       "       'Away_starters4_+/-', 'Away_starters4_noplay_PointDiff',\n",
       "       'Away_starters5_PointDiff', 'Away_starters5_Game_Start',\n",
       "       'Away_starters5_Minutes', 'Away_starters5_FG',\n",
       "       'Away_starters5_FGA', 'Away_starters5_FG_P', 'Away_starters5_P3',\n",
       "       'Away_starters5_P3A', 'Away_starters5_P3_P', 'Away_starters5_FT',\n",
       "       'Away_starters5_FTA', 'Away_starters5_FT_P', 'Away_starters5_ORB',\n",
       "       'Away_starters5_TRB', 'Away_starters5_AST', 'Away_starters5_STL',\n",
       "       'Away_starters5_BLK', 'Away_starters5_TOV', 'Away_starters5_PF',\n",
       "       'Away_starters5_PTS', 'Away_starters5_Game_Score',\n",
       "       'Away_starters5_+/-', 'Away_starters5_noplay_PointDiff', '客隊ELO',\n",
       "       '主隊ELO', 'win', 'Home_startersAll_+/-', 'Away_startersAll_+/-'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d0ee525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7357, 563), (7357,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = df.drop(['Home', 'HomeScore', 'Away', 'AwayScore',\"Eventcode_x\",\"Eventcode_y\",\"win\"],axis = 1)\n",
    "df_y = df[\"win\"]\n",
    "df_x.shape,df_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03e0e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = df_x[:\"2021-09\"]\n",
    "df_test_x = df_x[\"2021-10\":]\n",
    "df_train_y = df_y[:\"2021-09\"]\n",
    "df_test_y = df_y[\"2021-10\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "330b6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0358ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df_train_x, df_train_y, test_size = 0.2, random_state = 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f9be952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "mixmin_scaler = preprocessing.StandardScaler()\n",
    "df_x_train_mm = mixmin_scaler.fit_transform(x_train)\n",
    "df_x_val_mm = mixmin_scaler.transform(x_val)\n",
    "x_test_mm = mixmin_scaler.transform(df_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c5a663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Specs       Score\n",
      "10                     主勝率(終)  898.170766\n",
      "11                     客勝率(終)  898.065160\n",
      "0                          讓分  887.690256\n",
      "5                      客勝率(初)  851.264397\n",
      "4                      主勝率(初)  851.261659\n",
      "329  Home_starters1_PointDiff  783.366516\n",
      "375  Home_starters3_PointDiff  764.388762\n",
      "352  Home_starters2_PointDiff  760.377185\n",
      "398  Home_starters4_PointDiff  746.540147\n",
      "421  Home_starters5_PointDiff  703.304116\n"
     ]
    }
   ],
   "source": [
    "m = 10\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "#apply SelectKBest class to extract top 5 best features\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k=5)\n",
    "fit = bestfeatures.fit(df_x_train_mm,y_train)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(x_train.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(m,'Score'))  #print 5 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fba6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "clf = AutoML()\n",
    "clf.fit(df_x_train, df_y_train,task='classification',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "771269a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:14:03] {2600} INFO - task = classification\n",
      "[flaml.automl: 03-16 13:14:03] {2602} INFO - Data split method: stratified\n",
      "[flaml.automl: 03-16 13:14:03] {2605} INFO - Evaluation method: holdout\n",
      "[flaml.automl: 03-16 13:14:03] {2727} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl: 03-16 13:14:03] {2869} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'lrl1']\n",
      "[flaml.automl: 03-16 13:14:03] {3164} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:03] {3297} INFO - Estimated sufficient time budget=4249s. Estimated necessary time budget=98s.\n",
      "[flaml.automl: 03-16 13:14:03] {3344} INFO -  at 0.5s,\testimator lgbm's best error=0.4161,\tbest estimator lgbm's best error=0.4161\n",
      "[flaml.automl: 03-16 13:14:03] {3164} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:03] {3344} INFO -  at 0.9s,\testimator lgbm's best error=0.4161,\tbest estimator lgbm's best error=0.4161\n",
      "[flaml.automl: 03-16 13:14:03] {3164} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:04] {3344} INFO -  at 1.4s,\testimator lgbm's best error=0.4161,\tbest estimator lgbm's best error=0.4161\n",
      "[flaml.automl: 03-16 13:14:04] {3164} INFO - iteration 3, current learner xgboost\n",
      "[flaml.automl: 03-16 13:14:04] {3344} INFO -  at 1.7s,\testimator xgboost's best error=0.3230,\tbest estimator xgboost's best error=0.3230\n",
      "[flaml.automl: 03-16 13:14:04] {3164} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:05] {3344} INFO -  at 2.1s,\testimator lgbm's best error=0.4161,\tbest estimator xgboost's best error=0.3230\n",
      "[flaml.automl: 03-16 13:14:05] {3164} INFO - iteration 5, current learner xgboost\n",
      "[flaml.automl: 03-16 13:14:05] {3344} INFO -  at 2.5s,\testimator xgboost's best error=0.3230,\tbest estimator xgboost's best error=0.3230\n",
      "[flaml.automl: 03-16 13:14:05] {3164} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:05] {3344} INFO -  at 2.9s,\testimator lgbm's best error=0.4161,\tbest estimator xgboost's best error=0.3230\n",
      "[flaml.automl: 03-16 13:14:05] {3164} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:06] {3344} INFO -  at 3.3s,\testimator lgbm's best error=0.4161,\tbest estimator xgboost's best error=0.3230\n",
      "[flaml.automl: 03-16 13:14:06] {3164} INFO - iteration 8, current learner xgboost\n",
      "[flaml.automl: 03-16 13:14:06] {3344} INFO -  at 3.8s,\testimator xgboost's best error=0.3222,\tbest estimator xgboost's best error=0.3222\n",
      "[flaml.automl: 03-16 13:14:06] {3164} INFO - iteration 9, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:06] {3344} INFO -  at 3.9s,\testimator extra_tree's best error=0.3270,\tbest estimator xgboost's best error=0.3222\n",
      "[flaml.automl: 03-16 13:14:06] {3164} INFO - iteration 10, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:07] {3344} INFO -  at 4.0s,\testimator extra_tree's best error=0.3270,\tbest estimator xgboost's best error=0.3222\n",
      "[flaml.automl: 03-16 13:14:07] {3164} INFO - iteration 11, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:07] {3344} INFO -  at 4.1s,\testimator extra_tree's best error=0.3270,\tbest estimator xgboost's best error=0.3222\n",
      "[flaml.automl: 03-16 13:14:07] {3164} INFO - iteration 12, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:07] {3344} INFO -  at 4.3s,\testimator extra_tree's best error=0.3270,\tbest estimator xgboost's best error=0.3222\n",
      "[flaml.automl: 03-16 13:14:07] {3164} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl: 03-16 13:14:07] {3344} INFO -  at 4.7s,\testimator xgboost's best error=0.3222,\tbest estimator xgboost's best error=0.3222\n",
      "[flaml.automl: 03-16 13:14:07] {3164} INFO - iteration 14, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:07] {3344} INFO -  at 4.8s,\testimator extra_tree's best error=0.3087,\tbest estimator extra_tree's best error=0.3087\n",
      "[flaml.automl: 03-16 13:14:07] {3164} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:08] {3344} INFO -  at 5.2s,\testimator lgbm's best error=0.4161,\tbest estimator extra_tree's best error=0.3087\n",
      "[flaml.automl: 03-16 13:14:08] {3164} INFO - iteration 16, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:08] {3344} INFO -  at 5.4s,\testimator extra_tree's best error=0.3087,\tbest estimator extra_tree's best error=0.3087\n",
      "[flaml.automl: 03-16 13:14:08] {3164} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:08] {3344} INFO -  at 5.8s,\testimator lgbm's best error=0.4161,\tbest estimator extra_tree's best error=0.3087\n",
      "[flaml.automl: 03-16 13:14:08] {3164} INFO - iteration 18, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:08] {3344} INFO -  at 6.0s,\testimator extra_tree's best error=0.3087,\tbest estimator extra_tree's best error=0.3087\n",
      "[flaml.automl: 03-16 13:14:08] {3164} INFO - iteration 19, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:09] {3344} INFO -  at 6.2s,\testimator extra_tree's best error=0.3071,\tbest estimator extra_tree's best error=0.3071\n",
      "[flaml.automl: 03-16 13:14:09] {3164} INFO - iteration 20, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:09] {3344} INFO -  at 6.4s,\testimator extra_tree's best error=0.3071,\tbest estimator extra_tree's best error=0.3071\n",
      "[flaml.automl: 03-16 13:14:09] {3164} INFO - iteration 21, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:09] {3344} INFO -  at 6.8s,\testimator lgbm's best error=0.4161,\tbest estimator extra_tree's best error=0.3071\n",
      "[flaml.automl: 03-16 13:14:09] {3164} INFO - iteration 22, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:10] {3344} INFO -  at 7.0s,\testimator extra_tree's best error=0.3071,\tbest estimator extra_tree's best error=0.3071\n",
      "[flaml.automl: 03-16 13:14:10] {3164} INFO - iteration 23, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:10] {3344} INFO -  at 7.3s,\testimator extra_tree's best error=0.3063,\tbest estimator extra_tree's best error=0.3063\n",
      "[flaml.automl: 03-16 13:14:10] {3164} INFO - iteration 24, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:10] {3344} INFO -  at 7.5s,\testimator extra_tree's best error=0.3063,\tbest estimator extra_tree's best error=0.3063\n",
      "[flaml.automl: 03-16 13:14:10] {3164} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:10] {3344} INFO -  at 7.8s,\testimator extra_tree's best error=0.2967,\tbest estimator extra_tree's best error=0.2967\n",
      "[flaml.automl: 03-16 13:14:10] {3164} INFO - iteration 26, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:11] {3344} INFO -  at 8.1s,\testimator extra_tree's best error=0.2967,\tbest estimator extra_tree's best error=0.2967\n",
      "[flaml.automl: 03-16 13:14:11] {3164} INFO - iteration 27, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:11] {3344} INFO -  at 8.6s,\testimator extra_tree's best error=0.2959,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:11] {3164} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:12] {3344} INFO -  at 9.1s,\testimator extra_tree's best error=0.2959,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:12] {3164} INFO - iteration 29, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:12] {3344} INFO -  at 9.5s,\testimator lgbm's best error=0.4161,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:12] {3164} INFO - iteration 30, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:12] {3344} INFO -  at 10.0s,\testimator lgbm's best error=0.4161,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:12] {3164} INFO - iteration 31, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:13] {3344} INFO -  at 10.4s,\testimator extra_tree's best error=0.2959,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:13] {3164} INFO - iteration 32, current learner rf\n",
      "[flaml.automl: 03-16 13:14:13] {3344} INFO -  at 10.6s,\testimator rf's best error=0.3007,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:13] {3164} INFO - iteration 33, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:13] {3344} INFO -  at 11.0s,\testimator extra_tree's best error=0.2959,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:13] {3164} INFO - iteration 34, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:14:14] {3344} INFO -  at 11.1s,\testimator rf's best error=0.3007,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:14] {3164} INFO - iteration 35, current learner rf\n",
      "[flaml.automl: 03-16 13:14:14] {3344} INFO -  at 11.3s,\testimator rf's best error=0.3007,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:14] {3164} INFO - iteration 36, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:15] {3344} INFO -  at 12.3s,\testimator extra_tree's best error=0.2959,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:15] {3164} INFO - iteration 37, current learner rf\n",
      "[flaml.automl: 03-16 13:14:15] {3344} INFO -  at 12.5s,\testimator rf's best error=0.3007,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:15] {3164} INFO - iteration 38, current learner rf\n",
      "[flaml.automl: 03-16 13:14:15] {3344} INFO -  at 12.8s,\testimator rf's best error=0.3007,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:15] {3164} INFO - iteration 39, current learner rf\n",
      "[flaml.automl: 03-16 13:14:16] {3344} INFO -  at 13.1s,\testimator rf's best error=0.3007,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:16] {3164} INFO - iteration 40, current learner rf\n",
      "[flaml.automl: 03-16 13:14:16] {3344} INFO -  at 13.3s,\testimator rf's best error=0.3007,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:16] {3164} INFO - iteration 41, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:17] {3344} INFO -  at 14.0s,\testimator extra_tree's best error=0.2959,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:17] {3164} INFO - iteration 42, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:17] {3344} INFO -  at 14.4s,\testimator lgbm's best error=0.3429,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:17] {3164} INFO - iteration 43, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:17] {3344} INFO -  at 14.9s,\testimator extra_tree's best error=0.2959,\tbest estimator extra_tree's best error=0.2959\n",
      "[flaml.automl: 03-16 13:14:17] {3164} INFO - iteration 44, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:18] {3344} INFO -  at 15.4s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:18] {3164} INFO - iteration 45, current learner rf\n",
      "[flaml.automl: 03-16 13:14:18] {3344} INFO -  at 15.8s,\testimator rf's best error=0.3007,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:18] {3164} INFO - iteration 46, current learner rf\n",
      "[flaml.automl: 03-16 13:14:19] {3344} INFO -  at 16.0s,\testimator rf's best error=0.3007,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:19] {3164} INFO - iteration 47, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:19] {3344} INFO -  at 16.5s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:19] {3164} INFO - iteration 48, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:19] {3344} INFO -  at 16.9s,\testimator lgbm's best error=0.3429,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:19] {3164} INFO - iteration 49, current learner rf\n",
      "[flaml.automl: 03-16 13:14:20] {3344} INFO -  at 17.2s,\testimator rf's best error=0.3007,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:20] {3164} INFO - iteration 50, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:21] {3344} INFO -  at 18.1s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:21] {3164} INFO - iteration 51, current learner rf\n",
      "[flaml.automl: 03-16 13:14:21] {3344} INFO -  at 18.3s,\testimator rf's best error=0.3007,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:21] {3164} INFO - iteration 52, current learner rf\n",
      "[flaml.automl: 03-16 13:14:21] {3344} INFO -  at 18.4s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:21] {3164} INFO - iteration 53, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:21] {3344} INFO -  at 18.9s,\testimator lgbm's best error=0.3063,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:21] {3164} INFO - iteration 54, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:22] {3344} INFO -  at 19.4s,\testimator lgbm's best error=0.3063,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:22] {3164} INFO - iteration 55, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:22] {3344} INFO -  at 19.8s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:22] {3164} INFO - iteration 56, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:23] {3344} INFO -  at 20.3s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:23] {3164} INFO - iteration 57, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:23] {3344} INFO -  at 20.7s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:23] {3164} INFO - iteration 58, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:24] {3344} INFO -  at 21.1s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:24] {3164} INFO - iteration 59, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:24] {3344} INFO -  at 21.6s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:24] {3164} INFO - iteration 60, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:25] {3344} INFO -  at 22.1s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:25] {3164} INFO - iteration 61, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:25] {3344} INFO -  at 22.5s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:25] {3164} INFO - iteration 62, current learner rf\n",
      "[flaml.automl: 03-16 13:14:25] {3344} INFO -  at 22.9s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:25] {3164} INFO - iteration 63, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:26] {3344} INFO -  at 23.4s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:26] {3164} INFO - iteration 64, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:26] {3344} INFO -  at 23.9s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:26] {3164} INFO - iteration 65, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:27] {3344} INFO -  at 24.4s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:27] {3164} INFO - iteration 66, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:28] {3344} INFO -  at 25.0s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:28] {3164} INFO - iteration 67, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:28] {3344} INFO -  at 25.6s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:28] {3164} INFO - iteration 68, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:29] {3344} INFO -  at 26.2s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:29] {3164} INFO - iteration 69, current learner rf\n",
      "[flaml.automl: 03-16 13:14:29] {3344} INFO -  at 26.3s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:29] {3164} INFO - iteration 70, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:30] {3344} INFO -  at 27.0s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:30] {3164} INFO - iteration 71, current learner rf\n",
      "[flaml.automl: 03-16 13:14:30] {3344} INFO -  at 27.3s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:14:30] {3164} INFO - iteration 72, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:31] {3344} INFO -  at 28.2s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:31] {3164} INFO - iteration 73, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:31] {3344} INFO -  at 28.7s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:31] {3164} INFO - iteration 74, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:32] {3344} INFO -  at 29.4s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:32] {3164} INFO - iteration 75, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:32] {3344} INFO -  at 29.9s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:32] {3164} INFO - iteration 76, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:33] {3344} INFO -  at 30.3s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:33] {3164} INFO - iteration 77, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:33] {3344} INFO -  at 30.8s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:33] {3164} INFO - iteration 78, current learner rf\n",
      "[flaml.automl: 03-16 13:14:34] {3344} INFO -  at 31.0s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:34] {3164} INFO - iteration 79, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:34] {3344} INFO -  at 31.5s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:34] {3164} INFO - iteration 80, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:35] {3344} INFO -  at 32.5s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:35] {3164} INFO - iteration 81, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:35] {3344} INFO -  at 33.0s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:35] {3164} INFO - iteration 82, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:36] {3344} INFO -  at 33.4s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:36] {3164} INFO - iteration 83, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:36] {3344} INFO -  at 34.0s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:36] {3164} INFO - iteration 84, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:37] {3344} INFO -  at 34.6s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:37] {3164} INFO - iteration 85, current learner rf\n",
      "[flaml.automl: 03-16 13:14:37] {3344} INFO -  at 34.9s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:37] {3164} INFO - iteration 86, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:39] {3344} INFO -  at 36.4s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:39] {3164} INFO - iteration 87, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:39] {3344} INFO -  at 36.9s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:39] {3164} INFO - iteration 88, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:40] {3344} INFO -  at 37.5s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:40] {3164} INFO - iteration 89, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:40] {3344} INFO -  at 37.8s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:40] {3164} INFO - iteration 90, current learner rf\n",
      "[flaml.automl: 03-16 13:14:40] {3344} INFO -  at 38.0s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:40] {3164} INFO - iteration 91, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:41] {3344} INFO -  at 38.4s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:41] {3164} INFO - iteration 92, current learner xgboost\n",
      "[flaml.automl: 03-16 13:14:41] {3344} INFO -  at 38.7s,\testimator xgboost's best error=0.3222,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:41] {3164} INFO - iteration 93, current learner rf\n",
      "[flaml.automl: 03-16 13:14:42] {3344} INFO -  at 39.1s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:42] {3164} INFO - iteration 94, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:42] {3344} INFO -  at 39.6s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:42] {3164} INFO - iteration 95, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:43] {3344} INFO -  at 40.6s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:43] {3164} INFO - iteration 96, current learner rf\n",
      "[flaml.automl: 03-16 13:14:43] {3344} INFO -  at 40.8s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:43] {3164} INFO - iteration 97, current learner rf\n",
      "[flaml.automl: 03-16 13:14:44] {3344} INFO -  at 41.1s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:44] {3164} INFO - iteration 98, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:44] {3344} INFO -  at 41.6s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:44] {3164} INFO - iteration 99, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:45] {3344} INFO -  at 42.0s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:45] {3164} INFO - iteration 100, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:46] {3344} INFO -  at 43.1s,\testimator extra_tree's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:46] {3164} INFO - iteration 101, current learner rf\n",
      "[flaml.automl: 03-16 13:14:46] {3344} INFO -  at 43.3s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:46] {3164} INFO - iteration 102, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:46] {3344} INFO -  at 43.8s,\testimator lgbm's best error=0.2944,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:46] {3164} INFO - iteration 103, current learner rf\n",
      "[flaml.automl: 03-16 13:14:47] {3344} INFO -  at 44.0s,\testimator rf's best error=0.2983,\tbest estimator extra_tree's best error=0.2944\n",
      "[flaml.automl: 03-16 13:14:47] {3164} INFO - iteration 104, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:47] {3344} INFO -  at 44.5s,\testimator lgbm's best error=0.2896,\tbest estimator lgbm's best error=0.2896\n",
      "[flaml.automl: 03-16 13:14:47] {3164} INFO - iteration 105, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:47] {3344} INFO -  at 45.0s,\testimator lgbm's best error=0.2896,\tbest estimator lgbm's best error=0.2896\n",
      "[flaml.automl: 03-16 13:14:47] {3164} INFO - iteration 106, current learner xgboost\n",
      "[flaml.automl: 03-16 13:14:48] {3344} INFO -  at 45.3s,\testimator xgboost's best error=0.3222,\tbest estimator lgbm's best error=0.2896\n",
      "[flaml.automl: 03-16 13:14:48] {3164} INFO - iteration 107, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:48] {3344} INFO -  at 45.8s,\testimator lgbm's best error=0.2888,\tbest estimator lgbm's best error=0.2888\n",
      "[flaml.automl: 03-16 13:14:48] {3164} INFO - iteration 108, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:49] {3344} INFO -  at 46.4s,\testimator lgbm's best error=0.2888,\tbest estimator lgbm's best error=0.2888\n",
      "[flaml.automl: 03-16 13:14:49] {3164} INFO - iteration 109, current learner lgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:14:49] {3344} INFO -  at 46.8s,\testimator lgbm's best error=0.2888,\tbest estimator lgbm's best error=0.2888\n",
      "[flaml.automl: 03-16 13:14:49] {3164} INFO - iteration 110, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:50] {3344} INFO -  at 47.2s,\testimator lgbm's best error=0.2872,\tbest estimator lgbm's best error=0.2872\n",
      "[flaml.automl: 03-16 13:14:50] {3164} INFO - iteration 111, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:50] {3344} INFO -  at 47.6s,\testimator lgbm's best error=0.2872,\tbest estimator lgbm's best error=0.2872\n",
      "[flaml.automl: 03-16 13:14:50] {3164} INFO - iteration 112, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:51] {3344} INFO -  at 48.0s,\testimator lgbm's best error=0.2872,\tbest estimator lgbm's best error=0.2872\n",
      "[flaml.automl: 03-16 13:14:51] {3164} INFO - iteration 113, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:14:51] {3344} INFO -  at 48.4s,\testimator extra_tree's best error=0.2944,\tbest estimator lgbm's best error=0.2872\n",
      "[flaml.automl: 03-16 13:14:51] {3164} INFO - iteration 114, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:51] {3344} INFO -  at 48.8s,\testimator lgbm's best error=0.2872,\tbest estimator lgbm's best error=0.2872\n",
      "[flaml.automl: 03-16 13:14:51] {3164} INFO - iteration 115, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:52] {3344} INFO -  at 49.2s,\testimator lgbm's best error=0.2872,\tbest estimator lgbm's best error=0.2872\n",
      "[flaml.automl: 03-16 13:14:52] {3164} INFO - iteration 116, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:52] {3344} INFO -  at 49.7s,\testimator lgbm's best error=0.2872,\tbest estimator lgbm's best error=0.2872\n",
      "[flaml.automl: 03-16 13:14:52] {3164} INFO - iteration 117, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:53] {3344} INFO -  at 50.1s,\testimator lgbm's best error=0.2872,\tbest estimator lgbm's best error=0.2872\n",
      "[flaml.automl: 03-16 13:14:53] {3164} INFO - iteration 118, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:53] {3344} INFO -  at 50.5s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:53] {3164} INFO - iteration 119, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:53] {3344} INFO -  at 50.9s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:53] {3164} INFO - iteration 120, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:54] {3344} INFO -  at 51.4s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:54] {3164} INFO - iteration 121, current learner rf\n",
      "[flaml.automl: 03-16 13:14:54] {3344} INFO -  at 51.7s,\testimator rf's best error=0.2983,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:54] {3164} INFO - iteration 122, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:55] {3344} INFO -  at 52.1s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:55] {3164} INFO - iteration 123, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:55] {3344} INFO -  at 52.6s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:55] {3164} INFO - iteration 124, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:56] {3344} INFO -  at 53.0s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:56] {3164} INFO - iteration 125, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:56] {3344} INFO -  at 53.4s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:56] {3164} INFO - iteration 126, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:56] {3344} INFO -  at 53.9s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:56] {3164} INFO - iteration 127, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:57] {3344} INFO -  at 54.4s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:57] {3164} INFO - iteration 128, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:57] {3344} INFO -  at 54.8s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:57] {3164} INFO - iteration 129, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:58] {3344} INFO -  at 55.3s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:58] {3164} INFO - iteration 130, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:58] {3344} INFO -  at 55.8s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:58] {3164} INFO - iteration 131, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:59] {3344} INFO -  at 56.2s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:59] {3164} INFO - iteration 132, current learner lgbm\n",
      "[flaml.automl: 03-16 13:14:59] {3344} INFO -  at 56.7s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:14:59] {3164} INFO - iteration 133, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:00] {3344} INFO -  at 57.1s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:00] {3164} INFO - iteration 134, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:00] {3344} INFO -  at 57.7s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:00] {3164} INFO - iteration 135, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:15:01] {3344} INFO -  at 58.2s,\testimator extra_tree's best error=0.2944,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:01] {3164} INFO - iteration 136, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:01] {3344} INFO -  at 58.6s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:01] {3164} INFO - iteration 137, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:02] {3344} INFO -  at 59.0s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:02] {3164} INFO - iteration 138, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:02] {3344} INFO -  at 59.7s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:02] {3164} INFO - iteration 139, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:03] {3344} INFO -  at 60.1s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:03] {3164} INFO - iteration 140, current learner rf\n",
      "[flaml.automl: 03-16 13:15:03] {3344} INFO -  at 60.3s,\testimator rf's best error=0.2975,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:03] {3164} INFO - iteration 141, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:03] {3344} INFO -  at 60.8s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:03] {3164} INFO - iteration 142, current learner rf\n",
      "[flaml.automl: 03-16 13:15:04] {3344} INFO -  at 61.2s,\testimator rf's best error=0.2975,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:04] {3164} INFO - iteration 143, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:04] {3344} INFO -  at 61.7s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:04] {3164} INFO - iteration 144, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:05] {3344} INFO -  at 62.1s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:05] {3164} INFO - iteration 145, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:05] {3344} INFO -  at 62.5s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:05] {3164} INFO - iteration 146, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:05] {3344} INFO -  at 63.0s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:05] {3164} INFO - iteration 147, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:06] {3344} INFO -  at 63.4s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:15:06] {3164} INFO - iteration 148, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:06] {3344} INFO -  at 63.9s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:06] {3164} INFO - iteration 149, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:07] {3344} INFO -  at 64.3s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:07] {3164} INFO - iteration 150, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:07] {3344} INFO -  at 64.7s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:07] {3164} INFO - iteration 151, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:08] {3344} INFO -  at 65.2s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:08] {3164} INFO - iteration 152, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:08] {3344} INFO -  at 65.7s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:08] {3164} INFO - iteration 153, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:09] {3344} INFO -  at 66.1s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:09] {3164} INFO - iteration 154, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:09] {3344} INFO -  at 66.5s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:09] {3164} INFO - iteration 155, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:15:10] {3344} INFO -  at 67.2s,\testimator extra_tree's best error=0.2944,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:10] {3164} INFO - iteration 156, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:10] {3344} INFO -  at 67.6s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:10] {3164} INFO - iteration 157, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:11] {3344} INFO -  at 68.0s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:11] {3164} INFO - iteration 158, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:11] {3344} INFO -  at 68.5s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:11] {3164} INFO - iteration 159, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:11] {3344} INFO -  at 68.9s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:11] {3164} INFO - iteration 160, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:12] {3344} INFO -  at 69.3s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:12] {3164} INFO - iteration 161, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:12] {3344} INFO -  at 69.8s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:12] {3164} INFO - iteration 162, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:15:13] {3344} INFO -  at 70.7s,\testimator extra_tree's best error=0.2944,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:13] {3164} INFO - iteration 163, current learner rf\n",
      "[flaml.automl: 03-16 13:15:13] {3344} INFO -  at 71.0s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:13] {3164} INFO - iteration 164, current learner rf\n",
      "[flaml.automl: 03-16 13:15:14] {3344} INFO -  at 71.3s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:14] {3164} INFO - iteration 165, current learner rf\n",
      "[flaml.automl: 03-16 13:15:14] {3344} INFO -  at 71.5s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:14] {3164} INFO - iteration 166, current learner rf\n",
      "[flaml.automl: 03-16 13:15:15] {3344} INFO -  at 72.1s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:15] {3164} INFO - iteration 167, current learner rf\n",
      "[flaml.automl: 03-16 13:15:15] {3344} INFO -  at 72.3s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:15] {3164} INFO - iteration 168, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:15] {3344} INFO -  at 72.8s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:15] {3164} INFO - iteration 169, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:16] {3344} INFO -  at 73.3s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:16] {3164} INFO - iteration 170, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:16] {3344} INFO -  at 73.6s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:16] {3164} INFO - iteration 171, current learner rf\n",
      "[flaml.automl: 03-16 13:15:16] {3344} INFO -  at 74.0s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:16] {3164} INFO - iteration 172, current learner rf\n",
      "[flaml.automl: 03-16 13:15:17] {3344} INFO -  at 74.3s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:17] {3164} INFO - iteration 173, current learner rf\n",
      "[flaml.automl: 03-16 13:15:17] {3344} INFO -  at 74.9s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:17] {3164} INFO - iteration 174, current learner rf\n",
      "[flaml.automl: 03-16 13:15:18] {3344} INFO -  at 75.1s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:18] {3164} INFO - iteration 175, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:15:18] {3344} INFO -  at 75.5s,\testimator extra_tree's best error=0.2944,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:18] {3164} INFO - iteration 176, current learner rf\n",
      "[flaml.automl: 03-16 13:15:18] {3344} INFO -  at 75.7s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:18] {3164} INFO - iteration 177, current learner rf\n",
      "[flaml.automl: 03-16 13:15:19] {3344} INFO -  at 76.2s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:19] {3164} INFO - iteration 178, current learner rf\n",
      "[flaml.automl: 03-16 13:15:20] {3344} INFO -  at 77.0s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:20] {3164} INFO - iteration 179, current learner rf\n",
      "[flaml.automl: 03-16 13:15:20] {3344} INFO -  at 77.3s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:20] {3164} INFO - iteration 180, current learner rf\n",
      "[flaml.automl: 03-16 13:15:20] {3344} INFO -  at 77.5s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:20] {3164} INFO - iteration 181, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:20] {3344} INFO -  at 78.0s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:20] {3164} INFO - iteration 182, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:21] {3344} INFO -  at 78.4s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:21] {3164} INFO - iteration 183, current learner rf\n",
      "[flaml.automl: 03-16 13:15:21] {3344} INFO -  at 78.8s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:21] {3164} INFO - iteration 184, current learner rf\n",
      "[flaml.automl: 03-16 13:15:22] {3344} INFO -  at 79.3s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:22] {3164} INFO - iteration 185, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:22] {3344} INFO -  at 79.7s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:22] {3164} INFO - iteration 186, current learner rf\n",
      "[flaml.automl: 03-16 13:15:22] {3344} INFO -  at 79.9s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:15:22] {3164} INFO - iteration 187, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:23] {3344} INFO -  at 80.3s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:23] {3164} INFO - iteration 188, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:23] {3344} INFO -  at 80.7s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:23] {3164} INFO - iteration 189, current learner rf\n",
      "[flaml.automl: 03-16 13:15:24] {3344} INFO -  at 81.1s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:24] {3164} INFO - iteration 190, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:24] {3344} INFO -  at 81.5s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:24] {3164} INFO - iteration 191, current learner rf\n",
      "[flaml.automl: 03-16 13:15:24] {3344} INFO -  at 81.8s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:24] {3164} INFO - iteration 192, current learner rf\n",
      "[flaml.automl: 03-16 13:15:25] {3344} INFO -  at 82.1s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:25] {3164} INFO - iteration 193, current learner xgboost\n",
      "[flaml.automl: 03-16 13:15:25] {3344} INFO -  at 82.4s,\testimator xgboost's best error=0.3222,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:25] {3164} INFO - iteration 194, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:25] {3344} INFO -  at 82.8s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:25] {3164} INFO - iteration 195, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:26] {3344} INFO -  at 83.2s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:26] {3164} INFO - iteration 196, current learner rf\n",
      "[flaml.automl: 03-16 13:15:26] {3344} INFO -  at 83.5s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:26] {3164} INFO - iteration 197, current learner rf\n",
      "[flaml.automl: 03-16 13:15:26] {3344} INFO -  at 83.9s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:26] {3164} INFO - iteration 198, current learner rf\n",
      "[flaml.automl: 03-16 13:15:27] {3344} INFO -  at 84.2s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:27] {3164} INFO - iteration 199, current learner xgboost\n",
      "[flaml.automl: 03-16 13:15:27] {3344} INFO -  at 84.5s,\testimator xgboost's best error=0.3222,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:27] {3164} INFO - iteration 200, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:27] {3344} INFO -  at 84.8s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:27] {3164} INFO - iteration 201, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:28] {3344} INFO -  at 85.3s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:28] {3164} INFO - iteration 202, current learner rf\n",
      "[flaml.automl: 03-16 13:15:28] {3344} INFO -  at 85.7s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:28] {3164} INFO - iteration 203, current learner xgboost\n",
      "[flaml.automl: 03-16 13:15:28] {3344} INFO -  at 86.0s,\testimator xgboost's best error=0.3222,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:28] {3164} INFO - iteration 204, current learner xgboost\n",
      "[flaml.automl: 03-16 13:15:29] {3344} INFO -  at 86.3s,\testimator xgboost's best error=0.3222,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:29] {3164} INFO - iteration 205, current learner rf\n",
      "[flaml.automl: 03-16 13:15:29] {3344} INFO -  at 86.7s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:29] {3164} INFO - iteration 206, current learner rf\n",
      "[flaml.automl: 03-16 13:15:29] {3344} INFO -  at 87.0s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:29] {3164} INFO - iteration 207, current learner rf\n",
      "[flaml.automl: 03-16 13:15:30] {3344} INFO -  at 87.2s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:30] {3164} INFO - iteration 208, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:30] {3344} INFO -  at 87.6s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:30] {3164} INFO - iteration 209, current learner rf\n",
      "[flaml.automl: 03-16 13:15:31] {3344} INFO -  at 88.2s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:31] {3164} INFO - iteration 210, current learner rf\n",
      "[flaml.automl: 03-16 13:15:31] {3344} INFO -  at 88.4s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:31] {3164} INFO - iteration 211, current learner rf\n",
      "[flaml.automl: 03-16 13:15:31] {3344} INFO -  at 88.9s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:31] {3164} INFO - iteration 212, current learner xgboost\n",
      "[flaml.automl: 03-16 13:15:32] {3344} INFO -  at 89.1s,\testimator xgboost's best error=0.3222,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:32] {3164} INFO - iteration 213, current learner rf\n",
      "[flaml.automl: 03-16 13:15:32] {3344} INFO -  at 89.4s,\testimator rf's best error=0.2912,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:32] {3164} INFO - iteration 214, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:32] {3344} INFO -  at 89.8s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:32] {3164} INFO - iteration 215, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:15:33] {3344} INFO -  at 90.3s,\testimator extra_tree's best error=0.2944,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:33] {3164} INFO - iteration 216, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:33] {3344} INFO -  at 90.8s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:33] {3164} INFO - iteration 217, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:34] {3344} INFO -  at 91.2s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:34] {3164} INFO - iteration 218, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:34] {3344} INFO -  at 91.6s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:34] {3164} INFO - iteration 219, current learner rf\n",
      "[flaml.automl: 03-16 13:15:35] {3344} INFO -  at 92.1s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:35] {3164} INFO - iteration 220, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:35] {3344} INFO -  at 92.5s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:35] {3164} INFO - iteration 221, current learner xgboost\n",
      "[flaml.automl: 03-16 13:15:35] {3344} INFO -  at 92.8s,\testimator xgboost's best error=0.3222,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:35] {3164} INFO - iteration 222, current learner rf\n",
      "[flaml.automl: 03-16 13:15:36] {3344} INFO -  at 93.1s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:36] {3164} INFO - iteration 223, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:36] {3344} INFO -  at 93.5s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:36] {3164} INFO - iteration 224, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:36] {3344} INFO -  at 93.8s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:36] {3164} INFO - iteration 225, current learner rf\n",
      "[flaml.automl: 03-16 13:15:37] {3344} INFO -  at 94.5s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:15:37] {3164} INFO - iteration 226, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:37] {3344} INFO -  at 95.0s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:37] {3164} INFO - iteration 227, current learner rf\n",
      "[flaml.automl: 03-16 13:15:38] {3344} INFO -  at 95.2s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:38] {3164} INFO - iteration 228, current learner rf\n",
      "[flaml.automl: 03-16 13:15:38] {3344} INFO -  at 95.9s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:38] {3164} INFO - iteration 229, current learner rf\n",
      "[flaml.automl: 03-16 13:15:39] {3344} INFO -  at 96.5s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:39] {3164} INFO - iteration 230, current learner rf\n",
      "[flaml.automl: 03-16 13:15:39] {3344} INFO -  at 96.9s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:39] {3164} INFO - iteration 231, current learner rf\n",
      "[flaml.automl: 03-16 13:15:40] {3344} INFO -  at 97.3s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:40] {3164} INFO - iteration 232, current learner rf\n",
      "[flaml.automl: 03-16 13:15:40] {3344} INFO -  at 97.8s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:40] {3164} INFO - iteration 233, current learner rf\n",
      "[flaml.automl: 03-16 13:15:41] {3344} INFO -  at 98.3s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:41] {3164} INFO - iteration 234, current learner rf\n",
      "[flaml.automl: 03-16 13:15:41] {3344} INFO -  at 98.7s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:41] {3164} INFO - iteration 235, current learner rf\n",
      "[flaml.automl: 03-16 13:15:41] {3344} INFO -  at 98.9s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:41] {3164} INFO - iteration 236, current learner rf\n",
      "[flaml.automl: 03-16 13:15:42] {3344} INFO -  at 99.7s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:42] {3164} INFO - iteration 237, current learner rf\n",
      "[flaml.automl: 03-16 13:15:43] {3344} INFO -  at 100.5s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:43] {3164} INFO - iteration 238, current learner rf\n",
      "[flaml.automl: 03-16 13:15:43] {3344} INFO -  at 100.7s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:43] {3164} INFO - iteration 239, current learner rf\n",
      "[flaml.automl: 03-16 13:15:44] {3344} INFO -  at 101.1s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:44] {3164} INFO - iteration 240, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:44] {3344} INFO -  at 101.6s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:44] {3164} INFO - iteration 241, current learner rf\n",
      "[flaml.automl: 03-16 13:15:44] {3344} INFO -  at 102.0s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:44] {3164} INFO - iteration 242, current learner rf\n",
      "[flaml.automl: 03-16 13:15:45] {3344} INFO -  at 102.6s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:45] {3164} INFO - iteration 243, current learner rf\n",
      "[flaml.automl: 03-16 13:15:45] {3344} INFO -  at 102.9s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:45] {3164} INFO - iteration 244, current learner rf\n",
      "[flaml.automl: 03-16 13:15:46] {3344} INFO -  at 103.6s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:46] {3164} INFO - iteration 245, current learner rf\n",
      "[flaml.automl: 03-16 13:15:46] {3344} INFO -  at 103.9s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:46] {3164} INFO - iteration 246, current learner rf\n",
      "[flaml.automl: 03-16 13:15:47] {3344} INFO -  at 104.1s,\testimator rf's best error=0.2864,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:47] {3164} INFO - iteration 247, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:47] {3344} INFO -  at 104.5s,\testimator lgbm's best error=0.2856,\tbest estimator lgbm's best error=0.2856\n",
      "[flaml.automl: 03-16 13:15:47] {3164} INFO - iteration 248, current learner rf\n",
      "[flaml.automl: 03-16 13:15:48] {3344} INFO -  at 105.2s,\testimator rf's best error=0.2848,\tbest estimator rf's best error=0.2848\n",
      "[flaml.automl: 03-16 13:15:48] {3164} INFO - iteration 249, current learner rf\n",
      "[flaml.automl: 03-16 13:15:48] {3344} INFO -  at 105.7s,\testimator rf's best error=0.2848,\tbest estimator rf's best error=0.2848\n",
      "[flaml.automl: 03-16 13:15:48] {3164} INFO - iteration 250, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:49] {3344} INFO -  at 106.2s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2848\n",
      "[flaml.automl: 03-16 13:15:49] {3164} INFO - iteration 251, current learner rf\n",
      "[flaml.automl: 03-16 13:15:50] {3344} INFO -  at 107.0s,\testimator rf's best error=0.2848,\tbest estimator rf's best error=0.2848\n",
      "[flaml.automl: 03-16 13:15:50] {3164} INFO - iteration 252, current learner rf\n",
      "[flaml.automl: 03-16 13:15:51] {3344} INFO -  at 108.1s,\testimator rf's best error=0.2848,\tbest estimator rf's best error=0.2848\n",
      "[flaml.automl: 03-16 13:15:51] {3164} INFO - iteration 253, current learner rf\n",
      "[flaml.automl: 03-16 13:15:51] {3344} INFO -  at 108.5s,\testimator rf's best error=0.2848,\tbest estimator rf's best error=0.2848\n",
      "[flaml.automl: 03-16 13:15:51] {3164} INFO - iteration 254, current learner lgbm\n",
      "[flaml.automl: 03-16 13:15:51] {3344} INFO -  at 108.9s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2848\n",
      "[flaml.automl: 03-16 13:15:51] {3164} INFO - iteration 255, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:15:52] {3344} INFO -  at 109.5s,\testimator extra_tree's best error=0.2944,\tbest estimator rf's best error=0.2848\n",
      "[flaml.automl: 03-16 13:15:52] {3164} INFO - iteration 256, current learner rf\n",
      "[flaml.automl: 03-16 13:15:52] {3344} INFO -  at 110.0s,\testimator rf's best error=0.2848,\tbest estimator rf's best error=0.2848\n",
      "[flaml.automl: 03-16 13:15:52] {3164} INFO - iteration 257, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:15:53] {3344} INFO -  at 110.6s,\testimator extra_tree's best error=0.2944,\tbest estimator rf's best error=0.2848\n",
      "[flaml.automl: 03-16 13:15:53] {3164} INFO - iteration 258, current learner rf\n",
      "[flaml.automl: 03-16 13:15:54] {3344} INFO -  at 111.4s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:15:54] {3164} INFO - iteration 259, current learner rf\n",
      "[flaml.automl: 03-16 13:15:55] {3344} INFO -  at 112.2s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:15:55] {3164} INFO - iteration 260, current learner rf\n",
      "[flaml.automl: 03-16 13:15:56] {3344} INFO -  at 113.4s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:15:56] {3164} INFO - iteration 261, current learner rf\n",
      "[flaml.automl: 03-16 13:15:56] {3344} INFO -  at 114.0s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:15:56] {3164} INFO - iteration 262, current learner rf\n",
      "[flaml.automl: 03-16 13:15:58] {3344} INFO -  at 115.3s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:15:58] {3164} INFO - iteration 263, current learner rf\n",
      "[flaml.automl: 03-16 13:15:58] {3344} INFO -  at 116.0s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:15:58] {3164} INFO - iteration 264, current learner rf\n",
      "[flaml.automl: 03-16 13:16:00] {3344} INFO -  at 117.2s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:16:00] {3164} INFO - iteration 265, current learner rf\n",
      "[flaml.automl: 03-16 13:16:00] {3344} INFO -  at 118.0s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:00] {3164} INFO - iteration 266, current learner rf\n",
      "[flaml.automl: 03-16 13:16:02] {3344} INFO -  at 119.0s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:02] {3164} INFO - iteration 267, current learner rf\n",
      "[flaml.automl: 03-16 13:16:02] {3344} INFO -  at 119.6s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:02] {3164} INFO - iteration 268, current learner rf\n",
      "[flaml.automl: 03-16 13:16:04] {3344} INFO -  at 121.1s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:04] {3164} INFO - iteration 269, current learner rf\n",
      "[flaml.automl: 03-16 13:16:05] {3344} INFO -  at 122.2s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:05] {3164} INFO - iteration 270, current learner rf\n",
      "[flaml.automl: 03-16 13:16:05] {3344} INFO -  at 123.0s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:05] {3164} INFO - iteration 271, current learner rf\n",
      "[flaml.automl: 03-16 13:16:07] {3344} INFO -  at 124.2s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:07] {3164} INFO - iteration 272, current learner rf\n",
      "[flaml.automl: 03-16 13:16:07] {3344} INFO -  at 124.9s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:07] {3164} INFO - iteration 273, current learner rf\n",
      "[flaml.automl: 03-16 13:16:09] {3344} INFO -  at 126.0s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:09] {3164} INFO - iteration 274, current learner rf\n",
      "[flaml.automl: 03-16 13:16:09] {3344} INFO -  at 126.7s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:09] {3164} INFO - iteration 275, current learner rf\n",
      "[flaml.automl: 03-16 13:16:11] {3344} INFO -  at 128.2s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:11] {3164} INFO - iteration 276, current learner rf\n",
      "[flaml.automl: 03-16 13:16:11] {3344} INFO -  at 128.8s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:11] {3164} INFO - iteration 277, current learner rf\n",
      "[flaml.automl: 03-16 13:16:12] {3344} INFO -  at 129.9s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:12] {3164} INFO - iteration 278, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:16:13] {3344} INFO -  at 130.4s,\testimator extra_tree's best error=0.2944,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:13] {3164} INFO - iteration 279, current learner rf\n",
      "[flaml.automl: 03-16 13:16:14] {3344} INFO -  at 131.2s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:14] {3164} INFO - iteration 280, current learner rf\n",
      "[flaml.automl: 03-16 13:16:14] {3344} INFO -  at 131.9s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:14] {3164} INFO - iteration 281, current learner rf\n",
      "[flaml.automl: 03-16 13:16:15] {3344} INFO -  at 132.9s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:15] {3164} INFO - iteration 282, current learner rf\n",
      "[flaml.automl: 03-16 13:16:16] {3344} INFO -  at 133.7s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:16] {3164} INFO - iteration 283, current learner rf\n",
      "[flaml.automl: 03-16 13:16:17] {3344} INFO -  at 134.9s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:17] {3164} INFO - iteration 284, current learner rf\n",
      "[flaml.automl: 03-16 13:16:19] {3344} INFO -  at 136.3s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:19] {3164} INFO - iteration 285, current learner xgboost\n",
      "[flaml.automl: 03-16 13:16:19] {3344} INFO -  at 136.5s,\testimator xgboost's best error=0.3222,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:19] {3164} INFO - iteration 286, current learner rf\n",
      "[flaml.automl: 03-16 13:16:20] {3344} INFO -  at 137.2s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:20] {3164} INFO - iteration 287, current learner rf\n",
      "[flaml.automl: 03-16 13:16:20] {3344} INFO -  at 137.9s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:20] {3164} INFO - iteration 288, current learner rf\n",
      "[flaml.automl: 03-16 13:16:22] {3344} INFO -  at 139.3s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:22] {3164} INFO - iteration 289, current learner rf\n",
      "[flaml.automl: 03-16 13:16:23] {3344} INFO -  at 140.2s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:23] {3164} INFO - iteration 290, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:23] {3344} INFO -  at 140.5s,\testimator xgb_limitdepth's best error=0.3126,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:23] {3164} INFO - iteration 291, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:23] {3344} INFO -  at 140.8s,\testimator xgb_limitdepth's best error=0.3126,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:23] {3164} INFO - iteration 292, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:24] {3344} INFO -  at 141.1s,\testimator xgb_limitdepth's best error=0.3095,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:24] {3164} INFO - iteration 293, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:24] {3344} INFO -  at 141.3s,\testimator xgb_limitdepth's best error=0.3095,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:24] {3164} INFO - iteration 294, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:24] {3344} INFO -  at 141.6s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:24] {3164} INFO - iteration 295, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:24] {3344} INFO -  at 141.9s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:24] {3164} INFO - iteration 296, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:25] {3344} INFO -  at 142.2s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:25] {3164} INFO - iteration 297, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:25] {3344} INFO -  at 142.5s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:25] {3164} INFO - iteration 298, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:25] {3344} INFO -  at 142.9s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:25] {3164} INFO - iteration 299, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:26] {3344} INFO -  at 143.2s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:26] {3164} INFO - iteration 300, current learner rf\n",
      "[flaml.automl: 03-16 13:16:27] {3344} INFO -  at 144.2s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:27] {3164} INFO - iteration 301, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:27] {3344} INFO -  at 144.5s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:27] {3164} INFO - iteration 302, current learner rf\n",
      "[flaml.automl: 03-16 13:16:28] {3344} INFO -  at 145.6s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:16:28] {3164} INFO - iteration 303, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:28] {3344} INFO -  at 145.9s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:28] {3164} INFO - iteration 304, current learner rf\n",
      "[flaml.automl: 03-16 13:16:29] {3344} INFO -  at 146.6s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:29] {3164} INFO - iteration 305, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:29] {3344} INFO -  at 146.9s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:29] {3164} INFO - iteration 306, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:30] {3344} INFO -  at 147.2s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:30] {3164} INFO - iteration 307, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:30] {3344} INFO -  at 147.5s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:30] {3164} INFO - iteration 308, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:30] {3344} INFO -  at 147.8s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:30] {3164} INFO - iteration 309, current learner rf\n",
      "[flaml.automl: 03-16 13:16:31] {3344} INFO -  at 148.9s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:31] {3164} INFO - iteration 310, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:32] {3344} INFO -  at 149.2s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:32] {3164} INFO - iteration 311, current learner rf\n",
      "[flaml.automl: 03-16 13:16:33] {3344} INFO -  at 150.0s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:33] {3164} INFO - iteration 312, current learner rf\n",
      "[flaml.automl: 03-16 13:16:33] {3344} INFO -  at 150.8s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:33] {3164} INFO - iteration 313, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:34] {3344} INFO -  at 151.1s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:34] {3164} INFO - iteration 314, current learner rf\n",
      "[flaml.automl: 03-16 13:16:35] {3344} INFO -  at 152.2s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:35] {3164} INFO - iteration 315, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:35] {3344} INFO -  at 152.5s,\testimator xgb_limitdepth's best error=0.2991,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:35] {3164} INFO - iteration 316, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:35] {3344} INFO -  at 152.8s,\testimator xgb_limitdepth's best error=0.2944,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:35] {3164} INFO - iteration 317, current learner rf\n",
      "[flaml.automl: 03-16 13:16:37] {3344} INFO -  at 154.3s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:37] {3164} INFO - iteration 318, current learner rf\n",
      "[flaml.automl: 03-16 13:16:37] {3344} INFO -  at 154.9s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:37] {3164} INFO - iteration 319, current learner rf\n",
      "[flaml.automl: 03-16 13:16:39] {3344} INFO -  at 156.0s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:39] {3164} INFO - iteration 320, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:16:39] {3344} INFO -  at 156.6s,\testimator extra_tree's best error=0.2944,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:39] {3164} INFO - iteration 321, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:39] {3344} INFO -  at 156.9s,\testimator xgb_limitdepth's best error=0.2944,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:39] {3164} INFO - iteration 322, current learner rf\n",
      "[flaml.automl: 03-16 13:16:40] {3344} INFO -  at 157.7s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:40] {3164} INFO - iteration 323, current learner rf\n",
      "[flaml.automl: 03-16 13:16:41] {3344} INFO -  at 158.6s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:41] {3164} INFO - iteration 324, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:41] {3344} INFO -  at 158.8s,\testimator xgb_limitdepth's best error=0.2944,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:41] {3164} INFO - iteration 325, current learner rf\n",
      "[flaml.automl: 03-16 13:16:42] {3344} INFO -  at 159.6s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:42] {3164} INFO - iteration 326, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:42] {3344} INFO -  at 159.9s,\testimator xgb_limitdepth's best error=0.2944,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:42] {3164} INFO - iteration 327, current learner rf\n",
      "[flaml.automl: 03-16 13:16:43] {3344} INFO -  at 160.7s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:43] {3164} INFO - iteration 328, current learner rf\n",
      "[flaml.automl: 03-16 13:16:44] {3344} INFO -  at 161.8s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:44] {3164} INFO - iteration 329, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:16:45] {3344} INFO -  at 162.4s,\testimator extra_tree's best error=0.2944,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:45] {3164} INFO - iteration 330, current learner xgboost\n",
      "[flaml.automl: 03-16 13:16:45] {3344} INFO -  at 162.7s,\testimator xgboost's best error=0.3222,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:45] {3164} INFO - iteration 331, current learner rf\n",
      "[flaml.automl: 03-16 13:16:46] {3344} INFO -  at 163.5s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:46] {3164} INFO - iteration 332, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:46] {3344} INFO -  at 163.8s,\testimator xgb_limitdepth's best error=0.2944,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:46] {3164} INFO - iteration 333, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:47] {3344} INFO -  at 164.0s,\testimator xgb_limitdepth's best error=0.2944,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:47] {3164} INFO - iteration 334, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:47] {3344} INFO -  at 164.3s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:47] {3164} INFO - iteration 335, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:47] {3344} INFO -  at 164.5s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:47] {3164} INFO - iteration 336, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:47] {3344} INFO -  at 164.7s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:47] {3164} INFO - iteration 337, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:48] {3344} INFO -  at 165.0s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:48] {3164} INFO - iteration 338, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:48] {3344} INFO -  at 165.2s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:48] {3164} INFO - iteration 339, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:48] {3344} INFO -  at 165.5s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:16:48] {3164} INFO - iteration 340, current learner rf\n",
      "[flaml.automl: 03-16 13:16:49] {3344} INFO -  at 166.5s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:49] {3164} INFO - iteration 341, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:49] {3344} INFO -  at 166.8s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:49] {3164} INFO - iteration 342, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:50] {3344} INFO -  at 167.0s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:50] {3164} INFO - iteration 343, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:50] {3344} INFO -  at 167.2s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:50] {3164} INFO - iteration 344, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:50] {3344} INFO -  at 167.5s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:50] {3164} INFO - iteration 345, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:50] {3344} INFO -  at 167.7s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:50] {3164} INFO - iteration 346, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:50] {3344} INFO -  at 168.0s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:50] {3164} INFO - iteration 347, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:51] {3344} INFO -  at 168.2s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:51] {3164} INFO - iteration 348, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:51] {3344} INFO -  at 168.5s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:51] {3164} INFO - iteration 349, current learner rf\n",
      "[flaml.automl: 03-16 13:16:52] {3344} INFO -  at 169.5s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:52] {3164} INFO - iteration 350, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:52] {3344} INFO -  at 169.8s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:52] {3164} INFO - iteration 351, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:53] {3344} INFO -  at 170.0s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:53] {3164} INFO - iteration 352, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:53] {3344} INFO -  at 170.2s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:53] {3164} INFO - iteration 353, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:53] {3344} INFO -  at 170.5s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:53] {3164} INFO - iteration 354, current learner rf\n",
      "[flaml.automl: 03-16 13:16:54] {3344} INFO -  at 171.3s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:54] {3164} INFO - iteration 355, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:54] {3344} INFO -  at 171.6s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:54] {3164} INFO - iteration 356, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:54] {3344} INFO -  at 171.8s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:54] {3164} INFO - iteration 357, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:55] {3344} INFO -  at 172.0s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:55] {3164} INFO - iteration 358, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:55] {3344} INFO -  at 172.2s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:55] {3164} INFO - iteration 359, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:55] {3344} INFO -  at 172.5s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:55] {3164} INFO - iteration 360, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:55] {3344} INFO -  at 172.7s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:55] {3164} INFO - iteration 361, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:55] {3344} INFO -  at 172.9s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:55] {3164} INFO - iteration 362, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:56] {3344} INFO -  at 173.2s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:56] {3164} INFO - iteration 363, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:56] {3344} INFO -  at 173.4s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:56] {3164} INFO - iteration 364, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:56] {3344} INFO -  at 173.7s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:56] {3164} INFO - iteration 365, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:56] {3344} INFO -  at 173.9s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:56] {3164} INFO - iteration 366, current learner rf\n",
      "[flaml.automl: 03-16 13:16:58] {3344} INFO -  at 175.1s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:58] {3164} INFO - iteration 367, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:58] {3344} INFO -  at 175.4s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:58] {3164} INFO - iteration 368, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:58] {3344} INFO -  at 175.6s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:58] {3164} INFO - iteration 369, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:58] {3344} INFO -  at 175.8s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:58] {3164} INFO - iteration 370, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:59] {3344} INFO -  at 176.1s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:59] {3164} INFO - iteration 371, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:59] {3344} INFO -  at 176.3s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:59] {3164} INFO - iteration 372, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:59] {3344} INFO -  at 176.5s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:59] {3164} INFO - iteration 373, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:16:59] {3344} INFO -  at 176.8s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:16:59] {3164} INFO - iteration 374, current learner rf\n",
      "[flaml.automl: 03-16 13:17:00] {3344} INFO -  at 177.5s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:00] {3164} INFO - iteration 375, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:00] {3344} INFO -  at 177.7s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:17:00] {3164} INFO - iteration 376, current learner lgbm\n",
      "[flaml.automl: 03-16 13:17:01] {3344} INFO -  at 178.1s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:01] {3164} INFO - iteration 377, current learner rf\n",
      "[flaml.automl: 03-16 13:17:01] {3344} INFO -  at 178.8s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:01] {3164} INFO - iteration 378, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:02] {3344} INFO -  at 179.1s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:02] {3164} INFO - iteration 379, current learner lgbm\n",
      "[flaml.automl: 03-16 13:17:02] {3344} INFO -  at 179.5s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:02] {3164} INFO - iteration 380, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:02] {3344} INFO -  at 179.7s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:02] {3164} INFO - iteration 381, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:02] {3344} INFO -  at 180.0s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:02] {3164} INFO - iteration 382, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:03] {3344} INFO -  at 180.2s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:03] {3164} INFO - iteration 383, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:03] {3344} INFO -  at 180.5s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:03] {3164} INFO - iteration 384, current learner rf\n",
      "[flaml.automl: 03-16 13:17:04] {3344} INFO -  at 181.6s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:04] {3164} INFO - iteration 385, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:04] {3344} INFO -  at 181.9s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:04] {3164} INFO - iteration 386, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:05] {3344} INFO -  at 182.1s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:05] {3164} INFO - iteration 387, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:05] {3344} INFO -  at 182.3s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:05] {3164} INFO - iteration 388, current learner rf\n",
      "[flaml.automl: 03-16 13:17:06] {3344} INFO -  at 183.5s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:06] {3164} INFO - iteration 389, current learner rf\n",
      "[flaml.automl: 03-16 13:17:07] {3344} INFO -  at 184.3s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:07] {3164} INFO - iteration 390, current learner rf\n",
      "[flaml.automl: 03-16 13:17:08] {3344} INFO -  at 185.0s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:08] {3164} INFO - iteration 391, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:08] {3344} INFO -  at 185.2s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:08] {3164} INFO - iteration 392, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:08] {3344} INFO -  at 185.4s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:08] {3164} INFO - iteration 393, current learner rf\n",
      "[flaml.automl: 03-16 13:17:09] {3344} INFO -  at 186.6s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:09] {3164} INFO - iteration 394, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:09] {3344} INFO -  at 186.8s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:09] {3164} INFO - iteration 395, current learner rf\n",
      "[flaml.automl: 03-16 13:17:10] {3344} INFO -  at 187.4s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:10] {3164} INFO - iteration 396, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:10] {3344} INFO -  at 187.6s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:10] {3164} INFO - iteration 397, current learner rf\n",
      "[flaml.automl: 03-16 13:17:12] {3344} INFO -  at 189.1s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:12] {3164} INFO - iteration 398, current learner lgbm\n",
      "[flaml.automl: 03-16 13:17:12] {3344} INFO -  at 189.5s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:12] {3164} INFO - iteration 399, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:12] {3344} INFO -  at 189.7s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:12] {3164} INFO - iteration 400, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:12] {3344} INFO -  at 189.9s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:12] {3164} INFO - iteration 401, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:13] {3344} INFO -  at 190.1s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:13] {3164} INFO - iteration 402, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:13] {3344} INFO -  at 190.4s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:13] {3164} INFO - iteration 403, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:13] {3344} INFO -  at 190.6s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:13] {3164} INFO - iteration 404, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:13] {3344} INFO -  at 190.9s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:13] {3164} INFO - iteration 405, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:14] {3344} INFO -  at 191.1s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:14] {3164} INFO - iteration 406, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:14] {3344} INFO -  at 191.3s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:14] {3164} INFO - iteration 407, current learner lgbm\n",
      "[flaml.automl: 03-16 13:17:14] {3344} INFO -  at 191.8s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:14] {3164} INFO - iteration 408, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:15] {3344} INFO -  at 192.0s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:15] {3164} INFO - iteration 409, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:15] {3344} INFO -  at 192.3s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:15] {3164} INFO - iteration 410, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:15] {3344} INFO -  at 192.5s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:15] {3164} INFO - iteration 411, current learner rf\n",
      "[flaml.automl: 03-16 13:17:16] {3344} INFO -  at 193.6s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:16] {3164} INFO - iteration 412, current learner rf\n",
      "[flaml.automl: 03-16 13:17:17] {3344} INFO -  at 194.3s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:17:17] {3164} INFO - iteration 413, current learner rf\n",
      "[flaml.automl: 03-16 13:17:18] {3344} INFO -  at 195.4s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:18] {3164} INFO - iteration 414, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:18] {3344} INFO -  at 195.6s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:18] {3164} INFO - iteration 415, current learner rf\n",
      "[flaml.automl: 03-16 13:17:19] {3344} INFO -  at 196.4s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:19] {3164} INFO - iteration 416, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:19] {3344} INFO -  at 196.7s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:19] {3164} INFO - iteration 417, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:19] {3344} INFO -  at 196.9s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:19] {3164} INFO - iteration 418, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:20] {3344} INFO -  at 197.1s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:20] {3164} INFO - iteration 419, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:20] {3344} INFO -  at 197.3s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:20] {3164} INFO - iteration 420, current learner xgboost\n",
      "[flaml.automl: 03-16 13:17:20] {3344} INFO -  at 197.6s,\testimator xgboost's best error=0.3214,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:20] {3164} INFO - iteration 421, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:20] {3344} INFO -  at 197.9s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:20] {3164} INFO - iteration 422, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:21] {3344} INFO -  at 198.2s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:21] {3164} INFO - iteration 423, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:21] {3344} INFO -  at 198.4s,\testimator xgb_limitdepth's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:21] {3164} INFO - iteration 424, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:21] {3344} INFO -  at 198.6s,\testimator xgb_limitdepth's best error=0.2848,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:21] {3164} INFO - iteration 425, current learner rf\n",
      "[flaml.automl: 03-16 13:17:22] {3344} INFO -  at 199.4s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:22] {3164} INFO - iteration 426, current learner lgbm\n",
      "[flaml.automl: 03-16 13:17:22] {3344} INFO -  at 199.9s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:22] {3164} INFO - iteration 427, current learner lgbm\n",
      "[flaml.automl: 03-16 13:17:23] {3344} INFO -  at 200.3s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:23] {3164} INFO - iteration 428, current learner lgbm\n",
      "[flaml.automl: 03-16 13:17:23] {3344} INFO -  at 200.7s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:23] {3164} INFO - iteration 429, current learner rf\n",
      "[flaml.automl: 03-16 13:17:24] {3344} INFO -  at 201.6s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:24] {3164} INFO - iteration 430, current learner lgbm\n",
      "[flaml.automl: 03-16 13:17:25] {3344} INFO -  at 202.0s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:25] {3164} INFO - iteration 431, current learner lgbm\n",
      "[flaml.automl: 03-16 13:17:25] {3344} INFO -  at 202.4s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:25] {3164} INFO - iteration 432, current learner xgboost\n",
      "[flaml.automl: 03-16 13:17:25] {3344} INFO -  at 202.7s,\testimator xgboost's best error=0.3214,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:25] {3164} INFO - iteration 433, current learner rf\n",
      "[flaml.automl: 03-16 13:17:26] {3344} INFO -  at 203.4s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:26] {3164} INFO - iteration 434, current learner xgboost\n",
      "[flaml.automl: 03-16 13:17:26] {3344} INFO -  at 203.7s,\testimator xgboost's best error=0.3214,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:26] {3164} INFO - iteration 435, current learner rf\n",
      "[flaml.automl: 03-16 13:17:27] {3344} INFO -  at 204.9s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:27] {3164} INFO - iteration 436, current learner rf\n",
      "[flaml.automl: 03-16 13:17:28] {3344} INFO -  at 205.8s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:28] {3164} INFO - iteration 437, current learner rf\n",
      "[flaml.automl: 03-16 13:17:29] {3344} INFO -  at 206.7s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:29] {3164} INFO - iteration 438, current learner rf\n",
      "[flaml.automl: 03-16 13:17:30] {3344} INFO -  at 207.5s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:30] {3164} INFO - iteration 439, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:17:31] {3344} INFO -  at 208.1s,\testimator extra_tree's best error=0.2944,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:31] {3164} INFO - iteration 440, current learner rf\n",
      "[flaml.automl: 03-16 13:17:32] {3344} INFO -  at 209.0s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:32] {3164} INFO - iteration 441, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:17:32] {3344} INFO -  at 209.3s,\testimator xgb_limitdepth's best error=0.2848,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:32] {3164} INFO - iteration 442, current learner lgbm\n",
      "[flaml.automl: 03-16 13:17:32] {3344} INFO -  at 209.7s,\testimator lgbm's best error=0.2856,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:32] {3164} INFO - iteration 443, current learner xgboost\n",
      "[flaml.automl: 03-16 13:17:33] {3344} INFO -  at 210.0s,\testimator xgboost's best error=0.3214,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:33] {3164} INFO - iteration 444, current learner rf\n",
      "[flaml.automl: 03-16 13:17:33] {3344} INFO -  at 210.9s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:33] {3164} INFO - iteration 445, current learner rf\n",
      "[flaml.automl: 03-16 13:17:34] {3344} INFO -  at 211.8s,\testimator rf's best error=0.2784,\tbest estimator rf's best error=0.2784\n",
      "[flaml.automl: 03-16 13:17:34] {3164} INFO - iteration 446, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:17:44] {3344} INFO -  at 221.7s,\testimator lrl1's best error=0.2411,\tbest estimator lrl1's best error=0.2411\n",
      "[flaml.automl: 03-16 13:17:44] {3164} INFO - iteration 447, current learner rf\n",
      "[flaml.automl: 03-16 13:17:46] {3344} INFO -  at 223.2s,\testimator rf's best error=0.2784,\tbest estimator lrl1's best error=0.2411\n",
      "[flaml.automl: 03-16 13:17:46] {3164} INFO - iteration 448, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:17:56] {3344} INFO -  at 233.8s,\testimator lrl1's best error=0.2363,\tbest estimator lrl1's best error=0.2363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:17:56] {3164} INFO - iteration 449, current learner lgbm\n",
      "[flaml.automl: 03-16 13:17:57] {3344} INFO -  at 234.3s,\testimator lgbm's best error=0.2856,\tbest estimator lrl1's best error=0.2363\n",
      "[flaml.automl: 03-16 13:17:57] {3164} INFO - iteration 450, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:18:07] {3344} INFO -  at 244.6s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:18:07] {3164} INFO - iteration 451, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:18:17] {3344} INFO -  at 254.8s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:18:17] {3164} INFO - iteration 452, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:18:27] {3344} INFO -  at 264.5s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:18:27] {3164} INFO - iteration 453, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:18:38] {3344} INFO -  at 275.1s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:18:38] {3164} INFO - iteration 454, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:18:48] {3344} INFO -  at 285.6s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:18:48] {3164} INFO - iteration 455, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:18:59] {3344} INFO -  at 296.4s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:18:59] {3164} INFO - iteration 456, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:19:10] {3344} INFO -  at 307.5s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:19:10] {3164} INFO - iteration 457, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:19:21] {3344} INFO -  at 318.3s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:19:21] {3164} INFO - iteration 458, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:19:32] {3344} INFO -  at 329.6s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:19:32] {3164} INFO - iteration 459, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:19:43] {3344} INFO -  at 340.5s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:19:43] {3164} INFO - iteration 460, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:19:54] {3344} INFO -  at 351.4s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:19:54] {3164} INFO - iteration 461, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:20:05] {3344} INFO -  at 362.4s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:20:05] {3164} INFO - iteration 462, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:20:16] {3344} INFO -  at 373.3s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:20:16] {3164} INFO - iteration 463, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:20:27] {3344} INFO -  at 384.3s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:20:27] {3164} INFO - iteration 464, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:20:27] {3344} INFO -  at 384.8s,\testimator extra_tree's best error=0.2944,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:20:27] {3164} INFO - iteration 465, current learner xgboost\n",
      "[flaml.automl: 03-16 13:20:28] {3344} INFO -  at 385.1s,\testimator xgboost's best error=0.3214,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:20:28] {3164} INFO - iteration 466, current learner rf\n",
      "[flaml.automl: 03-16 13:20:28] {3344} INFO -  at 385.7s,\testimator rf's best error=0.2784,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:20:28] {3164} INFO - iteration 467, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:20:39] {3344} INFO -  at 396.5s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:20:39] {3164} INFO - iteration 468, current learner xgboost\n",
      "[flaml.automl: 03-16 13:20:39] {3344} INFO -  at 396.8s,\testimator xgboost's best error=0.3214,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:20:39] {3164} INFO - iteration 469, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:20:40] {3344} INFO -  at 397.0s,\testimator xgb_limitdepth's best error=0.2848,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:20:40] {3164} INFO - iteration 470, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:20:50] {3344} INFO -  at 407.9s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:20:50] {3164} INFO - iteration 471, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:20:51] {3344} INFO -  at 408.2s,\testimator xgb_limitdepth's best error=0.2848,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:20:51] {3164} INFO - iteration 472, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:21:02] {3344} INFO -  at 419.4s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:21:02] {3164} INFO - iteration 473, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:21:03] {3344} INFO -  at 420.1s,\testimator extra_tree's best error=0.2944,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:21:03] {3164} INFO - iteration 474, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:21:14] {3344} INFO -  at 431.1s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:21:14] {3164} INFO - iteration 475, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:21:14] {3344} INFO -  at 431.3s,\testimator xgb_limitdepth's best error=0.2848,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:21:14] {3164} INFO - iteration 476, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:21:25] {3344} INFO -  at 442.3s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:21:25] {3164} INFO - iteration 477, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:21:36] {3344} INFO -  at 453.8s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:21:36] {3164} INFO - iteration 478, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:21:48] {3344} INFO -  at 465.1s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:21:48] {3164} INFO - iteration 479, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:21:59] {3344} INFO -  at 476.2s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:21:59] {3164} INFO - iteration 480, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:22:10] {3344} INFO -  at 487.4s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:22:10] {3164} INFO - iteration 481, current learner lgbm\n",
      "[flaml.automl: 03-16 13:22:10] {3344} INFO -  at 487.8s,\testimator lgbm's best error=0.2856,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:22:10] {3164} INFO - iteration 482, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:22:21] {3344} INFO -  at 498.9s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:22:21] {3164} INFO - iteration 483, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:22:33] {3344} INFO -  at 510.1s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:22:33] {3164} INFO - iteration 484, current learner lgbm\n",
      "[flaml.automl: 03-16 13:22:33] {3344} INFO -  at 510.5s,\testimator lgbm's best error=0.2856,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:22:33] {3164} INFO - iteration 485, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:22:44] {3344} INFO -  at 521.6s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:22:44] {3164} INFO - iteration 486, current learner extra_tree\n",
      "[flaml.automl: 03-16 13:22:45] {3344} INFO -  at 522.0s,\testimator extra_tree's best error=0.2944,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:22:45] {3164} INFO - iteration 487, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:22:54] {3344} INFO -  at 531.7s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:22:54] {3164} INFO - iteration 488, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:23:05] {3344} INFO -  at 542.9s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:23:05] {3164} INFO - iteration 489, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:23:15] {3344} INFO -  at 552.7s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:23:15] {3164} INFO - iteration 490, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:23:15] {3344} INFO -  at 553.0s,\testimator xgb_limitdepth's best error=0.2848,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:23:15] {3164} INFO - iteration 491, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:23:27] {3344} INFO -  at 564.1s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:23:27] {3164} INFO - iteration 492, current learner xgboost\n",
      "[flaml.automl: 03-16 13:23:27] {3344} INFO -  at 564.4s,\testimator xgboost's best error=0.3214,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:23:27] {3164} INFO - iteration 493, current learner lgbm\n",
      "[flaml.automl: 03-16 13:23:27] {3344} INFO -  at 564.9s,\testimator lgbm's best error=0.2856,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:23:27] {3164} INFO - iteration 494, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:23:37] {3344} INFO -  at 574.6s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-16 13:23:37] {3164} INFO - iteration 495, current learner xgboost\n",
      "[flaml.automl: 03-16 13:23:37] {3344} INFO -  at 574.9s,\testimator xgboost's best error=0.3214,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:23:37] {3164} INFO - iteration 496, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:23:47] {3344} INFO -  at 584.6s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:23:47] {3164} INFO - iteration 497, current learner lrl1\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[flaml.automl: 03-16 13:23:59] {3344} INFO -  at 596.0s,\testimator lrl1's best error=0.2267,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:23:59] {3164} INFO - iteration 498, current learner rf\n",
      "[flaml.automl: 03-16 13:24:00] {3344} INFO -  at 597.1s,\testimator rf's best error=0.2784,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:24:00] {3164} INFO - iteration 499, current learner rf\n",
      "[flaml.automl: 03-16 13:24:00] {3344} INFO -  at 598.0s,\testimator rf's best error=0.2784,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:24:00] {3164} INFO - iteration 500, current learner rf\n",
      "[flaml.automl: 03-16 13:24:01] {3344} INFO -  at 598.8s,\testimator rf's best error=0.2784,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:24:01] {3164} INFO - iteration 501, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:24:02] {3344} INFO -  at 599.0s,\testimator xgb_limitdepth's best error=0.2848,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:24:02] {3164} INFO - iteration 502, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:24:02] {3344} INFO -  at 599.3s,\testimator xgb_limitdepth's best error=0.2848,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:24:02] {3164} INFO - iteration 503, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:24:02] {3344} INFO -  at 599.6s,\testimator xgb_limitdepth's best error=0.2848,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:24:02] {3164} INFO - iteration 504, current learner xgb_limitdepth\n",
      "[flaml.automl: 03-16 13:24:02] {3344} INFO -  at 599.8s,\testimator xgb_limitdepth's best error=0.2848,\tbest estimator lrl1's best error=0.2267\n",
      "[flaml.automl: 03-16 13:24:02] {3465} INFO - selected model: LogisticRegression(C=0.06249999999999999, n_jobs=-1, penalty='l1',\n",
      "                   solver='saga')\n",
      "[flaml.automl: 03-16 13:24:02] {2900} INFO - fit succeeded\n",
      "[flaml.automl: 03-16 13:24:02] {2901} INFO - Time taken to find the best model: 244.6297333240509\n"
     ]
    }
   ],
   "source": [
    "from flaml import AutoML\n",
    "clf = AutoML()\n",
    "automl_settings = {\n",
    "    \"time_budget\": 1,  # in seconds\n",
    "    \"metric\": 'accuracy',\n",
    "    \"task\": 'classification',\n",
    "    \"log_file_name\": \"iris.log\",\n",
    "    \"time_budget\" : 600\n",
    "}\n",
    "\n",
    "clf.fit(X_train = df_x_train_mm, y_train = y_train,\n",
    "        X_val = df_x_val_mm,y_val = y_val,\n",
    "        **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b86e151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prection loss (MSE): 0.27\n",
      "Prection accuracy: 0.73\n",
      "Prection PR AUC: 0.71\n"
     ]
    }
   ],
   "source": [
    "predicted = clf.predict(x_test_mm).flatten()\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, average_precision_score\n",
    "print('Prection loss (MSE):', mean_squared_error(\n",
    "    df_test_y, predicted).round(2))\n",
    "print('Prection accuracy:', accuracy_score(\n",
    "    df_test_y, predicted).round(2))\n",
    "print('Prection PR AUC:', average_precision_score(\n",
    "    df_test_y, predicted).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ef01d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9495207143773343,\n",
       " 0.14634155640208785,\n",
       " 0.5434151813494182,\n",
       " 0.7648039164421869,\n",
       " 0.02174178317453435,\n",
       " 0.6149124128422697,\n",
       " 0.4187886632378351,\n",
       " 0.780034794871283,\n",
       " 0.06293595772022183,\n",
       " 0.05086019283273852,\n",
       " 0.6522987206586056,\n",
       " 0.8172680181633564,\n",
       " 0.9669290698117047,\n",
       " 0.5725335227232381,\n",
       " 0.8556589917422525,\n",
       " 0.16437950801012544,\n",
       " 0.707204166083302,\n",
       " 0.771579080949372,\n",
       " 0.8948233233837275,\n",
       " 0.14218680114776464,\n",
       " 0.39726512449476115,\n",
       " 0.9939831706160779,\n",
       " 0.7583212599759614,\n",
       " 0.7301366208450345,\n",
       " 0.8424475549093478,\n",
       " 0.26676379407630524,\n",
       " 0.6811168238066367,\n",
       " 0.8765348145096145,\n",
       " 0.6373335253082576,\n",
       " 0.9949249062610134,\n",
       " 0.9390374104051291,\n",
       " 0.929364341562022,\n",
       " 0.8558019193710585,\n",
       " 0.40217263660991726,\n",
       " 0.8132615357200014,\n",
       " 0.40669258553726534,\n",
       " 0.505851610813712,\n",
       " 0.24149539212939158,\n",
       " 0.896922775228227,\n",
       " 0.07520753680548836,\n",
       " 0.4929411523938359,\n",
       " 0.2451065422370327,\n",
       " 0.8214058869459417,\n",
       " 0.5495544341416481,\n",
       " 0.573568707227489,\n",
       " 0.16878428648187718,\n",
       " 0.905303828802339,\n",
       " 0.16455309667548984,\n",
       " 0.662807884983237,\n",
       " 0.9320436539306118,\n",
       " 0.026991663168175934,\n",
       " 0.10529034412402202,\n",
       " 0.8663471552314213,\n",
       " 0.7227965409549042,\n",
       " 0.5888360289877672,\n",
       " 0.6572809245158165,\n",
       " 0.4834730467898816,\n",
       " 0.535335010771842,\n",
       " 0.81180532390982,\n",
       " 0.5935735622920495,\n",
       " 0.16030946548456354,\n",
       " 0.7570956949356653,\n",
       " 0.3949371974404855,\n",
       " 0.8958895906541491,\n",
       " 0.602864808863548,\n",
       " 0.1674885348783309,\n",
       " 0.7631520446654305,\n",
       " 0.9568975729957993,\n",
       " 0.9581871819660142,\n",
       " 0.46016442516879796,\n",
       " 0.8875095784775113,\n",
       " 0.3452641220573463,\n",
       " 0.45068981899472516,\n",
       " 0.8380740008877687,\n",
       " 0.5032192692634255,\n",
       " 0.4660389919333866,\n",
       " 0.5810026119822528,\n",
       " 0.8475180837290851,\n",
       " 0.8833034105416839,\n",
       " 0.20107173683593416,\n",
       " 0.6908696798889532,\n",
       " 0.7637170045141195,\n",
       " 0.5395523975079548,\n",
       " 0.8125724191722005,\n",
       " 0.3748907138741622,\n",
       " 0.2602298567165142,\n",
       " 0.9180508622002685,\n",
       " 0.8353814438366759,\n",
       " 0.32712625036879167,\n",
       " 0.5128066223206968,\n",
       " 0.30241811303031724,\n",
       " 0.34561150956382786,\n",
       " 0.787271528741575,\n",
       " 0.8072596924201965,\n",
       " 0.6898378249611035,\n",
       " 0.7292339908601905,\n",
       " 0.3476993785358369,\n",
       " 0.38509044065603876,\n",
       " 0.767172139183441,\n",
       " 0.8050638370754376,\n",
       " 0.49648052681113214,\n",
       " 0.7605507043158829,\n",
       " 0.4919983877370548,\n",
       " 0.7682704095476832,\n",
       " 0.8798579519899009,\n",
       " 0.09953779105341838,\n",
       " 0.7710504055548791,\n",
       " 0.8106000214794731,\n",
       " 0.20541758375649122,\n",
       " 0.8745426495586711,\n",
       " 0.543996380851453,\n",
       " 0.16253052696594672,\n",
       " 0.2223448619372804,\n",
       " 0.9011301886768491,\n",
       " 0.20657460033857686,\n",
       " 0.11326685465690975,\n",
       " 0.4886713366881173,\n",
       " 0.5250624832529497,\n",
       " 0.4121128157454598,\n",
       " 0.9016377255891876,\n",
       " 0.47794449867130423,\n",
       " 0.6421896231142684,\n",
       " 0.4279560799634417,\n",
       " 0.8731874838022319,\n",
       " 0.8183090594208671,\n",
       " 0.27183403332936323,\n",
       " 0.5596570646380726,\n",
       " 0.4769736764996223,\n",
       " 0.9086901837241187,\n",
       " 0.7039675897915987,\n",
       " 0.7131501182348303,\n",
       " 0.062066650683568717,\n",
       " 0.8381309126894723,\n",
       " 0.6032734030706146,\n",
       " 0.4139866784403356,\n",
       " 0.8839248815040404,\n",
       " 0.07594253831863594,\n",
       " 0.31257162536605204,\n",
       " 0.870967020756392,\n",
       " 0.8806449315509034,\n",
       " 0.48442572482439494,\n",
       " 0.5127069434235925,\n",
       " 0.6662501690672578,\n",
       " 0.7270926053670379,\n",
       " 0.43783255904380536,\n",
       " 0.6459647333866129,\n",
       " 0.9165734302694485,\n",
       " 0.4512085816409362,\n",
       " 0.9308426864739676,\n",
       " 0.8810357018492597,\n",
       " 0.3621536117866908,\n",
       " 0.6815136124231291,\n",
       " 0.1670739608474415,\n",
       " 0.8043235639255255,\n",
       " 0.9552816383372493,\n",
       " 0.3589608443663867,\n",
       " 0.7561337139785117,\n",
       " 0.8899090422679826,\n",
       " 0.19472737747103402,\n",
       " 0.922877071098698,\n",
       " 0.37895007928342517,\n",
       " 0.6709077909216252,\n",
       " 0.5052248135882733,\n",
       " 0.1456335722833813,\n",
       " 0.7040679237577786,\n",
       " 0.9378047176309814,\n",
       " 0.4466860075661156,\n",
       " 0.8031529881183526,\n",
       " 0.9488899042229415,\n",
       " 0.34846312910134697,\n",
       " 0.8561800798470628,\n",
       " 0.3300729142588487,\n",
       " 0.9546785029502767,\n",
       " 0.5164666288692151,\n",
       " 0.794553073814057,\n",
       " 0.8187373321896072,\n",
       " 0.8264259107317282,\n",
       " 0.3202697480424361,\n",
       " 0.518239928009401,\n",
       " 0.13381237117027805,\n",
       " 0.7095109207349324,\n",
       " 0.8184134895412718,\n",
       " 0.4035734163806892,\n",
       " 0.9157164655924985,\n",
       " 0.6614385659064855,\n",
       " 0.8954148862506883,\n",
       " 0.4876394408480187,\n",
       " 0.45350280573379853,\n",
       " 0.7326379197372372,\n",
       " 0.8745367308765336,\n",
       " 0.1768984198077557,\n",
       " 0.344551812935503,\n",
       " 0.8015106180522463,\n",
       " 0.262601486058438,\n",
       " 0.5368895561045164,\n",
       " 0.9823386063866949,\n",
       " 0.5166625719252447,\n",
       " 0.9334175703534439,\n",
       " 0.6298112427476614,\n",
       " 0.4590596137719444,\n",
       " 0.47176130192434707,\n",
       " 0.7019691855895216,\n",
       " 0.32645557560435434,\n",
       " 0.30542480663117816,\n",
       " 0.6780895554517229,\n",
       " 0.28066013267721107,\n",
       " 0.6791933254234419,\n",
       " 0.2562544430660965,\n",
       " 0.562305126079733,\n",
       " 0.3526724954874485,\n",
       " 0.7349516912281427,\n",
       " 0.21719136412761098,\n",
       " 0.2577233841483609,\n",
       " 0.408378348982071,\n",
       " 0.3506639493420576,\n",
       " 0.9982142828686176,\n",
       " 0.05234333448992678,\n",
       " 0.8405539291156947,\n",
       " 0.13838174973672976,\n",
       " 0.8298547729893392,\n",
       " 0.4738778991702554,\n",
       " 0.5431079270934055,\n",
       " 0.8923486264401822,\n",
       " 0.29624400858056804,\n",
       " 0.42855400588411596,\n",
       " 0.957849413662527,\n",
       " 0.1559986924116635,\n",
       " 0.6222031465335889,\n",
       " 0.3614913619768329,\n",
       " 0.25347262971367507,\n",
       " 0.3944802253574104,\n",
       " 0.7948700246675972,\n",
       " 0.6953397613665686,\n",
       " 0.38899579613355484,\n",
       " 0.26969938703845436,\n",
       " 0.9478063843553773,\n",
       " 0.6135977830172209,\n",
       " 0.25753095684398747,\n",
       " 0.6923954434546856,\n",
       " 0.8834774863024827,\n",
       " 0.11517085496876871,\n",
       " 0.9711084796918069,\n",
       " 0.25199968640243997,\n",
       " 0.6231958963840626,\n",
       " 0.884243261223892,\n",
       " 0.9651898886873149,\n",
       " 0.2726658934744658,\n",
       " 0.30725939669791263,\n",
       " 0.2320173547936319,\n",
       " 0.5191440653965059,\n",
       " 0.42104181403853747,\n",
       " 0.20080746643470274,\n",
       " 0.9479459037523236,\n",
       " 0.5959679073766591,\n",
       " 0.8090694044602335,\n",
       " 0.15765764612714767,\n",
       " 0.8384999363558452,\n",
       " 0.545201723121046,\n",
       " 0.5176070281378736,\n",
       " 0.891293639694292,\n",
       " 0.7416448448855217,\n",
       " 0.4206262061540505,\n",
       " 0.9009212234228889,\n",
       " 0.4475818944650767,\n",
       " 0.04259930356467896,\n",
       " 0.9110788226533154,\n",
       " 0.7088092733930089,\n",
       " 0.6219772407565958,\n",
       " 0.4703296932041991,\n",
       " 0.28695635225382915,\n",
       " 0.1290479278116962,\n",
       " 0.6170824706035919,\n",
       " 0.7225200534395941,\n",
       " 0.7050959253549428,\n",
       " 0.4590803035247495,\n",
       " 0.7017499943178249,\n",
       " 0.9312151063978076,\n",
       " 0.705197876245874,\n",
       " 0.42946642679654623,\n",
       " 0.9357772706093008,\n",
       " 0.724050218364663,\n",
       " 0.15539105145719867,\n",
       " 0.9642247851837105,\n",
       " 0.7615416493192998,\n",
       " 0.7071390713556626,\n",
       " 0.6645851801667262,\n",
       " 0.9212738362777703,\n",
       " 0.668450633381524,\n",
       " 0.3139618625520274,\n",
       " 0.5740030855322267,\n",
       " 0.9280833907142195,\n",
       " 0.7509618779935409,\n",
       " 0.21026549504455821,\n",
       " 0.4139026830444831,\n",
       " 0.33916134329486525,\n",
       " 0.48380647508928853,\n",
       " 0.7527994042231719,\n",
       " 0.7316373766072028,\n",
       " 0.8897454619486227,\n",
       " 0.015950005887525327,\n",
       " 0.44678974250454156,\n",
       " 0.7934672545351521,\n",
       " 0.7560598241149086,\n",
       " 0.26045238630389844,\n",
       " 0.7845411114853648,\n",
       " 0.9461876396476058,\n",
       " 0.227252665265633,\n",
       " 0.5305492148859545,\n",
       " 0.959185579586843,\n",
       " 0.8539937818031027,\n",
       " 0.44375213514504075,\n",
       " 0.45730520427619487,\n",
       " 0.9732679261664586,\n",
       " 0.7401736380434778,\n",
       " 0.9794906434195702,\n",
       " 0.19549981062197966,\n",
       " 0.8920264479583219,\n",
       " 0.7779798574218666,\n",
       " 0.916424548832002,\n",
       " 0.45207690502188397,\n",
       " 0.6349173307577654,\n",
       " 0.5348066169787157,\n",
       " 0.9501961348915676,\n",
       " 0.09633290643690645,\n",
       " 0.8810273950393339,\n",
       " 0.8402539210694171,\n",
       " 0.7898772416251104,\n",
       " 0.5811589724990915,\n",
       " 0.4629177390966856,\n",
       " 0.9080250609537902,\n",
       " 0.5330042615612441,\n",
       " 0.9193538780899846,\n",
       " 0.19980066699860843,\n",
       " 0.8636545138226694,\n",
       " 0.8947195772984002,\n",
       " 0.8850982070287972,\n",
       " 0.7999871525804176,\n",
       " 0.9618252804440472,\n",
       " 0.22165189178495612,\n",
       " 0.08947203099616605,\n",
       " 0.9753308840966037,\n",
       " 0.8181398388232949,\n",
       " 0.18702623458543677,\n",
       " 0.7631974751071482,\n",
       " 0.43369527274212943,\n",
       " 0.2771544730865559,\n",
       " 0.693405954533195,\n",
       " 0.4419250035299258,\n",
       " 0.6477854106593763,\n",
       " 0.4184467097208472,\n",
       " 0.35568127726114357,\n",
       " 0.16409757423922577,\n",
       " 0.47528954327540057,\n",
       " 0.5083185516993505,\n",
       " 0.5970477958041208,\n",
       " 0.5405746389172804,\n",
       " 0.21838918843796223,\n",
       " 0.5235380061909011,\n",
       " 0.9547288290650963,\n",
       " 0.6284093683828186,\n",
       " 0.3130885115954501,\n",
       " 0.6428019775138227,\n",
       " 0.6031000642115542,\n",
       " 0.36552860668095216,\n",
       " 0.3010203280063143,\n",
       " 0.21387219352782205,\n",
       " 0.3467199663717708,\n",
       " 0.9364138954706477,\n",
       " 0.32925093665738087,\n",
       " 0.5863577601078237,\n",
       " 0.9362270776368273,\n",
       " 0.9585997507197012,\n",
       " 0.9744931809621544,\n",
       " 0.4022348082204676,\n",
       " 0.1357553995716862,\n",
       " 0.18817348607115852,\n",
       " 0.5071947304861373,\n",
       " 0.9435849965546549,\n",
       " 0.3214772436666914,\n",
       " 0.7851833295763334,\n",
       " 0.783641442024772,\n",
       " 0.021308880289064407,\n",
       " 0.8605073713707748,\n",
       " 0.4809931701372418,\n",
       " 0.8492343651909166,\n",
       " 0.9540971612533103,\n",
       " 0.7758847776722294,\n",
       " 0.21462282367531868,\n",
       " 0.7413666723965133,\n",
       " 0.7499331417859766,\n",
       " 0.6629992754761683,\n",
       " 0.3780001080813185,\n",
       " 0.8539035096016941,\n",
       " 0.9646641203235484,\n",
       " 0.8816494464445431,\n",
       " 0.912487162393458,\n",
       " 0.7349265908733152,\n",
       " 0.3996750066671175,\n",
       " 0.7585915582791187,\n",
       " 0.36620069030749625,\n",
       " 0.45828947392010777,\n",
       " 0.7383311747917976,\n",
       " 0.6664049301589668,\n",
       " 0.3916464670809273,\n",
       " 0.2507892575639055,\n",
       " 0.6760846898675948,\n",
       " 0.8311902343111857,\n",
       " 0.5285621389495979,\n",
       " 0.9489082284802911,\n",
       " 0.6697426605999769,\n",
       " 0.07552168831002913,\n",
       " 0.42054635502092286,\n",
       " 0.6221548397342318,\n",
       " 0.8596463869913544,\n",
       " 0.4612563543180484,\n",
       " 0.3045888819707587,\n",
       " 0.14696496240748402,\n",
       " 0.4909762343663607,\n",
       " 0.753722178993504,\n",
       " 0.484942801018906,\n",
       " 0.9218169097309626,\n",
       " 0.3611312714410816,\n",
       " 0.3470182152161343,\n",
       " 0.6243314870526445,\n",
       " 0.9200860974428047,\n",
       " 0.8415591225271779,\n",
       " 0.6794482107002003,\n",
       " 0.17392012217461758,\n",
       " 0.22910503694638734,\n",
       " 0.9175387677696242,\n",
       " 0.11863838787827727,\n",
       " 0.27423175991217785,\n",
       " 0.42735063303684945,\n",
       " 0.8899113011648788,\n",
       " 0.33605884821878235,\n",
       " 0.4029811728502765,\n",
       " 0.15881276328815566,\n",
       " 0.6909150375951052,\n",
       " 0.3347377230877134,\n",
       " 0.7007968310350803,\n",
       " 0.5469711303011716,\n",
       " 0.7839483166102118,\n",
       " 0.684415737243665,\n",
       " 0.2930473606991484,\n",
       " 0.4071887767374437,\n",
       " 0.7775176747105721,\n",
       " 0.8593661044978911,\n",
       " 0.3180833117707763,\n",
       " 0.9110660450790377,\n",
       " 0.6065884312084684,\n",
       " 0.3451206760238876,\n",
       " 0.7339042047448063,\n",
       " 0.026522610283619925,\n",
       " 0.37152094336132024,\n",
       " 0.8540947740329203,\n",
       " 0.2752262399514878,\n",
       " 0.08679845682920848,\n",
       " 0.7921070630059576,\n",
       " 0.7024253324279613,\n",
       " 0.13815357328703162,\n",
       " 0.2761224359263583,\n",
       " 0.4837573704140718,\n",
       " 0.9820531871796057,\n",
       " 0.8190621954294559,\n",
       " 0.8479246501287435,\n",
       " 0.7925009919060731,\n",
       " 0.18115832248837696,\n",
       " 0.8525733636496232,\n",
       " 0.16796507702345181,\n",
       " 0.7613121287547573,\n",
       " 0.5563062626617709,\n",
       " 0.737561853599809,\n",
       " 0.8157349021317999,\n",
       " 0.36831648290686736,\n",
       " 0.4155259239865093,\n",
       " 0.39240324461771364,\n",
       " 0.8721962361478482,\n",
       " 0.7103765658068132,\n",
       " 0.4420976683911246,\n",
       " 0.884887389173152,\n",
       " 0.8508129914070366,\n",
       " 0.3789535021189123,\n",
       " 0.4937321510466768,\n",
       " 0.2616945161479114,\n",
       " 0.13225272227934406,\n",
       " 0.914013611011839,\n",
       " 0.5268592686751195,\n",
       " 0.9619456233135385,\n",
       " 0.783331611091075,\n",
       " 0.6131594151440182,\n",
       " 0.7754621230911307,\n",
       " 0.44257777146726135,\n",
       " 0.5076922808932482,\n",
       " 0.6896963870114861,\n",
       " 0.7947103127050413,\n",
       " 0.8064675874142707,\n",
       " 0.565957215900551,\n",
       " 0.47593148516349865,\n",
       " 0.9187479871946865,\n",
       " 0.6161334845954789,\n",
       " 0.9262637214518151,\n",
       " 0.5740600650569949,\n",
       " 0.9492356348990718,\n",
       " 0.7407436662556063,\n",
       " 0.7996836027436892,\n",
       " 0.2647902878620727,\n",
       " 0.6850979655202464,\n",
       " 0.508609692718301,\n",
       " 0.5487812741994361,\n",
       " 0.359859686473781,\n",
       " 0.798239709028658,\n",
       " 0.9270809525240007,\n",
       " 0.7363470846514084,\n",
       " 0.4685665949589502,\n",
       " 0.6276487283267074,\n",
       " 0.5453167306615968,\n",
       " 0.6257920226707776,\n",
       " 0.20877677262883226,\n",
       " 0.6805653951334816,\n",
       " 0.8805429310518792,\n",
       " 0.09984908886938668,\n",
       " 0.7754294361881904,\n",
       " 0.42843764646558236,\n",
       " 0.08931532937289835,\n",
       " 0.2962149037932148,\n",
       " 0.5706765548721887,\n",
       " 0.7237408149208547,\n",
       " 0.6610064187220488,\n",
       " 0.297803727632798,\n",
       " 0.06607542307632859,\n",
       " 0.8509777146972425,\n",
       " 0.7191297625806904,\n",
       " 0.6930464694543511,\n",
       " 0.7538053202574412,\n",
       " 0.6149714625659035,\n",
       " 0.08882736868101601,\n",
       " 0.23015527013763745,\n",
       " 0.7706862007173194,\n",
       " 0.6102586398771335,\n",
       " 0.6968924142016611,\n",
       " 0.9184673164786232,\n",
       " 0.22445722565436385,\n",
       " 0.8450301759536312,\n",
       " 0.9112479922270543,\n",
       " 0.6397891508980214,\n",
       " 0.20911015870886235,\n",
       " 0.8124967139103709,\n",
       " 0.3744012701579997,\n",
       " 0.4887081669021162,\n",
       " 0.062026192430468564,\n",
       " 0.4322574404650344,\n",
       " 0.6226647814887296,\n",
       " 0.4129226492132472,\n",
       " 0.6876829919399935,\n",
       " 0.46019805799000973,\n",
       " 0.6447941037010236,\n",
       " 0.9404029059425646,\n",
       " 0.8201506819239026,\n",
       " 0.07906937350655696,\n",
       " 0.8645229424225449,\n",
       " 0.17767757779258136,\n",
       " 0.5715307298496385,\n",
       " 0.9233758855617182,\n",
       " 0.877022032320009,\n",
       " 0.5965908719612182,\n",
       " 0.5568211008269531,\n",
       " 0.8405726662900486,\n",
       " 0.31255718766786084,\n",
       " 0.9389855154369365,\n",
       " 0.19218871601555781,\n",
       " 0.4587034432797185,\n",
       " 0.6593108435910197,\n",
       " 0.6688581059020552,\n",
       " 0.5022036909492974,\n",
       " 0.01835141563764799,\n",
       " 0.17486439541385607,\n",
       " 0.3995385882255135,\n",
       " 0.49998812722987673,\n",
       " 0.874870942637484,\n",
       " 0.19007869955279516,\n",
       " 0.05573971783859474,\n",
       " 0.8557376724176561,\n",
       " 0.5596762231121927,\n",
       " 0.8825151641925829,\n",
       " 0.45055225366008744,\n",
       " 0.43558055940091267,\n",
       " 0.6648412969169547,\n",
       " 0.46578232221147264,\n",
       " 0.7710547821798602,\n",
       " 0.20897370870285367,\n",
       " 0.12938866333975066,\n",
       " 0.803642988686006,\n",
       " 0.2750368407831609,\n",
       " 0.6161512418125241,\n",
       " 0.48383570453427466,\n",
       " 0.18131019809888674,\n",
       " 0.04394601848417652,\n",
       " 0.28704233067600815,\n",
       " 0.17848010555319277,\n",
       " 0.059878782420611766,\n",
       " 0.8284427319909042,\n",
       " 0.9539877563151241,\n",
       " 0.8446859452885637,\n",
       " 0.9252568271302566,\n",
       " 0.9154688301369953,\n",
       " 0.9630353410781697,\n",
       " 0.31160955367965354,\n",
       " 0.28399448439132924,\n",
       " 0.030052196577429132,\n",
       " 0.6667318512497077,\n",
       " 0.3216889009694771,\n",
       " 0.1500710083816515,\n",
       " 0.5874625863395461,\n",
       " 0.18851890531332324,\n",
       " 0.8719511528216459,\n",
       " 0.036871600872047036,\n",
       " 0.09764530778653252,\n",
       " 0.07074825601239888,\n",
       " 0.8656695375285288,\n",
       " 0.7626356812569518,\n",
       " 0.12133189691228245,\n",
       " 0.1388978301362808,\n",
       " 0.9401741530269927,\n",
       " 0.9965737343586505,\n",
       " 0.7239183191235296,\n",
       " 0.4843029209815157,\n",
       " 0.944374325500361,\n",
       " 0.8421817758236648,\n",
       " 0.3674499720888405,\n",
       " 0.2961236525346782,\n",
       " 0.12790437254792772,\n",
       " 0.5585103504644789,\n",
       " 0.6295714999391259,\n",
       " 0.8489500551146569,\n",
       " 0.669624877196219,\n",
       " 0.8454509554423605,\n",
       " 0.9913815360546322,\n",
       " 0.04227485774766313,\n",
       " 0.45130910684615927,\n",
       " 0.665776044740655,\n",
       " 0.8180860126241274,\n",
       " 0.5156861338769214,\n",
       " 0.6002167841083064,\n",
       " 0.968150574946401,\n",
       " 0.9480153496783457,\n",
       " 0.4130127759013207,\n",
       " 0.6769666725367254,\n",
       " 0.06518984214162772,\n",
       " 0.604901643848968,\n",
       " 0.21377374636659946,\n",
       " 0.7824223289813554,\n",
       " 0.5797677796443113,\n",
       " 0.9241073930451871,\n",
       " 0.6537903043921464,\n",
       " 0.3865439080450474,\n",
       " 0.667933780312986,\n",
       " 0.8887270955675515,\n",
       " 0.3729517519098962,\n",
       " 0.43145245951525535,\n",
       " 0.4343622162312312,\n",
       " 0.1530959268849962,\n",
       " 0.14311042911519967,\n",
       " 0.6774916943660739,\n",
       " 0.949243495302921,\n",
       " 0.24023523978802205,\n",
       " 0.5604183436083733,\n",
       " 0.16014480298281286,\n",
       " 0.32803568272838,\n",
       " 0.21772260253838388,\n",
       " 0.8772408602977247,\n",
       " 0.8692295088632983,\n",
       " 0.5569845765158333,\n",
       " 0.1731745700759015,\n",
       " 0.08094637641300616,\n",
       " 0.6158918864711115,\n",
       " 0.6546211705591168,\n",
       " 0.1390799544378923,\n",
       " 0.8698216131868551,\n",
       " 0.7571039264310043,\n",
       " 0.9798554612242798,\n",
       " 0.3949663177774199,\n",
       " 0.9181868333383045,\n",
       " 0.06473912870917685,\n",
       " 0.5726506160923652,\n",
       " 0.3982438960504985,\n",
       " 0.693042479115987,\n",
       " 0.38374678371532495,\n",
       " 0.1845189407438211,\n",
       " 0.28104537035680305,\n",
       " 0.2068539816154999,\n",
       " 0.9572885381198191,\n",
       " 0.9609582448984474,\n",
       " 0.25548650075066326,\n",
       " 0.9851999980034084,\n",
       " 0.5832338098773663,\n",
       " 0.9549336232416529,\n",
       " 0.1383429821374291,\n",
       " 0.2515280910339348,\n",
       " 0.20726081917071965,\n",
       " 0.4948569621095624,\n",
       " 0.9453707778039341,\n",
       " 0.7885404606550819,\n",
       " 0.7661820963002326,\n",
       " 0.03596865849788687,\n",
       " 0.9360344452004593,\n",
       " 0.7265175676160873,\n",
       " 0.770706406645112,\n",
       " 0.9413414354465311,\n",
       " 0.77956812805592,\n",
       " 0.6606512566137062,\n",
       " 0.84497948592215,\n",
       " 0.37209598563553103,\n",
       " 0.9284806171680572,\n",
       " 0.4794372609976216,\n",
       " 0.44609414260151486,\n",
       " 0.2309466933979716,\n",
       " 0.057146787425766454,\n",
       " 0.7051535126243383,\n",
       " 0.919726635618806,\n",
       " 0.6004682503674508,\n",
       " 0.8526590355380008,\n",
       " 0.8769431011696743,\n",
       " 0.9886123397810163,\n",
       " 0.6501887065560796,\n",
       " 0.3971601788503109,\n",
       " 0.9509378389469973,\n",
       " 0.1894143351682984,\n",
       " 0.14759382528158796,\n",
       " 0.5571867864332858,\n",
       " 0.7072341774154147,\n",
       " 0.6730477790354933,\n",
       " 0.5283786065332827,\n",
       " 0.8798602370228782,\n",
       " 0.8488292206178474,\n",
       " 0.922788040305695,\n",
       " 0.42784076775093743,\n",
       " 0.8735601967659332,\n",
       " 0.919528212847614,\n",
       " 0.09577508266715251,\n",
       " 0.3520034638159326,\n",
       " 0.8142578543595139,\n",
       " 0.10710486077645394,\n",
       " 0.9310065025226811,\n",
       " 0.3723777162480145,\n",
       " 0.23339358545072253,\n",
       " 0.7903717241836583,\n",
       " 0.9239384828944052,\n",
       " 0.6884752725669144,\n",
       " 0.49193389151624817,\n",
       " 0.3528375136519441,\n",
       " 0.27593748503819326,\n",
       " 0.05679871869867835,\n",
       " 0.9744394334505491,\n",
       " 0.8458856016525306,\n",
       " 0.45897878655006524,\n",
       " 0.807365478987174,\n",
       " 0.9699542059508945,\n",
       " 0.15273162870801632,\n",
       " 0.7774829321829713,\n",
       " 0.8032806191721108,\n",
       " 0.04641377838029812,\n",
       " 0.4285508271654523,\n",
       " 0.7794752991060963,\n",
       " 0.36596703104098116,\n",
       " 0.4247720074512188,\n",
       " 0.6128014867077092,\n",
       " 0.8059694636416148,\n",
       " 0.6583900222989545,\n",
       " 0.5655456351805993,\n",
       " 0.16839464804483975,\n",
       " 0.3148468136504032,\n",
       " 0.6397084742016177,\n",
       " 0.053928948189134365,\n",
       " 0.9258404992951296,\n",
       " 0.9480714329011872,\n",
       " 0.7743612217006627,\n",
       " 0.9050634618227504,\n",
       " 0.9159692495945096,\n",
       " 0.06877543260604249,\n",
       " 0.23627211801059395,\n",
       " 0.17609129593179795,\n",
       " 0.954982602213886,\n",
       " 0.4131974094001559,\n",
       " 0.05817395890765641,\n",
       " 0.09647896223088127,\n",
       " 0.9104742961600999,\n",
       " 0.44054852155757623,\n",
       " 0.2445812734579572,\n",
       " 0.8469028318539406,\n",
       " 0.9104436695856784,\n",
       " 0.56953539938824,\n",
       " 0.8624073001324564,\n",
       " 0.4626361159752102,\n",
       " 0.20724531054137635,\n",
       " 0.17472861653209495,\n",
       " 0.2029180249971801,\n",
       " 0.6374380422020112,\n",
       " 0.35389730169839173,\n",
       " 0.6877769771504533,\n",
       " 0.9314721056983083,\n",
       " 0.39278950700732873,\n",
       " 0.32569971641568174,\n",
       " 0.8884233805811914,\n",
       " 0.4085440169112364,\n",
       " 0.7067809776244025,\n",
       " 0.06491445119287662,\n",
       " 0.29881558364958405,\n",
       " 0.46303045820661026,\n",
       " 0.8237034381846021,\n",
       " 0.6961997223292811,\n",
       " 0.8537775196739955,\n",
       " 0.07390154805707927,\n",
       " 0.441301054166712,\n",
       " 0.727548030187413,\n",
       " 0.39643787720531204,\n",
       " 0.3428007513782641,\n",
       " 0.7851465559567317,\n",
       " 0.4831183912949689,\n",
       " 0.5605119061422302,\n",
       " 0.47139864930561715,\n",
       " 0.13017759613989618,\n",
       " 0.4614855081073878,\n",
       " 0.6223374412400909,\n",
       " 0.045394860716916195,\n",
       " 0.2265805051059179,\n",
       " 0.8175102276528872,\n",
       " 0.8763035687931529,\n",
       " 0.8377492602959459,\n",
       " 0.7378540646709001,\n",
       " 0.8571985862124503,\n",
       " 0.5795119206248653,\n",
       " 0.9050106570419806,\n",
       " 0.594399313045813,\n",
       " 0.532649354340663,\n",
       " 0.6085438180045293,\n",
       " 0.0753191415104515,\n",
       " 0.03141268767207454,\n",
       " 0.6163614424234379,\n",
       " 0.8370984327879216,\n",
       " 0.650118283293482,\n",
       " 0.9841687514247157,\n",
       " 0.27322562401221895,\n",
       " 0.5924111345378614,\n",
       " 0.5448279369102763,\n",
       " 0.6444050436671067,\n",
       " 0.858611160138467,\n",
       " 0.024222686375669135,\n",
       " 0.047470129660824756,\n",
       " 0.4241789722910794,\n",
       " 0.9561511793139292,\n",
       " 0.3670242120645477,\n",
       " 0.9066998298824408,\n",
       " 0.8912029150594253,\n",
       " 0.07327331543594455,\n",
       " 0.31004036061297735,\n",
       " 0.8805954120552876,\n",
       " 0.36119295376878985,\n",
       " 0.8953851776025948,\n",
       " 0.4193551534124461,\n",
       " 0.47492556147097226,\n",
       " 0.5989836776317058,\n",
       " 0.3131365682861176,\n",
       " 0.4476895217766046,\n",
       " 0.7725092768049399,\n",
       " 0.9775712447947446,\n",
       " 0.0663716189186938,\n",
       " 0.39350754315444103,\n",
       " 0.24286240106439574,\n",
       " 0.36129778726434075,\n",
       " 0.6783455580903474,\n",
       " 0.6643184641578828,\n",
       " 0.6557394182620695,\n",
       " 0.2342683036797351,\n",
       " 0.2905620462211754,\n",
       " 0.454241535204529,\n",
       " 0.7159852953419468,\n",
       " 0.8921631952871851,\n",
       " 0.2775610935736392,\n",
       " 0.18346881586297353,\n",
       " 0.1894701829793113,\n",
       " 0.027499567753223816,\n",
       " 0.503766221411625,\n",
       " 0.7754896007501676,\n",
       " 0.23060787033994218,\n",
       " 0.856152919956753,\n",
       " 0.6803741737173116,\n",
       " 0.9839362246807697,\n",
       " 0.0854150518636094,\n",
       " 0.3072741937094568,\n",
       " 0.19808092507173408,\n",
       " 0.8490232902852769,\n",
       " 0.6329982405496053,\n",
       " 0.48120249066264953,\n",
       " 0.9041153847707851,\n",
       " 0.4576065274156863,\n",
       " 0.0958258108866557,\n",
       " 0.46085561362245725,\n",
       " 0.9920990693618189,\n",
       " 0.8745568492113508,\n",
       " 0.32995103805757664,\n",
       " 0.657570540820864,\n",
       " 0.02089536372488105,\n",
       " 0.2919600439369192,\n",
       " 0.8332310963913838,\n",
       " 0.35885498944217903,\n",
       " 0.15651776094060837,\n",
       " 0.16980992123507793,\n",
       " 0.31214454155664856,\n",
       " 0.3704591528148407,\n",
       " 0.38675418854376814,\n",
       " 0.7995609312879921,\n",
       " 0.5956893911404882,\n",
       " 0.08365722751919397,\n",
       " 0.7003715002457515,\n",
       " 0.5912413922496125,\n",
       " 0.3384375007938184,\n",
       " 0.0641287084470246,\n",
       " 0.12438563162658364,\n",
       " 0.1030027896000025,\n",
       " 0.8552037254947166,\n",
       " 0.7650691377464163,\n",
       " 0.5823141731592775,\n",
       " 0.9161135367795138,\n",
       " 0.953813156393441,\n",
       " 0.7328975075799464,\n",
       " 0.6757086568564215,\n",
       " 0.9694937880711916,\n",
       " 0.13313381630463794,\n",
       " 0.19859454041113084,\n",
       " 0.8065931177194919,\n",
       " 0.19131776979013634,\n",
       " 0.39541215918633416,\n",
       " 0.9862359708218573,\n",
       " 0.9514145824635818,\n",
       " 0.8662868883051146,\n",
       " 0.6688363143988975,\n",
       " 0.783084180158142,\n",
       " 0.9069162080855715,\n",
       " 0.7160848957275343,\n",
       " 0.1907078517236532,\n",
       " 0.7643664294251153,\n",
       " 0.8262648870282914,\n",
       " 0.21678113070895635,\n",
       " 0.24416159233916554,\n",
       " 0.6199625440885868,\n",
       " 0.8641907269464816,\n",
       " 0.6284318360685082,\n",
       " 0.2065210176423265,\n",
       " 0.0078528823440327,\n",
       " 0.552669294337485,\n",
       " 0.6170345847539119,\n",
       " 0.9082208394331815,\n",
       " 0.5686980556099082,\n",
       " 0.00908061254445026,\n",
       " 0.3956603073167732,\n",
       " 0.9544555270009287,\n",
       " 0.7846764366218719,\n",
       " 0.5266744217974305,\n",
       " 0.7223971565963965,\n",
       " 0.9324726002376579,\n",
       " 0.8269515000239949,\n",
       " 0.6404275033948444,\n",
       " 0.5164026683711419,\n",
       " 0.483698617903094,\n",
       " 0.9854148644418396,\n",
       " 0.9324506907877962,\n",
       " 0.9494261067955153,\n",
       " 0.4557303816180665,\n",
       " 0.05040559005207349,\n",
       " 0.530332895165132,\n",
       " 0.5245258279134217,\n",
       " 0.2261823326854722,\n",
       " 0.9615184286057885,\n",
       " 0.027253030321955845,\n",
       " 0.4611013228780306,\n",
       " 0.3268574236846641,\n",
       " 0.6253187375905154,\n",
       " 0.5026142408921914,\n",
       " 0.5773056100718577,\n",
       " 0.7939254202110693,\n",
       " 0.0840411728421642,\n",
       " 0.9944209026261535,\n",
       " 0.9085943592143827,\n",
       " 0.1757252578088985,\n",
       " 0.97508387082149,\n",
       " 0.03506063864506532,\n",
       " 0.4420769212294118,\n",
       " 0.97386885646917,\n",
       " 0.9887678494392073,\n",
       " 0.301026176991748,\n",
       " 0.9775061372634883,\n",
       " 0.9903098058729565,\n",
       " 0.31913079129961275,\n",
       " 0.7644361994912738,\n",
       " 0.6614900528887719,\n",
       " 0.11555742347576303,\n",
       " 0.7258012519193722,\n",
       " 0.6930857636452025,\n",
       " 0.07753837356670133,\n",
       " 0.6341256050786468,\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = []\n",
    "pred = clf.predict_proba(x_test_mm)\n",
    "for p in pred:\n",
    "    pre.append(p[1])\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e2568a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fper, tper):\n",
    "    auc_rate = auc(fper,tper)\n",
    "    plt.figure(figsize=(10, 10),facecolor='white')\n",
    "    plt.plot(fper, tper, color='red', label=f'ROC (AUC = {round(auc_rate,2)})')\n",
    "    plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a272a0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fper, tper, thresholds = roc_curve(df_test_y,predicted)\n",
    "plot_roc_curve(fper, tper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b75e5dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[472, 135],\n",
       "       [154, 312]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2= confusion_matrix(df_test_y,predicted,labels=[1,0]) \n",
    "C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "382365dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>HomeScore</th>\n",
       "      <th>Away</th>\n",
       "      <th>AwayScore</th>\n",
       "      <th>讓分</th>\n",
       "      <th>總分</th>\n",
       "      <th>Eventcode_x</th>\n",
       "      <th>主勝(初)</th>\n",
       "      <th>客勝(初)</th>\n",
       "      <th>主勝率(初)</th>\n",
       "      <th>...</th>\n",
       "      <th>Away_starters5_PF</th>\n",
       "      <th>Away_starters5_PTS</th>\n",
       "      <th>Away_starters5_Game_Score</th>\n",
       "      <th>Away_starters5_+/-</th>\n",
       "      <th>Away_starters5_noplay_PointDiff</th>\n",
       "      <th>客隊ELO</th>\n",
       "      <th>主隊ELO</th>\n",
       "      <th>win</th>\n",
       "      <th>Home_startersAll_+/-</th>\n",
       "      <th>Away_startersAll_+/-</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-20</th>\n",
       "      <td>LAL</td>\n",
       "      <td>114</td>\n",
       "      <td>GSW</td>\n",
       "      <td>121</td>\n",
       "      <td>3.5</td>\n",
       "      <td>225.5</td>\n",
       "      <td>429689</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.52</td>\n",
       "      <td>62.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>7.2</td>\n",
       "      <td>10.49</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2139.7097</td>\n",
       "      <td>2226.4286</td>\n",
       "      <td>0</td>\n",
       "      <td>15.104</td>\n",
       "      <td>13.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>BOS</td>\n",
       "      <td>83</td>\n",
       "      <td>TOR</td>\n",
       "      <td>115</td>\n",
       "      <td>6.5</td>\n",
       "      <td>217.5</td>\n",
       "      <td>431010</td>\n",
       "      <td>1.37</td>\n",
       "      <td>3.09</td>\n",
       "      <td>69.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.50</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>2129.2507</td>\n",
       "      <td>2206.8953</td>\n",
       "      <td>0</td>\n",
       "      <td>13.596</td>\n",
       "      <td>12.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>CHI</td>\n",
       "      <td>128</td>\n",
       "      <td>NOP</td>\n",
       "      <td>112</td>\n",
       "      <td>6.5</td>\n",
       "      <td>223.5</td>\n",
       "      <td>431011</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.18</td>\n",
       "      <td>70.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.73</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>2000.4870</td>\n",
       "      <td>2048.2998</td>\n",
       "      <td>1</td>\n",
       "      <td>14.444</td>\n",
       "      <td>9.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>DEN</td>\n",
       "      <td>102</td>\n",
       "      <td>SAS</td>\n",
       "      <td>96</td>\n",
       "      <td>7.5</td>\n",
       "      <td>222.5</td>\n",
       "      <td>431013</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.32</td>\n",
       "      <td>71.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>10.2</td>\n",
       "      <td>6.56</td>\n",
       "      <td>-8.3</td>\n",
       "      <td>2105.2663</td>\n",
       "      <td>2261.0786</td>\n",
       "      <td>1</td>\n",
       "      <td>12.546</td>\n",
       "      <td>11.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>SAC</td>\n",
       "      <td>101</td>\n",
       "      <td>UTA</td>\n",
       "      <td>110</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>227.5</td>\n",
       "      <td>431014</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.41</td>\n",
       "      <td>33.13</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10.6</td>\n",
       "      <td>9.85</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>2321.1965</td>\n",
       "      <td>2081.5910</td>\n",
       "      <td>0</td>\n",
       "      <td>14.614</td>\n",
       "      <td>12.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>CHI</td>\n",
       "      <td>124</td>\n",
       "      <td>IND</td>\n",
       "      <td>109</td>\n",
       "      <td>7.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484626</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.46</td>\n",
       "      <td>72.59</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>8.9</td>\n",
       "      <td>7.93</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1942.8995</td>\n",
       "      <td>2193.6864</td>\n",
       "      <td>1</td>\n",
       "      <td>13.276</td>\n",
       "      <td>14.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>MIN</td>\n",
       "      <td>134</td>\n",
       "      <td>SAS</td>\n",
       "      <td>122</td>\n",
       "      <td>9.5</td>\n",
       "      <td>235.5</td>\n",
       "      <td>484627</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.90</td>\n",
       "      <td>75.65</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>9.83</td>\n",
       "      <td>-7.7</td>\n",
       "      <td>2113.3237</td>\n",
       "      <td>2066.1966</td>\n",
       "      <td>1</td>\n",
       "      <td>14.510</td>\n",
       "      <td>11.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>UTA</td>\n",
       "      <td>109</td>\n",
       "      <td>HOU</td>\n",
       "      <td>101</td>\n",
       "      <td>8.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484628</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.79</td>\n",
       "      <td>65.93</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.84</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>1794.1439</td>\n",
       "      <td>2285.1027</td>\n",
       "      <td>1</td>\n",
       "      <td>11.280</td>\n",
       "      <td>10.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>DEN</td>\n",
       "      <td>110</td>\n",
       "      <td>LAL</td>\n",
       "      <td>99</td>\n",
       "      <td>5.5</td>\n",
       "      <td>232.5</td>\n",
       "      <td>484629</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.97</td>\n",
       "      <td>68.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>11.8</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2088.1050</td>\n",
       "      <td>2225.9103</td>\n",
       "      <td>1</td>\n",
       "      <td>16.328</td>\n",
       "      <td>15.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>POR</td>\n",
       "      <td>98</td>\n",
       "      <td>MIA</td>\n",
       "      <td>119</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>221.5</td>\n",
       "      <td>484630</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.95</td>\n",
       "      <td>51.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6.88</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2238.5712</td>\n",
       "      <td>2060.2239</td>\n",
       "      <td>0</td>\n",
       "      <td>15.966</td>\n",
       "      <td>14.432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1073 rows × 570 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Home  HomeScore Away  AwayScore   讓分     總分  Eventcode_x  主勝(初)  \\\n",
       "Matchtime                                                                    \n",
       "2021-10-20  LAL        114  GSW        121  3.5  225.5       429689   1.52   \n",
       "2021-10-23  BOS         83  TOR        115  6.5  217.5       431010   1.37   \n",
       "2021-10-23  CHI        128  NOP        112  6.5  223.5       431011   1.35   \n",
       "2021-10-23  DEN        102  SAS         96  7.5  222.5       431013   1.32   \n",
       "2021-10-23  SAC        101  UTA        110 -6.5  227.5       431014   2.85   \n",
       "...         ...        ...  ...        ...  ...    ...          ...    ...   \n",
       "2022-10-27  CHI        124  IND        109  7.5  232.5       484626   1.30   \n",
       "2022-10-27  MIN        134  SAS        122  9.5  235.5       484627   1.25   \n",
       "2022-10-27  UTA        109  HOU        101  8.5  232.5       484628   1.44   \n",
       "2022-10-27  DEN        110  LAL         99  5.5  232.5       484629   1.39   \n",
       "2022-10-27  POR         98  MIA        119 -2.5  221.5       484630   1.85   \n",
       "\n",
       "            客勝(初)  主勝率(初)  ...  Away_starters5_PF  Away_starters5_PTS  \\\n",
       "Matchtime                  ...                                          \n",
       "2021-10-20   2.52   62.35  ...                0.6                 2.9   \n",
       "2021-10-23   3.09   69.20  ...                0.8                 1.1   \n",
       "2021-10-23   3.18   70.02  ...                0.3                 0.6   \n",
       "2021-10-23   3.32   71.44  ...                0.9                 1.7   \n",
       "2021-10-23   1.41   33.13  ...                1.9                 2.5   \n",
       "...           ...     ...  ...                ...                 ...   \n",
       "2022-10-27   3.46   72.59  ...                1.0                 2.2   \n",
       "2022-10-27   3.90   75.65  ...                1.3                 1.5   \n",
       "2022-10-27   2.79   65.93  ...                1.2                 3.9   \n",
       "2022-10-27   2.97   68.05  ...                0.7                 1.7   \n",
       "2022-10-27   1.95   51.34  ...                0.9                 2.0   \n",
       "\n",
       "            Away_starters5_Game_Score  Away_starters5_+/-  \\\n",
       "Matchtime                                                   \n",
       "2021-10-20                        7.2               10.49   \n",
       "2021-10-23                        6.7                5.50   \n",
       "2021-10-23                        4.0                3.73   \n",
       "2021-10-23                       10.2                6.56   \n",
       "2021-10-23                       10.6                9.85   \n",
       "...                               ...                 ...   \n",
       "2022-10-27                        8.9                7.93   \n",
       "2022-10-27                       11.7                9.83   \n",
       "2022-10-27                        9.0                6.84   \n",
       "2022-10-27                       11.8                7.60   \n",
       "2022-10-27                        8.7                6.88   \n",
       "\n",
       "            Away_starters5_noplay_PointDiff      客隊ELO      主隊ELO  win  \\\n",
       "Matchtime                                                                \n",
       "2021-10-20                              3.0  2139.7097  2226.4286    0   \n",
       "2021-10-23                             -3.2  2129.2507  2206.8953    0   \n",
       "2021-10-23                             -5.4  2000.4870  2048.2998    1   \n",
       "2021-10-23                             -8.3  2105.2663  2261.0786    1   \n",
       "2021-10-23                             -1.1  2321.1965  2081.5910    0   \n",
       "...                                     ...        ...        ...  ...   \n",
       "2022-10-27                             -2.0  1942.8995  2193.6864    1   \n",
       "2022-10-27                             -7.7  2113.3237  2066.1966    1   \n",
       "2022-10-27                            -10.3  1794.1439  2285.1027    1   \n",
       "2022-10-27                              0.3  2088.1050  2225.9103    1   \n",
       "2022-10-27                              0.9  2238.5712  2060.2239    0   \n",
       "\n",
       "            Home_startersAll_+/-  Away_startersAll_+/-  \n",
       "Matchtime                                               \n",
       "2021-10-20                15.104                13.440  \n",
       "2021-10-23                13.596                12.362  \n",
       "2021-10-23                14.444                 9.148  \n",
       "2021-10-23                12.546                11.622  \n",
       "2021-10-23                14.614                12.786  \n",
       "...                          ...                   ...  \n",
       "2022-10-27                13.276                14.372  \n",
       "2022-10-27                14.510                11.776  \n",
       "2022-10-27                11.280                10.264  \n",
       "2022-10-27                16.328                15.546  \n",
       "2022-10-27                15.966                14.432  \n",
       "\n",
       "[1073 rows x 570 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df[\"2021-10\":]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c76c103e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>HomeScore</th>\n",
       "      <th>Away</th>\n",
       "      <th>AwayScore</th>\n",
       "      <th>客勝(終)</th>\n",
       "      <th>主勝(終)</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-20</th>\n",
       "      <td>LAL</td>\n",
       "      <td>114</td>\n",
       "      <td>GSW</td>\n",
       "      <td>121</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>BOS</td>\n",
       "      <td>83</td>\n",
       "      <td>TOR</td>\n",
       "      <td>115</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>CHI</td>\n",
       "      <td>128</td>\n",
       "      <td>NOP</td>\n",
       "      <td>112</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>DEN</td>\n",
       "      <td>102</td>\n",
       "      <td>SAS</td>\n",
       "      <td>96</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>SAC</td>\n",
       "      <td>101</td>\n",
       "      <td>UTA</td>\n",
       "      <td>110</td>\n",
       "      <td>1.38</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>CHI</td>\n",
       "      <td>124</td>\n",
       "      <td>IND</td>\n",
       "      <td>109</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>MIN</td>\n",
       "      <td>134</td>\n",
       "      <td>SAS</td>\n",
       "      <td>122</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>UTA</td>\n",
       "      <td>109</td>\n",
       "      <td>HOU</td>\n",
       "      <td>101</td>\n",
       "      <td>3.72</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>DEN</td>\n",
       "      <td>110</td>\n",
       "      <td>LAL</td>\n",
       "      <td>99</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>POR</td>\n",
       "      <td>98</td>\n",
       "      <td>MIA</td>\n",
       "      <td>119</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1073 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Home  HomeScore Away  AwayScore  客勝(終)  主勝(終)  win\n",
       "Matchtime                                                    \n",
       "2021-10-20  LAL        114  GSW        121   2.30   1.62    0\n",
       "2021-10-23  BOS         83  TOR        115   3.33   1.33    0\n",
       "2021-10-23  CHI        128  NOP        112   3.10   1.37    1\n",
       "2021-10-23  DEN        102  SAS         96   3.49   1.30    1\n",
       "2021-10-23  SAC        101  UTA        110   1.38   3.03    0\n",
       "...         ...        ...  ...        ...    ...    ...  ...\n",
       "2022-10-27  CHI        124  IND        109   3.33   1.33    1\n",
       "2022-10-27  MIN        134  SAS        122   4.38   1.21    1\n",
       "2022-10-27  UTA        109  HOU        101   3.72   1.28    1\n",
       "2022-10-27  DEN        110  LAL         99   2.64   1.49    1\n",
       "2022-10-27  POR         98  MIA        119   1.69   2.18    0\n",
       "\n",
       "[1073 rows x 7 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s = df_test[['Home','HomeScore','Away','AwayScore','客勝(終)','主勝(終)','win']]\n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f82e83b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_40024/141304865.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_s['predict'] = predicted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>HomeScore</th>\n",
       "      <th>Away</th>\n",
       "      <th>AwayScore</th>\n",
       "      <th>客勝(終)</th>\n",
       "      <th>主勝(終)</th>\n",
       "      <th>win</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-20</th>\n",
       "      <td>LAL</td>\n",
       "      <td>114</td>\n",
       "      <td>GSW</td>\n",
       "      <td>121</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>BOS</td>\n",
       "      <td>83</td>\n",
       "      <td>TOR</td>\n",
       "      <td>115</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>CHI</td>\n",
       "      <td>128</td>\n",
       "      <td>NOP</td>\n",
       "      <td>112</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>DEN</td>\n",
       "      <td>102</td>\n",
       "      <td>SAS</td>\n",
       "      <td>96</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>SAC</td>\n",
       "      <td>101</td>\n",
       "      <td>UTA</td>\n",
       "      <td>110</td>\n",
       "      <td>1.38</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>CHI</td>\n",
       "      <td>124</td>\n",
       "      <td>IND</td>\n",
       "      <td>109</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>MIN</td>\n",
       "      <td>134</td>\n",
       "      <td>SAS</td>\n",
       "      <td>122</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>UTA</td>\n",
       "      <td>109</td>\n",
       "      <td>HOU</td>\n",
       "      <td>101</td>\n",
       "      <td>3.72</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>DEN</td>\n",
       "      <td>110</td>\n",
       "      <td>LAL</td>\n",
       "      <td>99</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>POR</td>\n",
       "      <td>98</td>\n",
       "      <td>MIA</td>\n",
       "      <td>119</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1073 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Home  HomeScore Away  AwayScore  客勝(終)  主勝(終)  win  predict\n",
       "Matchtime                                                             \n",
       "2021-10-20  LAL        114  GSW        121   2.30   1.62    0        1\n",
       "2021-10-23  BOS         83  TOR        115   3.33   1.33    0        0\n",
       "2021-10-23  CHI        128  NOP        112   3.10   1.37    1        1\n",
       "2021-10-23  DEN        102  SAS         96   3.49   1.30    1        1\n",
       "2021-10-23  SAC        101  UTA        110   1.38   3.03    0        0\n",
       "...         ...        ...  ...        ...    ...    ...  ...      ...\n",
       "2022-10-27  CHI        124  IND        109   3.33   1.33    1        0\n",
       "2022-10-27  MIN        134  SAS        122   4.38   1.21    1        1\n",
       "2022-10-27  UTA        109  HOU        101   3.72   1.28    1        1\n",
       "2022-10-27  DEN        110  LAL         99   2.64   1.49    1        1\n",
       "2022-10-27  POR         98  MIA        119   1.69   2.18    0        0\n",
       "\n",
       "[1073 rows x 8 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s['predict'] = predicted \n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b21f3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_40024/1582062268.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_s['predict'] = pre\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>HomeScore</th>\n",
       "      <th>Away</th>\n",
       "      <th>AwayScore</th>\n",
       "      <th>客勝(終)</th>\n",
       "      <th>主勝(終)</th>\n",
       "      <th>win</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-20</th>\n",
       "      <td>LAL</td>\n",
       "      <td>114</td>\n",
       "      <td>GSW</td>\n",
       "      <td>121</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>BOS</td>\n",
       "      <td>83</td>\n",
       "      <td>TOR</td>\n",
       "      <td>115</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.146342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>CHI</td>\n",
       "      <td>128</td>\n",
       "      <td>NOP</td>\n",
       "      <td>112</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1</td>\n",
       "      <td>0.543415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>DEN</td>\n",
       "      <td>102</td>\n",
       "      <td>SAS</td>\n",
       "      <td>96</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>SAC</td>\n",
       "      <td>101</td>\n",
       "      <td>UTA</td>\n",
       "      <td>110</td>\n",
       "      <td>1.38</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>CHI</td>\n",
       "      <td>124</td>\n",
       "      <td>IND</td>\n",
       "      <td>109</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.238127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>MIN</td>\n",
       "      <td>134</td>\n",
       "      <td>SAS</td>\n",
       "      <td>122</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.668105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>UTA</td>\n",
       "      <td>109</td>\n",
       "      <td>HOU</td>\n",
       "      <td>101</td>\n",
       "      <td>3.72</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1</td>\n",
       "      <td>0.887495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>DEN</td>\n",
       "      <td>110</td>\n",
       "      <td>LAL</td>\n",
       "      <td>99</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-27</th>\n",
       "      <td>POR</td>\n",
       "      <td>98</td>\n",
       "      <td>MIA</td>\n",
       "      <td>119</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.361405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1073 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Home  HomeScore Away  AwayScore  客勝(終)  主勝(終)  win   predict\n",
       "Matchtime                                                              \n",
       "2021-10-20  LAL        114  GSW        121   2.30   1.62    0  0.949521\n",
       "2021-10-23  BOS         83  TOR        115   3.33   1.33    0  0.146342\n",
       "2021-10-23  CHI        128  NOP        112   3.10   1.37    1  0.543415\n",
       "2021-10-23  DEN        102  SAS         96   3.49   1.30    1  0.764804\n",
       "2021-10-23  SAC        101  UTA        110   1.38   3.03    0  0.021742\n",
       "...         ...        ...  ...        ...    ...    ...  ...       ...\n",
       "2022-10-27  CHI        124  IND        109   3.33   1.33    1  0.238127\n",
       "2022-10-27  MIN        134  SAS        122   4.38   1.21    1  0.668105\n",
       "2022-10-27  UTA        109  HOU        101   3.72   1.28    1  0.887495\n",
       "2022-10-27  DEN        110  LAL         99   2.64   1.49    1  0.945951\n",
       "2022-10-27  POR         98  MIA        119   1.69   2.18    0  0.361405\n",
       "\n",
       "[1073 rows x 8 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s['predict'] = pre\n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e252d11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>HomeScore</th>\n",
       "      <th>Away</th>\n",
       "      <th>AwayScore</th>\n",
       "      <th>客勝(終)</th>\n",
       "      <th>主勝(終)</th>\n",
       "      <th>win</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-20</th>\n",
       "      <td>LAL</td>\n",
       "      <td>114</td>\n",
       "      <td>GSW</td>\n",
       "      <td>121</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>BOS</td>\n",
       "      <td>83</td>\n",
       "      <td>TOR</td>\n",
       "      <td>115</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.146342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>CHI</td>\n",
       "      <td>128</td>\n",
       "      <td>NOP</td>\n",
       "      <td>112</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1</td>\n",
       "      <td>0.543415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>DEN</td>\n",
       "      <td>102</td>\n",
       "      <td>SAS</td>\n",
       "      <td>96</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>SAC</td>\n",
       "      <td>101</td>\n",
       "      <td>UTA</td>\n",
       "      <td>110</td>\n",
       "      <td>1.38</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-06</th>\n",
       "      <td>GSW</td>\n",
       "      <td>107</td>\n",
       "      <td>BOS</td>\n",
       "      <td>88</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-09</th>\n",
       "      <td>BOS</td>\n",
       "      <td>116</td>\n",
       "      <td>GSW</td>\n",
       "      <td>100</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-11</th>\n",
       "      <td>BOS</td>\n",
       "      <td>97</td>\n",
       "      <td>GSW</td>\n",
       "      <td>107</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-14</th>\n",
       "      <td>GSW</td>\n",
       "      <td>104</td>\n",
       "      <td>BOS</td>\n",
       "      <td>94</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-17</th>\n",
       "      <td>BOS</td>\n",
       "      <td>90</td>\n",
       "      <td>GSW</td>\n",
       "      <td>103</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1024 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Home  HomeScore Away  AwayScore  客勝(終)  主勝(終)  win   predict\n",
       "Matchtime                                                              \n",
       "2021-10-20  LAL        114  GSW        121   2.30   1.62    0  0.949521\n",
       "2021-10-23  BOS         83  TOR        115   3.33   1.33    0  0.146342\n",
       "2021-10-23  CHI        128  NOP        112   3.10   1.37    1  0.543415\n",
       "2021-10-23  DEN        102  SAS         96   3.49   1.30    1  0.764804\n",
       "2021-10-23  SAC        101  UTA        110   1.38   3.03    0  0.021742\n",
       "...         ...        ...  ...        ...    ...    ...  ...       ...\n",
       "2022-06-06  GSW        107  BOS         88   2.64   1.49    1  0.186333\n",
       "2022-06-09  BOS        116  GSW        100   2.26   1.64    1  0.995534\n",
       "2022-06-11  BOS         97  GSW        107   2.37   1.59    0  0.996425\n",
       "2022-06-14  GSW        104  BOS         94   2.40   1.58    1  0.086130\n",
       "2022-06-17  BOS         90  GSW        103   2.41   1.58    0  0.998718\n",
       "\n",
       "[1024 rows x 8 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s = df_s[df_s.index < '2022-09-01']\n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b1934f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204110.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "money_count = []\n",
    "win_all = []\n",
    "money = 0\n",
    "for i in range(len(df_s)):\n",
    "    wl = (df_s[\"predict\"][i] >0.5)*1\n",
    "    if wl == df_s[\"win\"][i]:\n",
    "        if df_s[\"predict\"][i] > 0.5:\n",
    "            win = 1000 * (df_s[\"主勝(終)\"][i] - 1)\n",
    "        elif df_s[\"predict\"][i] < 0.5:\n",
    "            win = 1000 * (df_s[\"客勝(終)\"][i] - 1)\n",
    "        else:\n",
    "            win = 0\n",
    "    else:\n",
    "        if df_s[\"predict\"][i] > 0.5 or df_s[\"predict\"][i] < 0.5:\n",
    "            win = -1000        \n",
    "        else:\n",
    "            win = 0\n",
    "    money += win\n",
    "    money_count.append(money)\n",
    "    win_all.append(win)\n",
    "money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64235401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_40024/298355816.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_s[\"每筆獲利\"] = win_all\n",
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_40024/298355816.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_s[\"累計獲利\"] = money_count\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>HomeScore</th>\n",
       "      <th>Away</th>\n",
       "      <th>AwayScore</th>\n",
       "      <th>客勝(終)</th>\n",
       "      <th>主勝(終)</th>\n",
       "      <th>win</th>\n",
       "      <th>predict</th>\n",
       "      <th>每筆獲利</th>\n",
       "      <th>累計獲利</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-20</th>\n",
       "      <td>LAL</td>\n",
       "      <td>114</td>\n",
       "      <td>GSW</td>\n",
       "      <td>121</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949521</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>-1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>BOS</td>\n",
       "      <td>83</td>\n",
       "      <td>TOR</td>\n",
       "      <td>115</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.146342</td>\n",
       "      <td>2330.0</td>\n",
       "      <td>1330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>CHI</td>\n",
       "      <td>128</td>\n",
       "      <td>NOP</td>\n",
       "      <td>112</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1</td>\n",
       "      <td>0.543415</td>\n",
       "      <td>370.0</td>\n",
       "      <td>1700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>DEN</td>\n",
       "      <td>102</td>\n",
       "      <td>SAS</td>\n",
       "      <td>96</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764804</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>SAC</td>\n",
       "      <td>101</td>\n",
       "      <td>UTA</td>\n",
       "      <td>110</td>\n",
       "      <td>1.38</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>380.0</td>\n",
       "      <td>2380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-06</th>\n",
       "      <td>GSW</td>\n",
       "      <td>107</td>\n",
       "      <td>BOS</td>\n",
       "      <td>88</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186333</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>206470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-09</th>\n",
       "      <td>BOS</td>\n",
       "      <td>116</td>\n",
       "      <td>GSW</td>\n",
       "      <td>100</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995534</td>\n",
       "      <td>640.0</td>\n",
       "      <td>207110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-11</th>\n",
       "      <td>BOS</td>\n",
       "      <td>97</td>\n",
       "      <td>GSW</td>\n",
       "      <td>107</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996425</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>206110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-14</th>\n",
       "      <td>GSW</td>\n",
       "      <td>104</td>\n",
       "      <td>BOS</td>\n",
       "      <td>94</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086130</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>205110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-17</th>\n",
       "      <td>BOS</td>\n",
       "      <td>90</td>\n",
       "      <td>GSW</td>\n",
       "      <td>103</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998718</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>204110.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1024 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Home  HomeScore Away  AwayScore  客勝(終)  主勝(終)  win   predict  \\\n",
       "Matchtime                                                                 \n",
       "2021-10-20  LAL        114  GSW        121   2.30   1.62    0  0.949521   \n",
       "2021-10-23  BOS         83  TOR        115   3.33   1.33    0  0.146342   \n",
       "2021-10-23  CHI        128  NOP        112   3.10   1.37    1  0.543415   \n",
       "2021-10-23  DEN        102  SAS         96   3.49   1.30    1  0.764804   \n",
       "2021-10-23  SAC        101  UTA        110   1.38   3.03    0  0.021742   \n",
       "...         ...        ...  ...        ...    ...    ...  ...       ...   \n",
       "2022-06-06  GSW        107  BOS         88   2.64   1.49    1  0.186333   \n",
       "2022-06-09  BOS        116  GSW        100   2.26   1.64    1  0.995534   \n",
       "2022-06-11  BOS         97  GSW        107   2.37   1.59    0  0.996425   \n",
       "2022-06-14  GSW        104  BOS         94   2.40   1.58    1  0.086130   \n",
       "2022-06-17  BOS         90  GSW        103   2.41   1.58    0  0.998718   \n",
       "\n",
       "              每筆獲利      累計獲利  \n",
       "Matchtime                     \n",
       "2021-10-20 -1000.0   -1000.0  \n",
       "2021-10-23  2330.0    1330.0  \n",
       "2021-10-23   370.0    1700.0  \n",
       "2021-10-23   300.0    2000.0  \n",
       "2021-10-23   380.0    2380.0  \n",
       "...            ...       ...  \n",
       "2022-06-06 -1000.0  206470.0  \n",
       "2022-06-09   640.0  207110.0  \n",
       "2022-06-11 -1000.0  206110.0  \n",
       "2022-06-14 -1000.0  205110.0  \n",
       "2022-06-17 -1000.0  204110.0  \n",
       "\n",
       "[1024 rows x 10 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s[\"每筆獲利\"] = win_all\n",
    "df_s[\"累計獲利\"] = money_count\n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc0a3f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>HomeScore</th>\n",
       "      <th>Away</th>\n",
       "      <th>AwayScore</th>\n",
       "      <th>客勝(終)</th>\n",
       "      <th>主勝(終)</th>\n",
       "      <th>win</th>\n",
       "      <th>predict</th>\n",
       "      <th>每筆獲利</th>\n",
       "      <th>累計獲利</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matchtime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-10-20</th>\n",
       "      <td>LAL</td>\n",
       "      <td>114</td>\n",
       "      <td>GSW</td>\n",
       "      <td>121</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949521</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>-1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>BOS</td>\n",
       "      <td>83</td>\n",
       "      <td>TOR</td>\n",
       "      <td>115</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.146342</td>\n",
       "      <td>2330.0</td>\n",
       "      <td>1330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>CHI</td>\n",
       "      <td>128</td>\n",
       "      <td>NOP</td>\n",
       "      <td>112</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1</td>\n",
       "      <td>0.543415</td>\n",
       "      <td>370.0</td>\n",
       "      <td>1700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>DEN</td>\n",
       "      <td>102</td>\n",
       "      <td>SAS</td>\n",
       "      <td>96</td>\n",
       "      <td>3.49</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764804</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-23</th>\n",
       "      <td>SAC</td>\n",
       "      <td>101</td>\n",
       "      <td>UTA</td>\n",
       "      <td>110</td>\n",
       "      <td>1.38</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>380.0</td>\n",
       "      <td>2380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-06</th>\n",
       "      <td>GSW</td>\n",
       "      <td>107</td>\n",
       "      <td>BOS</td>\n",
       "      <td>88</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186333</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>206470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-09</th>\n",
       "      <td>BOS</td>\n",
       "      <td>116</td>\n",
       "      <td>GSW</td>\n",
       "      <td>100</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995534</td>\n",
       "      <td>640.0</td>\n",
       "      <td>207110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-11</th>\n",
       "      <td>BOS</td>\n",
       "      <td>97</td>\n",
       "      <td>GSW</td>\n",
       "      <td>107</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996425</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>206110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-14</th>\n",
       "      <td>GSW</td>\n",
       "      <td>104</td>\n",
       "      <td>BOS</td>\n",
       "      <td>94</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086130</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>205110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-17</th>\n",
       "      <td>BOS</td>\n",
       "      <td>90</td>\n",
       "      <td>GSW</td>\n",
       "      <td>103</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998718</td>\n",
       "      <td>-1000.0</td>\n",
       "      <td>204110.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1024 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Home  HomeScore Away  AwayScore  客勝(終)  主勝(終)  win   predict  \\\n",
       "Matchtime                                                                 \n",
       "2021-10-20  LAL        114  GSW        121   2.30   1.62    0  0.949521   \n",
       "2021-10-23  BOS         83  TOR        115   3.33   1.33    0  0.146342   \n",
       "2021-10-23  CHI        128  NOP        112   3.10   1.37    1  0.543415   \n",
       "2021-10-23  DEN        102  SAS         96   3.49   1.30    1  0.764804   \n",
       "2021-10-23  SAC        101  UTA        110   1.38   3.03    0  0.021742   \n",
       "...         ...        ...  ...        ...    ...    ...  ...       ...   \n",
       "2022-06-06  GSW        107  BOS         88   2.64   1.49    1  0.186333   \n",
       "2022-06-09  BOS        116  GSW        100   2.26   1.64    1  0.995534   \n",
       "2022-06-11  BOS         97  GSW        107   2.37   1.59    0  0.996425   \n",
       "2022-06-14  GSW        104  BOS         94   2.40   1.58    1  0.086130   \n",
       "2022-06-17  BOS         90  GSW        103   2.41   1.58    0  0.998718   \n",
       "\n",
       "              每筆獲利      累計獲利  \n",
       "Matchtime                     \n",
       "2021-10-20 -1000.0   -1000.0  \n",
       "2021-10-23  2330.0    1330.0  \n",
       "2021-10-23   370.0    1700.0  \n",
       "2021-10-23   300.0    2000.0  \n",
       "2021-10-23   380.0    2380.0  \n",
       "...            ...       ...  \n",
       "2022-06-06 -1000.0  206470.0  \n",
       "2022-06-09   640.0  207110.0  \n",
       "2022-06-11 -1000.0  206110.0  \n",
       "2022-06-14 -1000.0  205110.0  \n",
       "2022-06-17 -1000.0  204110.0  \n",
       "\n",
       "[1024 rows x 10 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df_s[df_s['每筆獲利'] != 0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b1be69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.732421875"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 0\n",
    "for i in range(len(s)):\n",
    "    if s['win'][i] == 1 and s['predict'][i] > 0.5:\n",
    "        w += 1\n",
    "    elif s['win'][i] == 0 and s['predict'][i] < 0.5:\n",
    "        w += 1\n",
    "w / len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e75931ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd6aee9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\user\\\\NBA預測\\\\20221217\\\\20230112\\\\nba_flaml_73%_scaler20230316.model']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump( mixmin_scaler.fit(x_train), r\"C:\\Users\\user\\NBA預測\\20221217\\20230112\\nba_flaml_73%_scaler20230316.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e556dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "529fda79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#with open(r'C:\\Users\\user\\NBA預測\\20221217\\20230112\\nba_flaml_73%20230316.pkl', 'wb') as f:\n",
    "#    pickle.dump(clf, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
